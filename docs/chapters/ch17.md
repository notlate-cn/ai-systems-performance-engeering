# 第17章 扩展分离式预填充和解码推理 (Scaling Disaggregated Prefill and Decode for Inference)

如前一章所述，LLM推理可以分为两个不同的阶段：预填充阶段和解码阶段。预填充阶段处理输入提示词，为该提示词生成模型的内部键值（KV）缓存，而解码阶段使用这些缓存值逐个生成输出token——或者在推测解码的情况下，一次生成几个token。

这两个阶段具有根本不同的性能特征。预填充阶段是计算密集型的，涉及对可能数千个token进行大规模并行矩阵乘法，消耗大量FLOPS。相比之下，解码阶段是内存I/O密集型的，每次token生成都需要读取大型KV缓存、写入新值，并对内存带宽造成压力。简单来说，预填充是高吞吐量、并行的工作负载，而解码是顺序的、延迟敏感的工作负载。

早期的LLM服务系统将这两个阶段视为同一硬件上的单一流水线。因此，它们通常通过使用请求批处理优先考虑吞吐量来偏向预填充阶段。然而，随着交互式应用的增长，首token时间（TTFT，或所有token的预填充延迟）和每输出token时间（TPOT，或每个token的解码延迟）等延迟指标变得与原始吞吐量同等重要。当同时服务两个阶段时，单个基于GPU的推理引擎很难同时优化TTFT和TPOT。

批处理许多请求将提高吞吐量，但会恶化TTFT，因为每个请求都要等待最慢的预填充。它还会影响TPOT，因为解码步骤会在新提示词预填充之后积压。

> As mentioned in an earlier chapter, LLM inference can be divided into two distinct phases: the prefill phase and the decode phase. The prefill phase processes the input prompt to produce the model's internal key-value (KV) cache for that prompt, while the decode phase generates output tokens one by one—or a few at a time, in the case of speculative decoding—using those cached values. These two phases have fundamentally different performance characteristics. The prefill phase is compute bound, involves heavy matrix multiplications over potentially thousands of tokens in parallel, and consumes a significant amount of FLOPS. In contrast, the decode phase is memory I/O bound, reads the large KV cache for each token generation, writes new values, and stresses memory bandwidth. In simpler terms, prefill is a high-throughput, parallel workload, whereas decode is a sequential, latency-sensitive workload. Early LLM serving systems treated the two phases as one monolithic pipeline on the same hardware. As such, they typically favored the prefill phase by prioritizing throughput using request batching. However, as interactive applications grew, latency metrics like time to first token (TTFT, or prefill latency for all tokens) and time per output token (TPOT, or decode latency per token) became as important as raw throughput. It's difficult for a single GPU-based inference engine to optimize both TTFT and TPOT simultaneously when serving both phases together. Batching many requests will improve throughput but will worsen TTFT since every request waits for the slowest prefill. It will also impact TPOT since the decode steps will get backlogged behind new-prompt prefills.

单一推理系统必须在以下两者之间做出选择：以较慢的后续token生成为代价改善（减少）首token时间，或者以新请求承受高初始延迟为代价改善（增加）每token吞吐量。在极端情况下，一个长提示词可能完全占用GPU，这会阻塞其他用户的所有其他提示词预填充工作。然后，一旦解码开始，一次一个token的处理会使GPU核心在每次token生成之间处于空闲状态。

为了解决这些问题，研究人员和工程师寻找将这两个阶段解耦的方法。关键见解是预填充和解码实际上不需要在同一硬件上运行——甚至不需要在同一类型的硬件上运行。

将预填充和解码阶段分离意味着将它们分配给不同的资源，每个资源都专门针对该特定阶段的需求进行优化。这个想法由DistServe论文中的系统开创，该论文证明通过消除阶段之间的干扰，可以同时满足TTFT和TPOT的严格延迟要求。DistServe的评估显示，与没有预填充/解码分离的最先进基线相比，在严格的延迟服务级别目标（SLO）内可以多服务7.4倍的请求。因此，行业框架开始尝试使用独立的预填充和解码服务器。

开源vLLM库结合LMCache和其他组件引入了分离式操作。NVIDIA的Dynamo实现了具有动态路由和自动扩展的分离式预填充和解码，并公开记录了操作细节。许多提供商和开源框架实现或评估了分离。例如，为了满足严格的延迟SLO，据报道OpenAI、Meta和xAI的行业规模服务系统已经采用了这种分离方法。因此，分离式预填充和解码是大规模LLM推理的标准做法。

在超大规模下，大型推理部署可能涉及数十万甚至数百万个GPU服务数十亿请求。在这些环境中，分离的成本和性能优势是巨大的。

通过拆分工作负载，您可以独立优化每个阶段，避免其中一个成为另一个的瓶颈。本章的其余部分探讨如何设计和运营超大规模的分离式预填充/解码推理系统。

在本章中，我们将探索在预填充和解码工作器之间路由请求的调度算法、在重负载下保持服务质量（QoS）的技术，以及使这种分离高效的机制。我们将探索从高速互连到专用解码内核的所有内容。我们还将讨论为每个阶段使用不同GPU类型的异构硬件策略。

> Monolithic inference systems must choose between improving (reducing) time to first token at the cost of slower subsequent token generation—or improving (increasing) per-token throughput while subjecting new requests to high initial latency. In extreme cases, one long prompt can completely tie up the GPU, which would block all other prompt prefill work for other users. And then, once decoding begins, the one-token-at-a-time processing would leave the GPU's cores idle between each token generation. To address these issues, researchers and engineers looked for ways to decouple the two phases. The key insight is that prefill and decode do not actually need to run on the same hardware—or even the same type of hardware. Disaggregating the prefill and decode phases means assigning them to different resources that are each specialized for the needs of that specific phase. This idea was pioneered by systems in a paper on DistServe, which demonstrated that by eliminating interference between the phases, one can meet strict latency requirements for both TTFT and TPOT simultaneously. DistServe's evaluation showed the potential for serving 7.4× more requests within strict latency service-level objectives (SLOs) compared to a state-of-the-art baseline without prefill/decode disaggregation. As such, industry frameworks began to experiment with separate prefill and decode servers. The open source vLLM library introduced disaggregated operation in conjunction with LMCache and other components. NVIDIA's Dynamo implements disaggregated prefill and decode with dynamic routing and autoscaling and publicly documents operational details. Many providers and open frameworks implement or evaluate disaggregation. For instance, to meet strict latency SLOs, industry-scale serving systems from OpenAI, Meta, and xAI have reportedly adopted this disaggregated approach. As such, disaggregated prefill and decode is standard practice for LLM inference at scale. At ultrascale, large inference deployments can involve hundreds of thousands or even millions of GPUs serving billions of requests. In these environments, the cost and performance benefits of disaggregation are massive. By splitting the workload, you can optimize each phase in isolation and avoid one of them becoming the bottleneck for the other. The remainder of this chapter explores how to design and operate a disaggregated prefill/decode inference system at extreme scale. In this chapter, we will explore scheduling algorithms to route requests between prefill and decode workers, techniques to maintain quality of service (QoS) under heavy load, and mechanisms that make this separation efficient. We'll explore everything from high-speed interconnects to specialized decoding kernels. We will also discuss heterogeneous hardware strategies that use different GPU types for each phase.

## 为什么要分离预填充和解码？(Why Prefill-Decode Disaggregation?)

现代交互式LLM服务通常针对p99（99%的请求）TTFT延迟< 200–300 ms。如果不分离预填充工作，这几乎不可能保证，因为一刀切的LLM服务方法会留下显著的性能提升空间。

作为参考，MLPerf v5.0（2025）的Llama2 70B（700亿参数）推理基准测试针对约450 ms TTFT和40 ms TPOT延迟的p99（第99百分位）SLO。对于Llama 3.1 405B（4050亿参数），基准测试针对约6秒TTFT和175 ms TPOT。具体来说，这些SLO反映了Llama 2 Chat 70B和Llama 3.1 405B Instruct的p99 TTFT和p99 TPOT目标。

考虑这样一个场景：一个用户的请求有一个极长的提示词（数千个token级别），而另一个用户的请求有一个非常短的提示词。如果没有分离式预填充和解码，如果这些请求大约同时到达，长提示词的预填充计算将长时间阻塞GPU。

没有分离，具有短提示词的第二个请求需要等待不必要长的时间才能开始解码。这被称为干扰，因为一个请求的预填充工作延迟了另一个请求的解码工作。在连续批处理的上下文中，预填充和解码之间的干扰如图17-1所示。

在简单的FIFO调度策略下，长提示词会放大所有人的尾延迟。通常，队列前面的长或计算密集型预填充会阻塞后面较短、较轻的请求。这被称为队头阻塞，它导致利用率低下、延迟异常值和不满的最终用户。

在灵活的分离架构中，可以将大型提示词预填充发送到专用的计算优化预填充工作器池，而轻量级提示词预填充可以直接发送到解码工作器——绕过预填充工作器。这种灵活性使较短的token不会遭受队头阻塞。这最大化了整体吞吐量并最小化了延迟尾效应。

![图17-1 在同一GPU上共置的预填充和解码引起的干扰](../assets/images/ch17/fig17_01.png)

> Figure 17-1. Interference caused by colocated prefill and decode running on the same GPU

> Modern interactive LLM services often target TTFT latency < 200–300 ms for p99 (99% of requests). This is nearly impossible to guarantee without separating the prefill work since a one-size-fits-all approach to LLM serving leaves significant performance on the table. For context, the MLPerf v5.0's (2025) inference benchmark for Llama2 70B (70 billion parameters) aimed for p99 (99th percentile) SLOs of ~450 ms TTFT and 40 ms TPOT latency. For Llama 3.1 405B (405 billion parameters), the benchmark aimed for ~6 seconds TTFT and 175 ms for TPOT. Specifically, these SLOs reflect p99 TTFT and p99 TPOT targets for Llama 2 Chat 70B and Llama 3.1 405B Instruct. Consider a scenario in which one user's request has an extremely long prompt on the order of many thousands of tokens—and another user's request has a very short prompt. Without disaggregated prefill and decode, if these requests arrive around the same time, the long prompt's prefill computation will block the GPU for an extended period. Without disaggregation, the second request with the short prompt needs to wait an unnecessarily long time before even starting their decode. This is called interference since the prefill work for one request delays the decode work of another. Interference between prefill and decode is shown in Figure 17-1 in the context of continuous batching. Under a simple FIFO scheduling strategy, long prompts can amplify tail latency for everyone. In general, long or compute-heavy prefills at the front of the queue will block shorter, lighter requests behind them. This is called head-of-line blocking, and it leads to poor utilization, latency outliers, and unhappy end users. In a flexible disaggregated architecture, it's possible to send a large prompt prefill to the dedicated pool of compute-optimized prefill workers, while a lightweight prompt prefill can be sent to the decode workers directly—bypassing the prefill workers. This type of flexibility allows shorter tokens to not suffer from head-of-line blocking. This maximizes overall throughput and minimizes latency tail effects.

### 分离的优势 (Advantages of Disaggregation)

分离有两个主要优势：减少干扰和阶段特定优化。让我们接下来讨论这些。

#### 减少干扰 (Reduced interference)

通过分离，预填充任务不再与同一设备上的解码任务竞争。一个忙碌的解码工作器正在生成许多token，不会阻止另一个用户的提示词被处理，反之亦然。

每个阶段的专用资源意味着长提示词的计算不会阻塞另一个用户的token生成。实际上，这产生了更可预测的延迟。图17-2显示了共置和分离式预填充和解码之间的比较。这个实验在DistServe论文和作者随后的博客文章中有更详细的描述。

![图17-2 共置和分离式预填充和解码之间的比较](../assets/images/ch17/fig17_02.png)

> Figure 17-2. Comparison between colocated and disaggregated prefill and decode

> Disaggregation has two primary advantages: reduced interference and phase-specific optimizations. Let's discuss each of these next. With disaggregation, prefill tasks no longer contend with decode tasks on the same device. A busy decode worker, generating many tokens, won't prevent another user's prompt from being processed, and vice versa. Dedicated resources for each stage mean a long prompt's computation won't block another user's token generation. In practice, this produces more predictable latency. Figure 17-2 shows the comparison between colocated and disaggregated prefill and decode. This experiment is described in more detail in the DistServe paper and subsequent blog post by the authors.

在这里，SLO设置为P90 TTFT为0.4秒，P90 TPOT为0.04秒（例如，图17-2中的水平线）。共置系统在给定的TTFT延迟范围内只能支持约3个请求每秒（RPS）的有效吞吐量。在给定的TPOT延迟范围内，它只能维持1.6 RPS。因此，共置配置的有效吞吐量只有1.6 RPS，因为TTFT和TPOT延迟SLO都需要满足。

在分离这两个阶段并分配两个预填充工作器（两个GPU）和一个解码工作器（一个GPU），称为2P1D配置后，预填充和解码工作器都比具有单个GPU的共置配置实现了更好的整体RPS。具体来说，预填充工作器达到约5.6 RPS，解码工作器在三个GPU上实现约10 RPS。因此，2P1D配置的有效吞吐量为每GPU 3.3 RPS。

每GPU 3.3 RPS是通过取预填充工作器的RPS（5.6 RPS × 2 = 11.2 RPS）和解码工作器（10 RPS）的最小值来计算的。这是所有三个GPU上的总共10 RPS。因此，我们必须将RPS除以GPU数量，在这种情况下为3。在配置的SLO下，该系统的有效吞吐量结果为10 RPS ÷ 3 GPU = 每GPU 3.3 RPS。

在这个比较中，解码端的改进主要影响每token延迟。同时，预填充隔离主要改善首token时间。两个SLO都必须满足才能算作有效吞吐量。

这种隔离也可以改善尾延迟。经验上，分离的系统显示出更紧密的延迟分布——并避免了单一系统中看到的长尾。通过消除跨阶段干扰，每个阶段更可靠地满足其SLO——并具有更可预测的一致性。

现在，您也应该问："3倍的成本值得2倍的改进吗？"您问这个问题是对的。需要额外的调优才能使此解决方案更具成本效益，但它显示了正确的方向来收紧延迟分布并改善有效吞吐量。您需要根据您的工作负载做出决定。分离是满足有效吞吐量RPS需求的流行选择。

> Here, the SLO is set to 0.4 seconds for P90 TTFT and 0.04 seconds for P90 TPOT (e.g., horizontal line in Figure 17-2). The colocated system can support only ~3 requests per second (RPS) of goodput within the given TTFT latency bounds. And it can sustain only 1.6 RPS within the given TPOT latency bounds. As such, the goodput of the colocated configuration is only 1.6 RPS since both the TTFT and TPOT latency SLOs need to be met. After disaggregating the two stages and assigning two prefill workers (two GPUs) and a single decode worker (one GPU), called a 2P1D configuration, both the prefill and decode workers achieve better overall RPS than the colocated configuration with a single GPU. Specifically, the prefill worker reaches ~5.6 RPS and the decode worker achieves ~10 RPS spread across the three GPUs. As such, the goodput for the 2P1D configuration is 3.3 RPS per GPU. The 3.3 RPS per GPU is calculated by taking the minimum of the RPS of the prefill workers (5.6 RPS × 2 = 11.2 RPS) and the decode worker (10 RPS). This is 10 RPS total across all three GPUs. As such, we have to divide the RPS by the number of GPUs, or 3 in this case. Under the configured SLOs, this system's goodput result is 10 RPS ÷ 3 GPUs = 3.3 RPS per GPU. In this comparison, the decode side improvements primarily impact per-token latency. Meanwhile, prefill isolation primarily improves time to first token. Both SLOs must be satisfied to count as goodput. This isolation can also improve tail latencies as well. Empirically, systems that disaggregate show tighter latency distributions—and avoid the long tails seen in monolithic systems. By eliminating cross-phase interference, each phase meets its SLO more reliably—and with more predictable consistency. Now, you should also be asking, "Is the 3× cost worth the 2× improvement?" And you would be right to ask that. Additional tuning is required to make this solution more cost-effective, but it shows the right direction to tighten latency distributions and improve goodput. You need to decide based on your workload. Disaggregation is a popular option to meet goodput RPS needs.

#### 阶段特定优化 (Phase-specific optimizations)

阶段特定优化让每个阶段使用最适合它的硬件和并行性。例如，预填充阶段是计算密集型的。因此，您通常会增加张量并行性以在高FLOPS GPU上驱动峰值FLOPS。此外，现代GPU提供低精度模式（FP8和FP4），可以提高计算密集型预填充阶段的吞吐量。

> 您应该优先使用FP4存储权重，并在验证后也用于激活。许多技术栈使用FP4存储权重，FP8存储激活。这些降低的精度有助于最大化吞吐量并最小化HBM占用——精度损失极小。这些精度得到现代硬件和软件栈的支持，包括NVIDIA Tensor Cores和Transformer Engine。

相比之下，解码阶段是内存带宽密集型的，并遭受跨GPU同步开销。因此，它最有效的方式是使用很少或不使用张量并行性（通常TP=1），因为它更多地依赖融合内核来提高算术强度——以及高内存吞吐量GPU。

在单一系统中，您必须为两个阶段选择一种GPU类型和一种并行性策略，这对至少一个阶段来说是次优的。另一方面，分离让您可以独立调整每个阶段以实现最大效率。

拆分阶段还为异构集群打开了大门，其中不同类型的GPU被分配给预填充和解码角色以获得最佳成本性能。例如，使用计算优化的GPU进行提示词预填充，使用内存优化的GPU进行token生成，可以产生比同构部署更好的每美元吞吐量。

> 实际上，最新的GPU通常具有更高的FLOPS和更多的GPU内存。因此，只使用最新GPU代际进行预填充和解码是诱人的——也更常见。但要知道，异构性是降低成本的可行选择。

我们将在本章后面探讨异构集群的想法。我们将展示如何使用高端GPU处理提示词，使用更便宜的GPU生成，可以在规模上带来显著的成本节约。

总之，分离消除了跨干扰并实现了每个阶段的专门处理。预期结果包括更紧密的延迟分布，因为不再有不匹配提示词大小引起的长尾，在延迟约束下改善吞吐量（有效吞吐量），以及更好的整体资源利用率。

> 您应该使用性能分析工具（例如，NVIDIA Nsight Systems）来识别预填充和解码阶段的瓶颈。这些可以跨不同工作器节点跟踪GPU内核和RDMA传输。这将有助于验证解码内核完全重叠通信等。

接下来，让我们讨论如何实际实现分离式服务系统，包括系统架构、通信和决定如何充分利用分离集群的调度策略。

> Phase-specific optimizations let each phase use the hardware and parallelism that suits it best. The prefill phase, for instance, is compute bound. As such, you'd typically increase tensor parallelism to drive peak FLOPS on a high-FLOPS GPU. Additionally, modern GPUs provide lower precision modes (FP8 and FP4) that can increase throughput for the compute-heavy prefill phase. In contrast, the decode phase is memory-bandwidth-bound and suffers from cross-GPU synchronization overhead. So it's most efficient with little or no tensor parallelism (often TP=1) as it relies more on fused kernels to increase arithmetic intensity—as well as high memory throughput GPUs. In a monolith you'd have to pick one type of GPU and one parallelism strategy for both phases, which is suboptimal for at least one of the phases. Disaggregation, on the other hand, lets you independently tune each phase for maximum efficiency. Splitting phases also opens the door to heterogeneous clusters in which different GPU types are assigned to prefill and decode roles for optimal cost-performance. For example, using compute-optimized GPUs for prompt prefill and memory-optimized GPUs for token generation can produce better throughput per dollar than a homogeneous deployment. In practice, the latest GPUs typically have both higher FLOPS and more GPU memory. As such, it's tempting—and more common—to just use the latest GPU generation for both prefill and decode. But just know that heterogeneity is a viable option to reduce cost. We will explore the idea of heterogeneous clusters later in the chapter. We'll show how using high-end GPUs for prompts and cheaper GPUs for generation can lead to significant cost savings at scale. In summary, disaggregation removes cross-interference and enables specialized treatment of each phase. The expected results include tighter latency distributions since no more long tails are caused by mismatched prompt sizes, improved throughput under latency constraints (goodput), and better overall resource utilization. You should use profiling tools (e.g., NVIDIA Nsight Systems) to identify bottlenecks in the prefill and decode phases. These can trace GPU kernels and RDMA transfers across the different worker nodes. This will help validate that decode kernels are fully overlapping communication, etc. Next, let's discuss how to actually implement a disaggregated serving system, including the system architecture, communication, and scheduling policies that decide how to fully utilize the disaggregated cluster.

## 分离式预填充和解码集群池 (Disaggregated Prefill and Decode Cluster Pools)

在分离式部署中，我们维护两个（或更多）工作器池，使得一组GPU专门用于预填充提示词处理，另一组专门用于token生成。这些工作器可以位于数据中心的不同节点或机架上——如果互连足够快，甚至可以在不同的数据中心。（注意：将预填充和解码保持在同一数据中心是实现现实SLO的实际设计选择。）

工作器池通过网络通信，将预填充产生的模型KV缓存移交给执行解码的GPU。调度器或路由器协调此通信。

考虑一个配置，其中模型权重加载在两组GPU服务器上。一组，即预填充工作器，处理提示词并计算KV缓存。另一组，即解码工作器，使用预填充工作器生成的KV缓存处理token生成。

两组工作器通常使用高速互连（例如，NVLink/NVSwitch和InfiniBand）和零拷贝GPU到GPU传输与RDMA进行通信。实际上，这些传输使用GPUDirect RDMA或UCX，并可以在Nsight Systems中与CUDA内核、NVLink活动、存储指标和InfiniBand交换机指标相关联以进行端到端验证。

> 对于基于超级芯片的NVL结构（例如，Grace-Blackwell、Vera-Rubin等），使用NVIDIA多节点NVLink（MNNVL），保持启用NVLink优先集合通信用于TP解码，并在可用时启用SHARP用于AllGather和ReduceScatter集合通信。

当系统收到新请求时，通常在解码工作器上接收。这称为以解码为中心的设计。这是首选方案，因为预填充工作器已经因KV计算而计算密集。

通过让解码工作器处理客户端I/O、路由和会话状态管理，推理系统避免使预填充工作器过载。此外，将请求入口集中在解码节点上简化了网络管理、自动扩展和策略执行。

> 这只是预填充-解码系统架构的一种风格，其中解码工作器是所有请求的集中入口。这种架构用于NVIDIA Dynamo推理系统。另一种常见的架构是使用专用的集中式API路由器将请求路由到预填充或解码工作器。然而，这需要在系统中增加一个额外的移动部件，以及路由器和预填充/解码工作器之间的额外协调——以及额外的扩展和延迟考虑。

在解码工作器接收请求后，它决定是自己进行预填充还是将其"卸载"到预填充工作器池。如果它决定卸载到预填充工作器池，解码工作器稍后将接收KV结果，然后继续解码并生成下一个token。

接下来是简化的NVIDIA Dynamo集群配置片段，定义了两个角色，使用NVIDIA的推理传输库（NIXL）进行基于GPUDirect RDMA的KV缓存传输，让一个GPU直接通过网络写入另一个GPU的内存：

```yaml
roles:
  - name: prefill_worker     # 预填充工作器角色
    model_path: models/llm-70b
    instance_count: 4        # 4个预填充工作器
    gpu_type: B200           # B200 Blackwell计算密集型预填充

  - name: decode_worker      # 解码工作器角色
    model_path: models/llm-70b
    instance_count: 8        # 8个解码工作器
    gpu_type: B300           # B300 Blackwell Ultra高内存解码
```

> NVIDIA的Rubin CPX加速器是预填充工作器的另一个选择。Rubin CPX（CP代表"context processing"）专门为计算密集型工作负载（如预填充）设计。Rubin CPX标志着NVIDIA从"通用加速计算"GPU转向专门为更广泛AI工作负载（如推理）中的特定阶段（例如，预填充）优化的专用芯片。

> In a disaggregated deployment, we maintain two (or more) pools of workers such that one set of GPUs is dedicated to prefill prompt processing and another set is dedicated to token generation. These workers can be on separate nodes or racks in a data center—or even in separate data centers if the interconnect is fast enough. (Note: keeping prefill and decode within the same data center is the practical design choice to achieve realistic SLOs.) The worker pools communicate over a network to hand off the model's KV cache produced by the prefill to whichever GPU will perform the decode. A scheduler, or router, coordinates this communication. Consider a configuration in which the model weights are loaded on two groups of GPU servers. One group, the prefill workers, handles prompts and computes the KV cache. The other group, the decode workers, handles token generation using the KV caches generated from the prefill workers. The two worker groups typically communicate using a high-speed interconnect (e.g., NVLink/NVSwitch and InfiniBand) and zero-copy GPU-to-GPU transfers with RDMA. In practice, these transfers use GPUDirect RDMA or UCX and can be correlated in Nsight Systems alongside CUDA kernels, NVLink activity, storage metrics, and InfiniBand switch metrics for end-to-end validation. For superchip-based NVL fabrics (e.g., Grace-Blackwell, Vera-Rubin, etc.), use NVIDIA Multi-Node NVLink (MNNVL), keep NVLink-first collectives enabled for TP decode, and enable SHARP for AllGather and ReduceScatter collectives when available. When the system receives a new request, it typically receives it on the decode worker. This is called a decode-centric design. This is preferred because the prefill workers are already compute bound with the KV computations. By having the decode workers handle the client I/O, routing, and session state management, the inference system avoids overloading the prefill workers. Also, centralizing request ingress on the decode nodes simplifies network management, autoscaling, and policy enforcement. This is just one style of system architecture for prefill-decode in which the decode worker is the centralized ingress for all requests. This architecture is used in the NVIDIA Dynamo inference system. Another common architecture is to use a dedicated, centralized API router to route the request to either the prefill or decode worker. However, this requires an extra moving part in the system and additional coordination between the router and prefill/decode workers, —as well as additional scaling and latency considerations. After the decode worker receives the requests, it decides whether to do the prefill itself or "offload" it to the prefill worker pool. If it decides to offload to the prefill worker pool, the decode worker will later receive the KV results back and then continue with decoding and generating the next token. Next is a snippet from a simplified NVIDIA Dynamo cluster configuration that defines two roles that use NVIDIA's Inference Xfer Library (NIXL) for GPUDirect RDMA-based KV cache transfers to let one GPU write into another GPU's memory directly over the network: ... NVIDIA's Rubin CPX accelerator is another option for prefill workers. Rubin CPX (the CP stands for "context processing") is specifically designed for compute-bound workloads such as prefill. The Rubin CPX marks NVIDIA's departure from "general accelerated computing" GPUs into specialized chips that are optimized for a specific stage (e.g., prefill) within a broader AI workload such as inference.

在此配置中，我们有四个使用B200 GPU的预填充工作器（适合计算密集型预填充的足够计算能力）和八个使用B300 GPU的解码工作器（用于内存密集型解码的高HBM容量）。混合B200和B300有助于匹配其FLOPS和HBM容量特性，同时最小化成本。两个角色都将使用NIXL和GPUDirect RDMA传输KV缓存块。NIXL抽象了通过NVLink和RDMA NIC进行GPU到GPU数据移动的传输。它还为GPUDirect Storage提供连接器，以便可以从（或写入）不同存储层读取（或写入）KV缓存页面。

在底层，当此系统运行时，每个解码工作器注册其GPU内存的一个区域，以便预填充工作器可以使用RDMA直接写入其中。通常，内存注册元数据（如NIXL描述符）在启动时或首次接触时交换。这样，对于每个远程预填充任务，只需要发送一个小标识符，而不是完整的内存地址结构。

例如，Dynamo使用etcd进行工作器发现和租约。工作器向路由器或控制平面注册必要的内存句柄，以便对等方在需要时获取描述符。预填充工作器将在首次使用时检索它们。这样，预填充请求可以只包含目标KV缓冲区的ID，使控制消息轻量级。

此外，NVIDIA Dynamo的NIXL实现为推理数据移动提供了高吞吐量RDMA和存储抽象，并包括NVLink、基于UCX的结构和GPUDirect Storage的插件。因此，预填充工作器可以直接将KV块写入解码GPU内存。

> 在预填充和解码使用不同TP布局的混合并行部署中，需要在NIXL读取后立即在解码端执行布局转换。这样，KV页面与解码内核的预期布局匹配。此转换与网络传输相比延迟微不足道，并避免了重新预填充。

> In this configuration, we have four prefill workers using B200 GPUs (adequate compute for compute-heavy prefills) and eight decode workers using B300 GPUs (high HBM capacity for heavy memory-bound decodes). Mixing B200s and B300s helps to match their FLOPS and HBM capacity characteristics while minimizing cost. Both roles will use NIXL and GPUDirect RDMA to transfer the KV cache blocks. NIXL abstracts transport for GPU-to-GPU data movement over NVLink and RDMA NICs. It also provides connectors for GPUDirect Storage so that KV cache pages can be read from (or written to) different storage tiers. Under the hood, when this system runs, each decode worker registers a region of its GPU memory so that prefill workers can write directly into it using RDMA. Typically, memory-registration metadata such as NIXL descriptors are exchanged at startup or on first contact. This way, for each remote prefill task, only a small identifier needs to be sent rather than a full memory address structure. For instance, Dynamo uses etcd for worker discovery and leases. Workers register the necessary memory handles with the router or control plane so that peers can obtain the descriptors when needed. The prefill workers will retrieve them on first use. This way, a prefill request can include just an ID for the target KV buffer, making control messages lightweight. Furthermore, NVIDIA Dynamo's NIXL implementation provides a high throughput RDMA and storage abstraction for inference data movement and includes plugins for NVLink, UCX-based fabric, and GPUDirect Storage. As such, prefill workers can write KV blocks directly into decode GPU memory. In mixed parallelism deployments where prefill and decode use different TP layouts, you need to perform a layout transform on the decode side immediately after the NIXL read. This way, the KV pages match the decode kernel's expected layout. This transform is latency-insignificant compared to network transfer and avoids re-prefill.

此架构解耦了每个阶段的扩展。例如，如果您发现由于许多并发长提示词导致预填充成为吞吐量瓶颈，可以添加更多预填充工作器以增加提示词处理容量。

如果由于许多用户生成长输出导致解码成为瓶颈，您将扩展解码工作器。因为解码和预填充是分离的，扩展一个不会直接干扰另一个。

像NVIDIA Dynamo这样的系统支持动态、运行时可配置的分离，这样您可以在不停止集群的情况下动态添加或删除预填充工作器。新的预填充工作器只需注册并开始从队列中拉取任务。如果预填充工作器因任何原因离开集群（崩溃、重启、自动扩展事件、网络分区等），解码工作器将临时进行更多本地预填充以补偿。

NVIDIA Dynamo的分布式运行时使用etcd进行工作器发现和租约。其Planner组件可以通过撤销租约或启动自动发现的新工作器来扩展工作器。这种动态灵活性在超大规模时至关重要，因为负载经常会波动。发生这种情况时，您需要根据需要在角色之间交换工作器。

> This architecture decouples scaling for each phase. If you find that prefill is the throughput bottleneck due to many concurrent long prompts, for instance, you can add more prefill workers to increase prompt processing capacity. If decode becomes the bottleneck due to many users generating long outputs, for instance, you would scale out decode workers. Because decode and prefill are separated, scaling one doesn't directly interfere with the other. Systems like NVIDIA Dynamo support dynamic, runtime-configurable disaggregation such that you can add or remove prefill workers on the fly—without stopping the cluster. New prefill workers simply register and start pulling tasks from the queue. If a prefill worker leaves the cluster for whatever reason (crash, restart, autoscale event, network partition, etc.), the decode workers will temporarily do more local prefills to compensate. NVIDIA Dynamo's distributed runtime uses etcd for worker discovery and leases. Its Planner component can scale workers by revoking leases or launching new workers which are auto-discovered. This dynamic flexibility is crucial at ultrascale when load will often fluctuate. When this happens, you'll want to swap workers between roles as needed.

### 预填充工作器设计 (Prefill workers design)

预填充工作器，或提示词服务器，是专门执行请求初始提示词预填充处理阶段的计算节点。本节讨论预填充节点如何架构以高效处理繁重的计算——以及它们如何在负载下平衡延迟与KV缓存填充的吞吐量。

由于预填充工作负载计算密集，预填充节点应使用高FLOPS GPU，并针对大型矩阵乘法进行优化。每个预填充任务将n个输入token通过所有模型层。

预填充工作器将并行使用数千个GPU线程——如果可用，还会跨多个GPU节点。它们使用熟悉的并行技术，包括张量并行和流水线并行，以减少TTFT。

> Prefill workers, or prompt servers, are the compute nodes dedicated to executing the initial prompt prefill processing phase of requests. This section discusses how prefill nodes are architected to handle the heavy computation efficiently—and how they balance latency versus throughput for KV cache population under load. Because the prefill workload is computationally intensive, prefill nodes should use GPUs with high FLOPS and be optimized for large matrix multiplications. Each prefill task feeds n input tokens through all model layers. The prefill workers will use thousands of GPU threads in parallel—and across many GPU nodes, if available. They use familiar parallelism techniques, including tensor parallelism and pipeline parallelism, to reduce TTFT.

#### 内存管理 (Memory management)

在内存方面，预填充节点需要加载完整的模型权重，并为提示词分配KV缓存。然后，此KV缓存将传输到解码工作器，稍后我们会看到。

预填充用模型参数和模型前向传递的工作激活填充GPU内存，以及提示词输入。一旦创建KV缓存，它会立即发送到解码工作器。KV缓存不会在预填充节点的内存中长期存在。

如果模型非常大或提示词非常长，由于内存限制，预填充可能需要跨GPU进行张量或并行拆分。预填充服务器应灵活使用其并行策略（数据、张量、流水线、专家（MoE）和上下文）以满足延迟目标。

一些推理框架预分配一大块GPU内存供预填充用作工作空间。这减少了整体内存碎片和缓冲区分配时间。

> Memory-wise, prefill nodes need to load the full model weights and also allocate KV cache for the prompt. This KV cache is then transferred to the decode workers, as we'll see in a bit. Prefill populates GPU memory with model parameters and the working activations of the model's forward pass with the prompt inputs. Once the KV cache is created, it's immediately sent to the decode workers. The KV cache doesn't persist long in the prefill node's memory. If a model is extremely large or a prompt is extremely long, prefill may require tensor or parallel splits across GPUs due to memory limitations. Prefill servers should be flexible with their parallelism strategies (data, tensor, pipeline, expert (MoE), and context) to meet latency targets. Some inference frameworks preallocate a big chunk of GPU memory for prefills to use as working space. This reduces overall memory fragmentation and buffer allocation time.

#### 延迟与吞吐量的优化 (Optimizing for latency versus throughput)

在调整分离式预填充集群时，您面临一个基本权衡：最小化每个单独提示词的TTFT，还是在重负载下最大化整体每秒请求数（RPS），或减少TPOT。

分离式系统通过支持不同的调度策略来处理这种权衡，包括延迟优先方法和吞吐量优先方法。让我们接下来描述这些方法：

**延迟优先方法 (Latency-first approach)**

为了减少TTFT，预填充节点应在提示词到达时立即处理——几乎不进行批处理。在此模式下，您避免等待其他请求填充批次。因此，每个提示词立即开始执行并尽快完成——假设集群中有可用的GPU。

这种延迟优先方法的缺点是GPU利用率较低，因为您使用的是小型或不存在的批次。因此，GPU经常处于空闲状态，对于给定的集群大小，您的系统将服务较少的并发请求。在这种情况下，您可以过度配置预填充集群容量，或使用批处理大小为1来保证请求的严格延迟SLO。

**吞吐量优先方法 (Throughput-first approach)**

如果峰值吞吐量（RPS）和最小TPOT是您的优先事项，您应该将提示词批处理成更大的组以充分利用每个GPU。通过将8-32个提示词累积到单个批次中，您可以提高算术强度并保持GPU计算单元忙碌。这将增加整体吞吐量。

吞吐量优先方法的缺点是每个请求都会产生批处理延迟，等于收集批次所需的时间。批次大小越大，延迟越长。

对于极端吞吐量推理系统配置，您可以选择使用数据并行或流水线并行为每个请求分配多个GPU。

使用数据并行，整个模型在每个GPU上复制。批次被拆分为跨GPU的微批次。每个GPU通过其完整的模型副本对其数据子集执行前向传递。然后从所有GPU聚合输出以获得最终输出。

数据并行聚合所有GPU的内存带宽和计算能力以提高每批次性能。然而，它将最大并行度降低为GPU总数÷每个请求的GPU数。这降低了您的整体并发请求容量。如果系统为单个请求使用过多GPU，这可能会使资源闲置。这将造成吞吐量和并发性之间的不平衡。

流水线并行将模型的层划分为不同GPU上的顺序阶段，例如GPU 0和GPU 1。一旦GPU 0完成微批次0的阶段，它将激活转发给GPU 1并开始微批次1的阶段1。这种流水线模式使所有GPU忙碌于不同的工作块。

流水线并行增加了每批次吞吐量，但如果微批次大小或阶段划分没有仔细平衡，它会增加GPU间通信开销和流水线"气泡"。

最终，您专用的每个额外GPU都会增加吞吐量，但会减少您一次可以处理的请求数量——假设集群大小固定。您始终可以扩展GPU集群，但假设集群大小固定，您应该根据延迟SLO还是吞吐量SLO对您的用例最重要来选择配置。

> When tuning a disaggregated prefill cluster, you face a fundamental trade-off between minimizing the TTFT for each individual prompt and maximizing overall requests per second (RPS), or reducing TPOT, under heavy load. Disaggregated systems handle this trade-off by supporting different scheduling policies for a latency-first approach versus a throughput-first approach. Let's describe each of these approaches next: To reduce TTFT, prefill nodes should process prompts as soon as they arrive—and with little to no batching. In this mode, you avoid waiting for other requests to fill a batch. As such, every prompt starts execution immediately and finishes as fast as possible—assuming available GPUs in the cluster. The downside of this latency-first approach is lower GPU utilization since you are using small or no batches. As such, GPUs often sit idle, and your system will serve fewer concurrent requests for a given cluster size. In this case, you can either over-provision your prefill cluster capacity or use a tiny batch size of 1 to guarantee strict latency SLOs for your requests. If peak throughput, or RPS, and minimal TPOT are your priorities, you should batch prompts into larger groups to fully load each GPU. By accumulating 8–32 prompts into a single batch, you raise arithmetic intensity and keep the GPU compute units busy. This will increase the overall throughput. The downside to the throughput-first approach is that each request incurs a batching delay equal to the time it takes to collect the batch. The larger the batch size, the longer the delay. For extreme throughput inference system configurations, you can choose to assign multiple GPUs per request using either data parallelism or pipeline parallelism. With data parallelism, the entire model is replicated on each GPU. The batch is split into minibatches across the GPUs. Each GPU performs a forward pass on its subset of data through its complete copy of the model. The output is then aggregated from all the GPUs for the final output. Data parallelism aggregates memory bandwidth and compute power across all of the GPUs to increase per-batch performance. However, it reduces maximum parallelism to the total # of GPUs ÷ GPUs per request. This reduces your overall concurrent request capacity. This can leave resources idle if the system uses too many GPUs per single request. This will create an imbalance between throughput and concurrency. Pipeline parallelism divides the model's layers into sequential stages on different GPUs, such as GPU 0 and GPU 1. As soon as GPU 0 finishes its stage for microbatch 0, it forwards activations to GPU 1 and begins stage 1 for microbatch 1. This assembly-line pattern keeps all GPUs busy on different chunks of work. Pipeline parallelism increases per-batch throughput, but it adds inter-GPU communication overhead and pipeline "bubbles" if the microbatch size or stage splits are not carefully balanced. Ultimately, each additional GPU that you dedicate will increase throughput but decrease how many requests you can handle at once—given a fixed-size cluster. You can always scale out the GPU cluster, but assuming a fixed cluster size, you should choose your configuration based on whether latency SLOs or throughput SLOs are most important to your use case.

#### 延迟感知调度和批处理 (Latency-aware scheduling and batching)

分离式系统结合前面提到的延迟感知调度策略来平衡这些因素。例如，它们可能保证单请求执行——而不批处理请求——除非负载足够高，以至于合并少量请求不会违反TTFT目标。

许多集群设计在调度器中包含SLO约束。例如，如果p90 TTFT必须≤ X ms，系统将选择仍能满足典型提示词大小SLO的最大批次大小或并行策略。

另一种策略是自适应批处理窗口。例如，在低负载时，它可以使用批次大小为1立即运行请求。在较高负载时，系统可以允许在较小时间窗口（如2-10 ms）内到达的请求微批次。这样，轻微延迟可以产生大的GPU利用率提升——但仅在需要和可容忍时。

许多推理引擎为其预填充工作器优先考虑延迟。系统通常尽快执行提示词任务，甚至容忍一些GPU利用率不足，因为快速的首token显著改善用户体验。

通常会为平均负载配置比所需更多的预填充容量。这样，预填充集群可以吸收提示词突发而不会出现延迟峰值。在下一章中，我们将讨论自适应机制以动态重新平衡资源，以便预填充和解码工作器都不会随时间成为瓶颈。

像Kubernetes这样的现代编排器可以自动扩展每个层级。例如，如果预填充GPU利用率保持高位而解码利用率低，编排器可以触发自动扩展事件以添加预填充pod（或节点）——甚至可能删除一些解码pod/节点。

> 这种自适应扩展通常使用预填充队列长度等指标来帮助驱动决策。

另一种选择是实现优先级队列，使短提示词在具有较少批处理的单独快速通道上调度。长、可批处理的提示词进入吞吐量优化的队列。NVIDIA Dynamo在调度中支持延迟类别。您可以通过标记请求并为每个类别设置不同的批处理窗口来模拟这一点。

关键要点是预填充工作器优先考虑快速周转。分离让我们可以做到这一点而不会损害解码性能，因为解码在另一组工作器上运行。我们可能会在低流量期间"浪费"一些预填充GPU周期，但我们在高峰流量期间保持低TTFT。对于交互式服务来说，这是一个值得的权衡。

> Disaggregated systems incorporate the latency-aware scheduling policies mentioned earlier to balance these factors. For instance, they might guarantee single-request execution—and not batch requests—unless load is high enough that combining a small number of requests won't violate the TTFT target. Many cluster designs include an SLO constraint in the scheduler. For instance, if p90 TTFT must be ≤ X ms, the system will choose the largest batch size or parallelism strategy that still meets the SLO for a typical prompt size. Another strategy is adaptive batching windows. For instance, at low load, it can run requests immediately using a batch size of 1. And at higher loads, the system can allow microbatches of requests arriving within a small time window, such as 2–10 ms. This way, a slight delay can produce a big GPU utilization win—but only when it's needed and tolerable. Many inference engines favor latency for their prefill workers. Systems often execute prompt tasks as soon as possible and even tolerate some GPU underutilization, because a fast first token significantly improves user experience. It's common to provision more prefill capacity than needed for an average load. This way, the prefill cluster absorbs bursts of prompts without latency spikes. In the next chapter, we will discuss adaptive mechanisms to rebalance resources on the fly so that neither the prefill nor the decode workers become a bottleneck over time. Modern orchestrators like Kubernetes can automatically scale each tier. For example, if prefill GPU utilization stays high and decode is low, the orchestrator can trigger an autoscaling event to add prefill pods (or nodes)—and possibly even remove some decode pods/nodes. This kind of adaptive scaling is often implemented with metrics like prefill queue length to help drive the decisions. Another option is to implement priority queues such that short prompts are scheduled on a separate fast lane with less batching. Long, batchable prompts go to a throughput-optimized queue. NVIDIA Dynamo supports latency classes in scheduling. You can emulate this by tagging requests and having different batching windows per class. The key takeaway is that prefill workers prioritize quick turnaround. Disaggregation lets us do this without harming decode performance since decode is running on a different set of workers. We might "waste" some prefill GPU cycles during low-traffic periods, but we maintain low TTFT during peak traffic periods. It's a worthwhile trade-off for interactive services.



