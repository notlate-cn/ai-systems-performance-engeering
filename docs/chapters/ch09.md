# 第9章 提高CUDA内核效率和算术强度 (Increasing CUDA Kernel Efficiency and Arithmetic Intensity)

即使您通过大规模并行性和高ILP完全隐藏了延迟，内核的性能仍可能受限于每次内存访问所做的有用工作量。算术强度，也称为操作强度，衡量每字节从内存传输的数据执行多少浮点运算，或每字节FLOPS。

新一代GPU的计算吞吐量提升远超内存带宽。这种不断扩大的差距意味着增加算术强度比以往任何时候都更加关键。更高的算术强度表示内核对每个获取的字节做更多计算，这对于充分利用GPU的计算能力至关重要。

算术强度是屋顶线性能模型中的关键指标。屋顶线模型是一个有用的可视化工具，将内核性能（FLOPs/sec）相对于算术强度（FLOPs/byte）进行绘制。它显示内存带宽和计算吞吐量的硬件上限（屋顶），使我们能够看到内核是内存受限（性能受内存传输限制）还是计算受限（性能受ALU吞吐量限制）。

在实践中，您可以使用Nsight Compute等工具生成屋顶线图，其中包括屋顶线分析视图。使用这些工具，您可以验证内核最初是内存受限还是计算受限——然后在进行优化时继续分析和验证改进。

目标是将内核推向计算受限状态，并利用GPU日益增长的计算能力。屋顶线性能模型可以正确指导您的优化朝着该目标进行。

> Modern GPU-accelerated workloads are pushing hardware to its limits. Multi-die GPUs like Blackwell connect multiple reticle-limited dies with a 10 TB/s NV-HBI link and increase L2 to 126 MB. These hardware design choices materially change memory-vs-compute tradeoffs and occupancy sweet spots. This makes profiling and optimization even more critical than ever. Building on the fundamentals of memory optimizations, we now turn to advanced latency-hiding techniques and throughput enhancements designed to fully leverage the full power of modern GPUs. We will focus on identifying performance bottlenecks and then applying a systematic set of optimization strategies to eliminate them one by one. Key themes in this chapter include tuning occupancy, optimizing warp efficiency, and increasing instruction-level parallelism. By the end of the chapter, you will be able to identify root causes of GPU underutilization—as well as apply the right combination of optimizations. We will also prepare you for more advanced techniques like kernel fusion and pipelining with primitives like CUDA Graphs and CUDA streams, which we cover in subsequent chapters. While our focus is higher-level languages like CUDA C++ and AI frameworks like PyTorch, the principles of profiling and tuning apply at all levels of the stack right down to the hardware. As such, understanding low-level hardware performance remains critical for diagnosing bottlenecks that higher-level abstractions make difficult to fully resolve.

如前一章所示，屋顶线图使用一条水平线表示硬件的峰值计算吞吐量（屋顶）——一条从原点出发的对角线表示受内存带宽限制的可实现峰值吞吐量。内核的算术强度决定它在x轴上的位置，其性能可以与这些上限进行比较，如图9-1所示。

> As shown in a previous chapter, a roofline chart uses one horizontal line to represent the hardware's peak compute throughput (the roof)—and a diagonal line from the origin represents the peak achievable throughput limited by memory bandwidth. A kernel's arithmetic intensity determines where it falls on the x-axis, and its performance can be compared against these ceilings, as shown in Figure 9-1.

![图9-1 示例屋顶线模型（GFLOP/s与算术强度FLOPs/byte）](images/ch09/fig9-1.png)

> Figure 9-1. Example Roofline model (GFLOP/s versus arithmetic intensity in FLOPs/byte)

算术强度低的内核，或每移动字节数据只有少量数学操作，将是内存受限的。在这种情况下，内核的速度受硬件内存带宽限制，因为GPU大部分时间在等待数据而不是进行计算。

> A kernel with low arithmetic intensity, or few math operations per byte of data moved, will be memory bound. In this case, the kernel's speed is capped by the hardware's memory bandwidth, because the GPU spends most of its time waiting for data rather than crunching numbers.

相反，算术强度很高的内核，或每移动字节有很多FLOPs，将是计算受限的，因为它在接近峰值能力的情况下利用ALU和Tensor Core。在这种情况下，内核的内存带宽使用是次要关注点。

> Conversely, a kernel with very high arithmetic intensity, or many FLOPs per byte moved, will be compute bound because it is utilizing ALUs and Tensor Cores near their peak capacity. In this case, the kernel's memory bandwidth usage is a secondary concern.

目标始终是在可能的情况下增加算术强度，通过对每字节从全局内存传输的数据做更多计算工作（每字节FLOPs）。您可以使用循环分块重用数据、使用片上L1/共享内存进行重用，以及将多个内核融合为一个以使中间结果不写入全局内存等技术来增加算术强度。

> Modern compiler frameworks such as PyTorch's TorchInductor automatically do some of these optimizations to keep computations on the GPU, reduce off-chip memory traffic, and increase effective arithmetic intensity. However, as a developer, you may still need to manually combine these techniques or write custom CUDA kernels to ensure that data is reused optimally before being evicted from caches, for instance.

您还可以使用低精度数据类型（FP16、FP8、FP4）来减少内存传输量——并利用Tensor Core增加每秒FLOPs。这些共同将增加每字节FLOPs比率并增加算术强度。接下来，让我们讨论其中一些技术。

请记住，并非每个工作负载都能轻易增加其算术强度。它受算法特性约束。但是，您应该寻找任何机会改进算法、重用数据、融合操作并增加批量大小以提高算术强度而不改变算法的结果（例如精度）。

## 多级微块化和软件预取 (Multilevel Microtiling and Software Prefetching)

如第7章所述，分块（也称为分块或阻塞）和数据重用是提高算术强度的有效方法。在该章中，我们展示了如何将A和B的小子矩阵（块）加载到共享内存中，使从全局内存获取的每个字节可用于静态随机存取存储器（SRAM）速度下的许多乘累加操作。

每当您重构代码使每个元素加载一次并被使用数十或数百次时，就像分块的情况一样，您将每字节FLOPs比率乘以重用因子。例如，在典型的矩阵乘法中，A和B的32×32块为共享内存中的每个元素产生1,024（1,024 = 32 × 32）次独立乘法。因此，与每次操作直接从DRAM获取每个元素相比，算术强度提高了。

除了简单的共享内存分块外，您还可以通过多级分块进一步提高强度并暴露更多ILP。使用多级分块，在将块暂存到共享内存后，您让每个线程使用向量化类型如float4和<half2>将微块加载到寄存器中。这样，重复的操作完全在寄存器中进行。多级分块示例如图9-2所示。

![图9-2 全局内存（DRAM）、共享内存（SMEM）和寄存器之间的多级分块](images/ch09/fig9-2.png)

> Figure 9-2. Multilevel tiling between global memory (DRAM), shared memory (SMEM), and registers

这种SM内重用（寄存器→SMEM→DRAM）减少了每一级的工作集——并最小化了片外流量。一如既往，确保在填充共享内存时合并全局读取，并填充/交叉共享数据以避免内存bank冲突，如我们在第7章中介绍的那样。

在现代GPU上，这些内循环分块步骤通常通过使用MMA片段API来覆盖。硬件使用Tensor Core指令在共享内存和张量内存（TMEM）之间隐式移动数据。TMEM使用由编译器和库管理。在现代GPU上，tcgen05指令在共享内存和TMEM之间隐式暂存数据。它们使用独特的TMEM地址空间。但是，开发人员在实现某些算法时仍可以手动使用cp.async或TMA将块移动到共享内存，并需要显式控制。

一个密切相关的技术是软件预取，通常实现为双缓冲。例如，不是等到当前块的计算完成，您可以向共享内存发出下一个块的异步加载。这将重叠DRAM→共享内存（SMEM）传输与正在进行的算术。仔细的预取可以显著减少停顿时间并提高吞吐量。其思想是将数据传输与计算重叠，使ALU永远不会因等待数据而饥饿。

当在CPU-GPU超级芯片（如Grace Blackwell）上使用统一内存时，您可以使用`cudaMemPrefetchAsync()`提示很快将需要某个块。这向运行时提示通过NVLink-C2C迁移页面。但是，预取只是一个提示而不是保证。您仍然希望确保您正在重叠传输并适当同步以避免页面错误停顿。以这种方式重叠数据移动与计算确保每当需要新块时ALU保持供给。这进一步隐藏了内存延迟——并提高了实现的FLOPS。

> Unified memory eases development but may not produce the best performance. Expert users often prefer explicit cudaMemcpy or pinned memory allocations to fully avoid page migration overheads.

简而言之，DRAM中的每个字节在片上级别（寄存器或共享内存）被重用的次数越多，您的算术强度就越高。更高的算术强度将内核推向计算受限状态。

## 使用线程块集群进行分块 (Tiling with Thread Block Clusters)

在现代GPU上，您可以使用来自协作组的CUDA线程块集群扩展分块重用思想（在第10章讨论）。这些允许多个线程块使用分布式共享内存（DSMEM）共享数据，如图9-3所示。

![图9-3 CGA内的CTA（线程块）共享的DSMEM](images/ch09/fig9-3.png)

> Figure 9-3. DSMEM shared by CTAs (thread blocks) within a CGA

我们将在下一章详细介绍CGA和线程块集群，但在这里值得一提，因为它们可以直接增加算术强度。例如，四个线程块的集群可以使用张量内存加速器（TMA）多播功能协作加载一个块，如图9-4所示，该图使用四个CTA演示此机制。

![图9-4 对于这四个（2×2）线程块集群，A和B的每个块使用多播同时加载到四个CTA（线程块）中（来源：https://oreil.ly/EEO_O）](images/ch09/fig9-4.png)

> Figure 9-4. For these four (2 × 2) thread block clusters, each tile of A and B is loaded into four CTAs (thread blocks) simultaneously using multicast (source: https://oreil.ly/EEO_O)

每个块在四个线程块之间分区，因此该块的全局内存流量在集群上分摊。块只获取一次并被所有四个线程块重用。

当四个CTA使用多播重用相同数据时，线程块集群可以在2×2集群中将全局内存流量减少多达4倍。此外，线程块集群通过降低分母或从全局内存移动的字节数来增加每个GPU的算术强度。这些的一种特殊形式称为线程块对，将在稍后在Tensor Core分块的上下文中讨论。

> Blackwell supports thread block clusters up to 16 thread blocks when you opt into a nonportable cluster size beyond the default portable limit of 8 CTAs. To enable this, set the cudaFuncAttributeNonPortableClusterSizeAllowed attribute on the kernel. Larger clusters can raise reuse but may reduce occupancy, so profile before enabling 16. This can support even larger multi-SM tiles, which maximizes data reuse (16×) and increases arithmetic intensity by a similar factor.

## 内核融合 (Kernel Fusion)

增加算术强度的另一种方法是将多个操作——或循环迭代——融合为一个操作。通过将多个内核融合在一起，从内存加载的数据可以在写回之前用于多次计算和迭代。

类似地，上一节讨论的循环展开允许单个线程对每个加载的数据元素执行更多计算，但以更多寄存器使用为代价。过多融合会增加每线程寄存器压力并降低占用率，因此存在权衡。

始终分析融合内核。如果寄存器使用变得过多并开始溢出到本地内存，融合的好处可能会被额外的内存流量抵消。但是，如果您找到正确的平衡，您可以改善每移动字节的FLOPS，如果内存带宽是限制因素，这是有益的。

现代深度学习框架可以通过其即时编译器和图优化器自动融合和展开。例如，PyTorch的torch.compile，特别是TorchInductor，可以自动融合逐元素操作序列。我们将在第13章和第14章介绍PyTorch编译器。

> Elementwise operations, also called pointwise operations, apply a simple computation independently to each element of a tensor.

融合这些逐元素操作通过将中间值保持在片上来消除不必要的内存流量。这增加了从全局内存获取的每字节所做的工作量——提高了算术强度。

例如，朴素实现启动两个内核。第一个内核读取x并将y写入全局内存。第二个读取y并写入z：

```cpp
y = sin(x);
z = sqrt(y);
```

在这里，每个元素被触摸两次：一次在sin(x)之后，一次在sqrt(y)之后。因此，每个内核的算术强度非常低，因为它对每个元素——每个加载/存储操作——只执行一个昂贵的数学函数（一个多周期ALU指令）。相比之下，融合内核在单次传递中执行这组相同的操作：

```cpp
z[i] = sqrt(sin(x[i]));
```

每个x[i]加载一次，然后在寄存器中应用sin和sqrt，只有最终的z[i]写入内存。因为中间的y永远不会输出到全局内存，有效的每字节FLOPS急剧上升，将操作推向计算屋顶。

> As a rule of thumb, if data will be read more than once by threads in the same thread block, it's often worth staging the data into shared memory to eliminate redundant global loads. This will help lift your kernel out of the memory-bound regime and into the compute-bound regime to better utilize the ample GPU FLOPS.

融合减少了全局内存流量并增加了算术强度，因为每个元素现在对每次读取和写入内存操作经历两次数学操作。在我们的示例中，我们将每元素的FLOPS翻倍（sin + sqrt），同时大致减半内存流量，因为没有中间写入。这产生了显著更高的算术强度或FLOPS/byte。

为了说明这一点，让我们用一个具体示例演示算术强度。假设我们想对2D张量x（形状[batch, hidden]）的每个长度为hidden的行进行L2归一化。对于每行b，计算单个范数，`norm_b = sqrt(Σ_i x[b,i]*x[b,i] + ε)`，然后对所有i写入`y[b,i] = x[b,i] / norm_b`。

朴素实现会在一个内核中平方，在第二个内核中将每行归约为标量，在第三个内核中除法。这将需要多次内核启动并中间写入HBM。

让我们假设4个内核中的每一个需要1 FLOP的计算。因此，4个内核中每一个的算术强度是每12字节1 FLOP（2次float读取，1次float写入），或0.083 FLOPS/byte。

相反，我们可以将4个内核融合为单个内核并增加算术强度。手动融合的内核代码如下所示：

```cpp
__global__ void fusedL2Norm(const float* __restrict__ x,
                            float* __restrict__ y,
                            int hidden) {
  extern __shared__ float sdata[];        // reduction buffer
  const int batch   = blockIdx.x;         // one block per batch row
  const int tid     = threadIdx.x;
  const float* batch_ptr = x + size_t(batch) * hidden;

  // 1) Accumulate sum of squares into shared memory
  float local = 0.f;
  for (int i = tid; i < hidden; i += blockDim.x) {
    float v = batch_ptr[i];
    local   = fmaf(v, v, local);          // v*v + local
  }
  sdata[tid] = local;
  __syncthreads();

  // 2) Parallel reduction to sdata[0]
  for (int offset = blockDim.x >> 1; offset > 0; offset >>= 1) {
    if (tid < offset) sdata[tid] += sdata[tid + offset];
    __syncthreads();
  }

  // 3) Normalize (guard tiny norms)
  float norm = sqrtf(sdata[0]);
  float inv = rsqrtf(sdata[0]); // prefer inverse

  float* out_batch = y + size_t(batch) * hidden;
  for (int i = tid; i < hidden; i += blockDim.x) {
    // multiply by inverse (rsqrt) vs. divide by sqrt
    out_batch[i] = batch_ptr[i] * inv;
  }
}
```

在这个融合内核中，每个线程遍历其x[b,*]切片两次——一次累积平方的局部和，一次写入归一化输出——因此全局流量约为每元素12字节（两次读取 + 一次写入）。每元素内核在归约期间做约1次乘法 + 1次加法，在归一化期间做1次乘法。

sqrt和rsqrt在整个行上分摊。对于屋顶线定位，保守的算术强度是≈3 FLOPs / 12字节 ≈ 0.25 FLOPs/byte（加上每行sqrt的微小1/(hidden * 12)贡献）。这让我们通过给每个线程多个元素来增加ILP来隐藏sqrt和rsqrt延迟。

> Additionally, as shown in the previous code, we compute the inverse sqrt (rsqrtf) and multiply instead of dividing. This is a common micro-optimization—especially for hot inner loops. The idea is to replace a slow division instruction stream for a high-throughput multiply instruction stream. We are also trading a sqrtf with a cheaper rsqrtf approximation. These are micro-optimizations because overall, this pipeline is memory-bound and not compute-bound—but it's interesting to highlight. There is yet another optimization not shown here. It involves doing one rsqrtf/sqrtf in a single thread within the thread block and broadcasting the scalar result to the other threads using shared memory. This has more impact on improving performance. Please see the book's GitHub repo for more details on this optimization.

与具有中间写入HBM的朴素三内核流水线（平方→归约→除法）相比，融合版本至少删除了一次全局写入/读取往返和一次启动屏障。因此，其实际强度和运行时间要好得多——即使每元素FLOP/byte只有约0.25。这是由于延迟节省和缓存局部性改进。

在实践中，这个单一融合内核由于更高的算术强度（FLOPS/byte）、更好的缓存局部性以及通过将三个独立内核折叠为一个而减少的启动开销，执行速度比一系列独立内核更快。

融合不仅增加算术强度并将内核更多地推向屋顶线的计算受限一侧，而且还节省内存带宽。在朴素的多内核版本中，我们必须将中间结果写入全局内存并在下一个内核中读回。在融合版本中，中间结果（例如ai*ai）永远不必离开线程的寄存器。

在代码示例中，加法可以直接使用这些寄存器来计算总和。然后sqrt可以使用该总和——所有这些都不需要额外的全局内存流量。只有最终结果写回全局内存。

因此，融合内核可能对全局内存的12字节数据移动实现4 FLOPS，而朴素、未融合方法在考虑中间内存移动后对36字节的加载和存储实现4 FLOPS。这意味着更少的DRAM流量和更低的延迟。

这个简单示例展示了融合如何增加内核的算术强度和整体性能。让我们看看另一种通过利用GPU的Tensor Core硬件单元来增加算术强度的方法。

> State-of-the-art GPU kernels achieve higher arithmetic intensity using vertical fusion, which combines sequential operations on the same data—as well as horizontal fusion, which combines parallel operations across data. Libraries like NVIDIA's CUTLASS or OpenAI's Triton (integrated into PyTorch's compiler backend, TorchInductor) can help you implement these different types of fused kernels very efficiently using Tensor Cores, TMEM, TMA, etc.

## 结构化稀疏 (Structured Sparsity)

在现代GPU上，2:4结构化稀疏由Sparse Tensor Core和cuSPARSELt在硬件中加速。2:4意味着每四个连续权重中恰好有两个是非零的。创建这种类型的稀疏性有时称为剪枝。

通过将一半权重剪枝为2:4模式，每次内存加载现在传递两倍数量的实际参与乘法的非零值。换句话说，您不再获取最终为零的权重。因此，您不会在您知道为零的事物上浪费矩阵乘法操作，如图9-5所示。

![图9-5 2:4结构化稀疏](images/ch09/fig9-5.png)

> Figure 9-5. 2:4 structured sparsity

结构化稀疏在模型训练后应用。模型被剪枝并优化用于推理。剪枝和格式转换在软件栈中完成，如cuSPARSELt和框架工具。请注意，Transformer Engine加速支持的稀疏执行，但在转换期间不强制稀疏性。

剪枝和格式转换在软件中处理——通常通过cuSPARSELt和框架工具。在PyTorch中，使用`to_sparse_semi_structured()`在部署稀疏GEMM到Sparse Tensor Core之前将训练好的密集模块转换为2:4稀疏格式。

一旦您的模型被转换，它将调用在Sparse Tensor Core上运行的优化稀疏GEMM内核，而不是标准内核。Sparse Tensor Core对于许多推理工作负载可以达到比其密集对应物高近2倍的加速——特别是在提交大批量输入时，因为这些分摊了内核启动开销。

> Batching is a very common and practical way to increase arithmetic intensity. Instead of processing one item at a time—with all of the associated memory I/O, etc.—you process multiple items in one pass so that memory access (e.g., loading weights, etc.) is amortized over multiple computations.

这给稀疏加速的矩阵乘法足够的并行工作来隐藏处理索引或压缩表示的任何开销。在较小的批量中，这种开销可能占主导地位并限制您观察到的加速量。

2:4稀疏将在使用大型矩阵乘法时产生最大收益，这在基于transformer的模型（如LLM）中很常见。这是因为硬件可以充分利用专用的Sparse Tensor Core。这些Sparse Tensor Core直接在硬件中对半宽数据进行操作。这跳过零并在相同的周期预算内对非零元素执行两倍的工作。

由于Blackwell上的计算能力增长快于HBM带宽，结构化稀疏是保持计算受限的好方法。即使在Grace Blackwell系统上，NVLink-C2C让GPU以非常高的吞吐量从CPU内存流式传输数据，您仍然希望在每个加载的块上最大化每字节FLOPS。

例如，通过以2:4模式剪枝50%的权重，您确保一半的内存流量永远不需要。这立即减少了全局内存读取并将有效算术强度提高近2倍。

NVIDIA GPU在硬件中实现这种2:4结构化稀疏，使得每个16元素块可以将8个元素置零。这是用于将Tensor Core吞吐量翻倍用于稀疏矩阵的模式。截至本文撰写时，没有其他任意稀疏模式获得这种特殊的硬件加速。

> The speedups from sparsity assume that the model's accuracy is maintained. In practice, it's important to fine-tune or carefully calibrate after pruning. This way, you can minimize accuracy loss.

在应用稀疏性之前，首先实现前面介绍的基本优化很重要：合并所有全局加载，使用分块重用数据，并融合逐元素操作以消除额外的内存往返。一旦这些基础到位并验证，结构化稀疏可以为推理提供另一个加速。

> Structured sparsity typically applies to inference workloads. During training, gradients do not benefit from 2:4 sparsity. In addition, maintaining sparsity in gradient updates is complex. As such, it's recommended to use it for deployment scenarios in which you've prepruned and calibrated the model. NVIDIA's 2:4 sparse Tensor Core feature is primarily used for inference. Training support is limited and model-dependent and framework-dependent. Verify support in your software stack before relying on it.

## 重计算与内存权衡 (Recomputation Versus Memory Trade-Off)

此外，如果内存带宽是瓶颈，考虑按需重新计算预计算的值（例如x²），而不是存储或加载它们。例如，在寄存器中重复计算x*x通常比从全局内存加载先前计算的x²更快。廉价表达式的连续重计算可以增加算术强度，并且在内存稀缺时是一种有用的技术。

许多LLM推理引擎使用这种技术来节省内存。它们不是在HBM中存储大型激活张量并稍后读回，而是可以即时重新计算某些层和激活。这类似于模型训练上下文中的激活检查点。

重计算改善有效的FLOPS/byte，并可以将大型模型放入较小的内存量中。此外，重计算释放内存用于更大的批量大小，并以少量额外FLOPS换取内存流量的显著减少。

## PyTorch与算术强度 (PyTorch and Arithmetic Intensity)

在PyTorch中，许多这些想法会自动应用。如前所述，PyTorch编译器（在第13章和第14章讨论）可以自动融合逐元素操作链——甚至一些归约。它使用执行图级优化将数据保持在GPU上并尽可能重用。

因为它在底层使用cuDNN和cuBLAS等优化库，PyTorch在使用`torch.matmul`执行矩阵操作时会为您执行分块并使用共享内存。此外，PyTorch的`scaled_dot_product_attention`（SPDA）可能根据张量形状和dtype分派到FlashAttention、内存高效或cuDNN后端。要控制后端选择，使用`torch.nn.attention.sdpa_kernel(SDPBackend.FLASH_ATTENTION)`等。作为注重性能的开发人员，您应该了解这些优化以及如何验证它们何时被使用。

值得注意的是，虽然PyTorch可以识别和编译大多数操作，但一些非标准操作或自定义CUDA操作可能不会被融合。在这些情况下，可能仍需要手动优化，如融合、分块等。

> If you're writing PyTorch code, prefer fused operations and optimized library calls that perform multiple computations rather than long sequences of individual kernel launches. In practice, this means using high-level operations like torch.nn.functional activations or torch.matmul instead of writing many small elementwise kernels in Python. These libraries call efficient kernels for these types of high-level operations. And the compiler knows how to fuse them efficiently with surrounding operations.

PyTorch的嵌套张量或锯齿张量让您表示可变长度输入的批次而无需填充。每个嵌套张量将可变长度序列打包成单个高效的基础缓冲区。

NestedTensor暴露正常的张量接口，但它消除了不必要的零填充。因此，全局内存加载变得更加高效，因为获取的每个字节在计算中都是有用的。

嵌套张量对于具有可变长度序列的LLM很有用。使用嵌套张量时，注意力和批量矩阵乘法等操作将只从内存检索必要的数据。这将您的内核推向屋顶线模型上的计算受限状态，并有助于减少内存受限停顿。结果是更高的持续吞吐量——特别是在内存敏感的工作负载上。

> In practice, nested tensors require careful validation for operator coverage and performance characteristics. Support is workload dependent, and speedups are shape dependent. You can verify end-to-end benefits with representative sequence length distributions and attention patterns. Profile both memory traffic and kernel time.

简而言之，PyTorch暴露了各种机制来增加内核的算术强度。了解这些选项并决定什么最适合您的工作负载很重要。在现代GPU上增加算术强度的另一个有效方法是使用降低精度的Tensor Core。让我们接下来介绍这些机制。

## 混合精度和利用Tensor Core (Mixed Precision and Utilizing Tensor Cores)

现代NVIDIA GPU在Tensor Core中实现降低精度计算，如TF32、FP16、FP8、FP4和INT8。Blackwell中的每个SM都有一个256 KB的片上TMEM专用于Tensor Core数据。它还有一个专门的TMA单元，在全局内存和共享内存之间异步复制块。Tensor Core指令（例如`tcgen05.mma`）然后在共享内存和TMEM之间隐式移动操作数和累加器。这种设计以高吞吐量为Tensor Core供给数据并最小化停顿。Blackwell的基于TMEM的累加器有助于减少寄存器压力，相对于之前直接在寄存器中累加的GPU世代。

当正确使用时，这些功能可以通过将算术强度（每字节FLOPS）提高2倍、4倍甚至8倍，将曾经内存受限、张量密集的内核转变为完全计算受限的内核。您可以通过监控Nsight Compute中的屋顶线图和停顿统计来验证影响。Nsight Compute的光速分析显示内存受限停顿原因，如"Memory Throttle"和缓存未命中。当使用低精度格式的Tensor Core时，这些将显著下降。Nsight Compute集成了屋顶线图来交叉检查算术强度是否增加以将内核推向计算屋顶。

当您从FP32转向TF32、FP16、FP8或FP4等低精度Tensor Core内核时，Nsight Compute的线程束停顿指标通常显示内存相关停顿的减少，以及依赖或流水线停顿的相对增加。这表明算术强度增加，并从内存受限向计算受限执行转变。

### 使用TMEM和TMA为Tensor Core供给数据 (Feeding Tensor Cores with TMEM and TMA)

高吞吐量张量计算的核心是TMEM，每个SM的256 KB SRAM缓冲区。在高层次上，程序员不显式分配或管理TMEM。当您使用Tensor Core操作时，TMEM由硬件或库处理。TMEM如图9-6所示。

![图9-6 TMEM通过累加部分结果支持Tensor Core（而不是寄存器）](images/ch09/fig9-6.png)

> Figure 9-6. TMEM supports the Tensor Cores by accumulating partial results (instead of registers)

在底层，Blackwell使用与TMEM一起操作的tcgen05.mma指令进行操作数和累加器存储。CUTLASS和库内核通过内核配置和并行线程执行（PTX）汇编管理所需的分配和使用。因此，Transformer Engine使用TMEM存储部分结果。这减少了MMA对寄存器的依赖。

像CUTLASS这样的高级API自动为您处理所有这些复杂性。尽可能使用CUTLASS和其他高级库，因为CUTLASS使用tcgen05.* PTX指令，这些指令实现了Tensor Core矩阵操作和内存加载/存储接口。

每当您使用CUDA MMA内建函数或CUTLASS GEMM启动Tensor Core MMA操作时，实现通过共享内存和TMEM管理操作数移动。TMA在全局内存和共享内存之间流式传输块，Tensor Core指令在共享内存和TMEM之间隐式移动操作数。

Nsight Compute包括内置的屋顶线和光速分析，以确认您的内核在采用低精度Tensor Core路径后是否从内存受限转变为计算受限。

Tensor Core指令然后作为MMA流水线的一部分在共享内存和TMEM之间移动操作数。这在幕后发生，无需显式用户代码。这样，数据被暂存在Tensor Core需要的地方。要从全局内存执行此数据传输到共享内存，请使用TMA或带有CUDA Pipeline API（`<cuda/pipeline>`）中流水线的`cuda::memcpy_async`。在代码中，使用`cuda::memcpy_async`和CUDA Pipeline API实现简单的两阶段流水线如下所示：

```cpp
// two_stage_pipeline.cu

#include <cuda/pipeline>
#include <cooperative_groups.h>
namespace cg = cooperative_groups;

extern "C" __global__
void stage_ab_tiles(const float* __restrict__ globalA,
                    const float* __restrict__ globalB,
                    float* __restrict__ outC,
                    int tile_elems,
                    int num_tiles) {
     // Alignment / size guards for vectorized copies (runtime parameter)
    assert((tile_elems % (32 * 4)) == 0 &&
           "tile_elems must be multiple of 128 for float4 vectorization");
    // If you cannot guarantee 16B alignment or sizes, handle 
    // the tail/ragged edges with a fallback 4B loop.
  extern __shared__ float smem[];
  auto block = cg::this_thread_block();

  // Shared buffers for double buffering of A and B
  float* A0 = smem + 0 * tile_elems;
  float* A1 = smem + 1 * tile_elems;
  float* B0 = smem + 2 * tile_elems;
  float* B1 = smem + 3 * tile_elems;

  constexpr auto scope = cuda::thread_scope_block;
  constexpr int stages = 2;
  __shared__ cuda::pipeline_shared_state<scope, stages> pstate;
  auto pipe = cuda::make_pipeline(block, &pstate);

  // Prime the pipeline with tile 0
  pipe.producer_acquire();
  cuda::memcpy_async(block, A0, globalA + 0 * tile_elems,
                     cuda::aligned_size_t<32>{tile_elems * sizeof(float)}, pipe);
  cuda::memcpy_async(block, B0, globalB + 0 * tile_elems,
                     cuda::aligned_size_t<32>{tile_elems * sizeof(float)}, pipe);
  pipe.producer_commit();

  for (int t = 1; t < num_tiles; ++t) {
    // Stage the next A and B tiles
    pipe.producer_acquire();
    cuda::memcpy_async(block, (t & 1) ? A1 : A0,
                       globalA + t * tile_elems,
                       cuda::aligned_size_t<32>{tile_elems*sizeof(float)}, pipe);
    cuda::memcpy_async(block, (t & 1) ? B1 : B0,
                       globalB + t * tile_elems,
                       cuda::aligned_size_t<32>{tile_elems*sizeof(float)}, pipe);
    pipe.producer_commit();

    // Consume the previously staged tiles
    pipe.consumer_wait();
    float* prevA = (t & 1) ? A0 : A1;
    float* prevB = (t & 1) ? B0 : B1;
    // Perform compute using prevA and prevB
    pipe.consumer_release();
  }

  // Consume the final staged tiles
  pipe.consumer_wait();
  int last = (num_tiles - 1) & 1;
  float* lastA = last ? A1 : A0;
  float* lastB = last ? B1 : B0;
  // Perform compute using lastA and lastB
  pipe.consumer_release();
}
```

启动此内核时，将动态共享内存大小设置为4 × tile_elems × sizeof(float)，以便在共享内存中分配A0、A1、B0和B1。这种双缓冲模式确保一旦一个块驻留在共享内存中，Tensor Core就可以开始处理它。同时，`cuda::memcpy_async`并行地将下一个块获取到共享内存中。因为TMEM为Tensor Core指令提供片上数据缓冲区，共享内存提供暂存空间，您可以完全在片上暂存和重用FP16、FP8或FP4块。结果是当流水线调优得当且块和副本大小适当时，停顿更少。`cuda::memcpy_async`可以重叠从HBM到共享内存的传输并保持内核忙碌。这有助于将内存延迟隐藏在计算之后。

### TF32和自动混合精度（PyTorch）(TF32 and Automatic Mixed Precision (PyTorch))

虽然Tensor Core最初是为FP16设计的，但它们也支持TF32，它介于FP32和FP16之间。TF32使用8位指数（像FP32）和10位尾数（像FP16）。TF32在Tensor Core上执行，吞吐量远高于FP32在CUDA核心上的执行，同时保留FP32的指数范围。在PyTorch中，启用TF32只需在PyTorch代码中设置以下内容：

```python
import torch
torch.set_float32_matmul_precision('high') # {'highest'|'high'|'medium'}
```

一旦设置了这些标志，像`torch.matmul`和`torch.nn.Linear`这样的高级操作将自动作为TF32 Tensor Core内核执行，而不是在标准CUDA核心上以FP32执行。

除了TF32，PyTorch的自动混合精度（AMP）可以为每个操作选择最佳精度（FP16或BF16），并以FP32累加结果以保持稳定性。BF16有助于避免FP16的溢出问题。默认情况下，CUDA autocast使用float-16。只需传递`dtype=torch.bfloat16`即可在支持它的GPU上选择BF16。例如，您可以将模型代码包装在上下文管理器中，如下所示：

```python
with torch.amp.autocast("cuda", dtype=torch.bfloat16):
    output = model(input)
```

在底层，TorchInductor（在第13章和第14章介绍）自动融合这些精度转换以确保以下内容：大型GEMM操作在Tensor Core上以FP16或TF32运行，累加保持FP32以保持数值稳定性，小型"敏感"内核如层归一化和softmax以FP32运行，GradScaler防止FP16训练期间的下溢。请注意，BF16具有FP32指数范围。因此，使用BF16训练时通常不需要GradScaler。

在PyTorch中，这些混合精度决策集成到编译器中，因此您无需手动干预即可获得最佳dtype选择（例如，FP16/FP8用于计算，FP32用于累加）。这如图9-7所示，作为混合精度矩阵乘累加（MMA）。

![图9-7 混合精度和矩阵乘累加（MMA）](images/ch09/fig9-7.png)

> Figure 9-7. Mixed-precision and matrix multiply-accumulate (MMA)

这种自动混合精度流水线以最少的代码更改最大化算术强度。融合的Tensor Core内核通过在共享内存（例如操作数）和TMEM（例如累加器）中暂存和重用数据，最小化到HBM的往返。

当使用前面描述的结构化稀疏或极低精度（FP8/FP4）时，请确保保持足够大的批量大小或块粒度，以便TMEM和Tensor Core保持充分利用。小批量会产生开销，包括格式转换、稀疏索引处理、不规则内存模式等。这可能会降低实现的加速。

例如，当使用FP8或2:4稀疏时，批量大小为1可能几乎看不到好处，因为固定开销没有被分摊。相比之下，批量大小为128或256将充分利用TMEM流水线并产生接近峰值的吞吐量。

### BF16/FP16、FP8和FP4降低精度 (BF16/FP16, FP8, and FP4 Reduced Precision)

BF16/FP16（半精度）已被许多GPU世代支持，但现代GPU上的Tensor Core通常可以维持BF16/FP16峰值吞吐量的90%以上，约为FP32峰值吞吐量的4倍。这是因为每个周期，硬件并行发出许多BF16/FP16 FMA操作。

FP16训练使用比FP32更窄的5位指数，因此非常小的梯度值可能会下溢为零，除非您应用损失缩放。损失缩放在反向传播期间保持数值稳定性。这种缩放可以是静态的或动态的。

相比之下，BF16匹配FP32的8位指数范围，并原生避免下溢。因此，它很少（如果有的话）需要损失缩放。这简化了混合精度工作流程，通常在现代GPU上提高训练精度。

> BF16 is typically preferred for training on modern GPUs as it can maintain accuracy comparable to FP32 without the complexity of loss scaling that FP16 demands.

为了进一步提高吞吐量，您可以使用FP8。通过将16位权重减少50%到8位，您将内存流量减半——并使每个HBM事务加载的权重数量翻倍。在实践中，带有FP32或TF32累加的FP8矩阵乘法实现BF16/FP16 TFLOPS的2-3倍——假设模型由于量化误差导致的精度轻微损失仍然可以接受。

为了解决极低精度的精度问题，Transformer Engine支持FP8以及NVIDIA的4位NVFP4格式和微缩放。NVFP4应用两级缩放，结合每微块缩放和更高级别的缩放，使模型在使用4位存储权重时能够保持精度。此外，Blackwell B200的NVFP4的激进微缩放量化和量化提供10 petaFLOPS（密集），而FP32峰值约为80 teraFLOPS（密集）。这是每个权重约两个数量级更高的理论吞吐量加速。Blackwell的B300（Ultra） boasting 15 petaFLOPS（密集）的NVFP4计算能力，比B200高50%。

如果您的模型在校准后能够容忍精度下降，NVFP4内核可以在支持的硬件上提供远高于FP32的吞吐量，但每个模型必须验证精度。

由于精度如此之低，每个SM的256 KB TMEM可以容纳大型FP4块（例如256 × 256），这进一步增加了片上重用并提高了性能。请注意，所有低精度→累加转换自动发生。内核从HBM读取FP4输入，Tensor Core执行FP4 × FP4乘法，MMA API将结果累加到BF16/FP16或FP32累加器中。

每次精度下降都会使每字节操作数量翻倍或翻四倍，从而增加算术强度。当TMEM/TMA重叠内存和计算时，这些低精度格式将以前内存受限的内核转变为完全计算受限的内核。这充分利用了现代GPU中每GPU多PFLOPS的Tensor Core引擎。

### INT8降低精度和用于推理的DP4A指令 (INT8 Reduced Precision and DP4A Instructions for Inference)

LLM推理用例通常可以容忍现代GPU支持的降低精度INT8量化，使用常规CUDA核心上的DP4A（SIMD点积）指令和Tensor Core上的整数矩阵乘累加（MMA）指令。在指令级别，DP4A每条指令执行四次INT8乘累加（MAC）操作，而FP32每条指令执行一次融合乘加（FMA）。

因为INT8的权重流量每个元素只有一字节而不是FP32的四字节，权重的内存流量下降75%。由于更高的峰值INT8 Tensor Core吞吐量和减少的内存流量，INT8推理工作负载可以显著优于FP32。这是因为每个GPU在使用INT8权重时每秒可以从内存处理大约多4倍的数据。这得益于TMEM和TMA完美重叠数据和计算——并尽可能高效地为Tensor Core供给数据。

### Transformer Engine和TMEM深入探讨 (Transformer Engine and TMEM in Depth)

现代NVIDIA GPU包括Transformer Engine，它结合了Tensor Core对低精度格式的硬件支持与用于缩放和转换的软件运行时。cuBLASLt、cuDNN、CUTLASS或OpenAI的Triton中的内核执行cp.async指令或TMA传输到共享内存。Tensor Core指令然后在共享内存和TMEM之间隐式移动操作数。

请记住，TMEM是Transformer Engine和Tensor Core用于存储结果的每个SM 256 KB SRAM缓冲区（而不是寄存器）。在实践中，您从不显式分配TMEM。它全部由硬件处理。例如，当调用Tensor Core的MMA操作时，硬件处理所有内存分配和数据传输。

使用MMA指令，每个线程束直接驱动Tensor Core执行高吞吐量混合精度MMA操作。这些操作管理片段加载、寄存器映射和混合精度MMA操作。

> As of this writing, PyTorch's INT8 quantization support is provided through TorchAO and vendor backends. Quantized modules run using dedicated INT8 kernels. Using cuBLASLt or CUTLASS for low-level INT8 GEMM can ensure Tensor Core utilization.

每当您启动基于Tensor Core的内核或GEMM库函数（例如CUTLASS）时，实现通过共享内存和TMEM自动管理操作数移动。这使Tensor Core充满准备好处理的块。（请注意，应用程序代码不直接分配TMEM。）

Transformer Engine工作流程很简单。首先，您的内核发出MMA调用或启动CUTLASS GEMM。接下来，Transformer Engine的固件安排TMA（或`cuda::memcpy_async`）将权重和激活从HBM复制到共享内存（SMEM）。Tensor Core指令（例如`tcgen05.mma`）然后在MMA流水线期间在SMEM和TMEM之间隐式移动操作数。理想情况下，权重为FP8或FP4，激活在可能时转换为FP8/FP4——否则，激活可以保持FP16/FP32格式。

Tensor Core MMA操作以低精度执行，例如FP8 × FP8带有更高精度累加，或FP16 × FP16带有FP32累加。部分和以更高精度（例如BF16、FP16、FP32）累加在TMEM中，具体取决于内核。累加器状态驻留在TMEM中。此状态使用tcgen05加载和存储接口访问。硬件透明地管理这些移动。

如果您构建自定义块循环，您可以使用`cuda::memcpy_async`和CUDA Pipeline API重叠数据移动与Tensor Core计算，如下代码所示：

```cpp
#include <cuda/pipeline>
#include <cooperative_groups.h>
namespace cg = cooperative_groups;

extern "C" __global__
void double_buffer_a(const float* __restrict__ globalA,
                     int tile_elems,
                     int numTiles) {
  __shared__ float tileA0[TILE][TILE];
  __shared__ float tileA1[TILE][TILE];

  auto block = cg::this_thread_block();

  constexpr auto scope = cuda::thread_scope_block;
  constexpr int stages = 2;
  __shared__ cuda::pipeline_shared_state<scope, stages> pstate;
  auto pipe = cuda::make_pipeline(block, &pstate);

  // Prime pipeline with tile 0
  pipe.producer_acquire();
  cuda::memcpy_async(block,
                     &tileA0[0][0],
                     globalA + 0 * tile_elems,
                     cuda::aligned_size_t<32>{tile_elems * sizeof(float)}
                     pipe);
  pipe.producer_commit();

  for (int t = 1; t < numTiles; ++t) {
    // Stage next tile into the alternate buffer
    pipe.producer_acquire();
    float* nxtA = (t & 1) ? &tileA1[0][0] : &tileA0[0][0];
    cuda::memcpy_async(block,
                       nxtA,
                       globalA + t * tile_elems,
                       cuda::aligned_size_t<32>{tile_elems * sizeof(float)}
                       pipe);
    pipe.producer_commit();

    // Consume the previously staged tile
    pipe.consumer_wait();
    float* curA = ((t - 1) & 1) ? &tileA1[0][0] : &tileA0[0][0];
    pipe.consumer_release();
  }

  // Consume the final staged tile
  pipe.consumer_wait();
  float* lastA = ((numTiles - 1) & 1) ? &tileA1[0][0] : &tileA0[0][0];
  // Use lastA with your compute
  pipe.consumer_release();
}
```

因为TMEM是Tensor Core指令使用的专用片上缓冲区，数据保持靠近计算单元。当Tensor Core处理当前块时，`cuda::memcpy_async`将下一个块从HBM流式传输到共享内存。

这种重叠有助于隐藏内存延迟，并可以在流水线调优当时保持Tensor Core忙碌。Transformer Engine、TMEM和TMA之间的这种协作可以显著提高算术强度，并在优化情况下接近光速效率。

> While load and store operations are synchronous with respect to the calling warp, the overlap of compute and data movement should come from the CUDA Pipeline API. Used with pipeline primitives like wait/release, cuda::memcpy_async maps to the Tensor Memory Accelerator (TMA) and should always be preferred for bulk tensor transfers. Reserve cp.async for niche cases that TMA cannot express. However, these are rare. You should also make sure that copies complete before using the data.

## 使用CUTLASS实现最佳算术强度和Tensor Core性能 (Using CUTLASS for Optimal Arithmetic Intensity and Tensor Core Performance)

利用这些优化最简单的方法之一是使用NVIDIA的CUTLASS库。使用CUTLASS，您只需编写一个模板化调用，它将自动应用许多高级优化。

CUTLASS应用的一些优化包括共享内存分块、异步内存传输，以及在TMEM的每个SM 256 KB缓冲区帮助下的双缓冲。这样，您的Tensor Core无需任何手动内核调优即可以接近峰值的吞吐量运行。

> CUTLASS also implements warp specialization, which is a high-performance GPU optimization technique that we'll discuss in the next chapter.

例如，假设您想计算GEMM，C = A * B，使用半精度输入和半精度输出，适当地以FP16或FP32累加。与其编写手工调优的MMA循环，您可以简单地包含CUTLASS并实例化模板，如下代码所示：

```cpp
#include <cutlass/numeric_types.h>
#include <cutlass/gemm/device/gemm.h>

using Gemm = cutlass::gemm::device::Gemm<
  cutlass::half_t,  // A (FP16)
  cutlass::layout::RowMajor,
  cutlass::half_t,  // B (FP16)
  cutlass::layout::ColumnMajor,
  cutlass::half_t,  // C / output (FP16)
  cutlass::layout::RowMajor,
  float, // accumulator (FP32 accumulate)
  cutlass::arch::OpClassTensorOp,
  cutlass::arch::Sm100 // e.g., Blackwell B200
>;

// ... (allocate device pointers A_d, B_d, C_d, 
// set up dimensions M,N,K, and strides lda, ldb, ldc) ...

Gemm gemm_op;
cutlass::Status status = gemm_op(
    { M, N, K },              // GEMM shape
    float(1.0f)               // alpha
    A_d, lda,                 // A pointer + leading dimension
    B_d, ldb,                 // B pointer + leading dimension
    float(0.0f),              // beta
    C_d, ldc                  // C pointer + leading dimension
);
```

当您编译并运行此代码时，CUTLASS自动执行几项关键操作。首先，CUTLASS选择块来平衡寄存器压力、共享内存容量和Tensor Core利用率。在现代GPU上，TMEM与共享内存和L1并存。CUTLASS在共享内存中暂存块，并使用与TMEM交互的Tensor Core指令存储累加器数据。块形状是根据经验和每个内核选择的。例如，它可能选择128 × 128或256 × 128等块大小。这些将适合TMEM的每个SM 256 KB缓冲区，并在整个Tensor Core计算期间保持在片上。

根据精度，256 × 512块将用尽每个SM 256 KB TMEM预算，因为256 × 512元素 × 每元素2字节 = 256 KiB。而256 × 256元素 × 每元素4字节 = 256 KB。较大的块提高每块吞吐量，但减少每个SM的并发块数量。这可能导致较小GEMM上的利用率不足。相比之下，非常小的块以并行性为代价牺牲算术强度。

CUTLASS然后发出异步内存副本（cp.async或TMA），将每个块从DRAM流式传输到共享内存。cp.async指令将数据从全局内存暂存到共享内存，而不使用每线程寄存器（或可选地L1缓存），如图9-8所示。缓存行为使用cp.async修饰符或使用TMA进行批量张量传输来控制。

![图9-8 使用异步内存复制指令（cp.async）从全局内存加载数据到共享内存，不涉及寄存器文件，可选地也不涉及L1缓存](images/ch09/fig9-8.png)

> Figure 9-8. Using the asynchronous memory copy instruction (cp.async) to load data from global memory into shared memory without involving the register file and optionally the L1 cache

CUTLASS使用cp.async或TMA（`cp.async.bulk.tensor`）将块从全局DRAM暂存到SMEM。Tensor Core `tcgen05.mma`指令然后从SMEM读取操作数并将结果隐式累加到TMEM。这在共享内存中创建了一个软件管理的暂存区，用于双缓冲。这样，当Tensor Core正在处理当前块时，TMA已经在将下一个块获取到共享内存中。

使用CUDA Pipeline API和线程束专用计算阶段（在下一章讨论），CUTLASS保持所有Tensor Core流水线忙碌。它以您指定的精度累加部分和（例如，当输入为FP16或FP8时为FP32）以确保数值保真度——然后将结果从TMEM以合并方式写出到共享或全局内存。

> CUTLASS also leverages thread block clusters when beneficial by tiling across multiple SMs for even larger effective tiles. We'll cover thread block clusters in the next chapter.

因为所有这些复杂性都被隐藏，CUTLASS为您提供了一个即插即用的高性能GEMM内核，与手工调优的MMA内核相匹配——在整体Tensor Core利用率和性能方面通常与手工编写版本相差几个百分点，如表9-1所示。

**表9-1. 手工调优MMA与CUTLASS内核性能和资源使用比较**

| 指标 | 手工调优MMA内核 | CUTLASS GEMM |
|------|-----------------|--------------|
| Tensor Core利用率 | 98% | 98% |
| 每线程寄存器数 | ~52 | ~60（略高） |
| 每线程块（CTA）共享内存 | ~2 KB | ~4 KB |
| 开发工作量 | 高 | 低（简单模板配置） |

> Note: The numeric values in all metrics tables are illustrative to explain the concepts. For actual benchmark results on different GPU architectures, see the GitHub repository.

NVIDIA持续更新CUTLASS和cuBLAS等库，以利用最新的硬件功能，如FP8、FP4、线程块集群对、TMEM等。使用这些库使您无需为每个新GPU世代重写新内核。切换到新GPU架构时，请务必检查新版本的CUTLASS。

## 用于微优化的内联PTX和SASS调优 (Inline PTX and SASS Tuning for Microoptimizations)

对于那些愿意超越C++进入低级微优化的人，CUDA允许内联并行线程执行（PTX）代码和SASS（NVIDIA的汇编语言），以挖掘可能被遗漏的最后一点性能。

这确实是高级领域，因为CUDA编译器已经非常擅长优化。但在某些极端情况下，您可以手工调度汇编指令——或使用特殊用途指令——在非常特定的情况下获得小百分比的性能提升。

使用PTX和流式汇编器（SASS），您还可以启用高级CUDA语言尚未公开的功能。现代GPU通常不会引入激进的新汇编指令，但它们确实提供自定义调优的机会。例如，您可以调整GPU缓存策略、修改CPU-GPU统一内存访问的协调，以及实现其他细粒度微优化。

> PTX ("pee-tex") is a low-level parallel-thread execution virtual machine and exposes the GPU as a parallel computing device. It provides the programming model and instruction for NVIDIA GPUs. A high-level compiler (e.g., CUDA C++) generates PTX instructions, which are translated into native target-architecture instructions. SASS is the low-level assembly language that actually executes natively on NVIDIA GPU hardware.

例如，考虑一段代码，您知道特定的指令序列将是最优的，但编译器没有生成特定的序列。常见场景包括使用没有直接CUDA内建函数的GPU指令、在特定访问上应用内存加载修饰符（缓存提示）、在精确点插入内存栅栏或屏障，或手动重新排序指令以避免流水线停顿。

另一个用途是读取特殊寄存器或状态，如SM ID、线程束通道ID等，这些可能没有高级API。内联PTX允许您使用`asm()`语句在CUDA C++代码中嵌入汇编。您可以通过指定汇编代码的输入和输出，将C++和PTX混合。然后编译器将您的PTX指令合并到最终的SASS中。

让我们看一个使用内联PTX指令将全局内存地址预取到L2缓存的简单示例。在这里，我们使用PTX指令`cp.async.bulk.prefetch.global`进行内核端预取：

```cpp
__global__ void PrefetchExample(const float *in, float *out) {
    // ... assume idx is our thread's data index
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    // Manually prefetch the next cache line (128B) of in[] into L2:
    // Prefetch 128B from global to L2.
    // Address must be 16B-aligned
    // and size is a 16B multiple.
    asm volatile("cp.async.bulk.prefetch.L2.global [%0], %1;"
                 :: "l"(in + idx + 32), "n"(128));
    float x = in[idx];
    // (do some work here before using in[idx+32] to give time for prefetch)
    out[idx] = x;
}
```

在这个代码片段中，内联PTX `cp.async.bulk.prefetch.L2.global [%0]`使用我们提供的地址操作数（`in + idx + 32`字节，即32个float之后）并向L2发出预取。我们将其标记为volatile以确保编译器不会优化掉它。

这些PTX指令将被注入到机器代码中。像这样使用内联汇编将给我们非常细粒度的控制。例如，我们可以预取到L2或L1（通过使用`.L1`）或选择距离（在这个例子中是32个float之后）。

这本质上是`__prefetch_async`可能编译成的结果。更一般地，我们可以使用内联PTX控制正常加载的缓存行为。例如，我们可能编写`asm("ld.global.cg.f32 %0, [%1];" : "=f"(val) : "l"(ptr))`来使用`.cg`（"cache global"）修饰符加载float。

在某些架构上，这意味着我们希望在L2中缓存数据但绕过L1缓存。如果我们知道某个访问正在驱逐L1并且我们只想使用L2，这可能会有帮助。通常，编译器的选择可能默认在L1中缓存（`.ca`），但我们可以使用PTX覆盖编译器的决定。

> For L2 prefetch on modern architectures, use cp.async.bulk.prefetch.tensor.L2 where available. This is preferred over using undocumented built-ins. Regardless, it's useful to know that this capability exists.

内联PTX有帮助的另一个领域是指令调度。默认情况下，编译器将以其认为最优的顺序发出指令。但您可能会发现一个情况，您希望更有效地混合操作。

例如，假设您有两个独立的内存加载，然后是这两个结果的两次使用。编译器可能发出load1，然后use1，然后load2，然后use2。但也许更好的指令调度是先执行load1和load2（背靠背），然后使用两个结果。这可以重叠内存延迟。

通过为加载编写内联PTX，您可以强制它们提前发生，然后进行计算。这是前面讨论的手动增加ILP的一种形式。在实践中，编译器在这里已经做得很好，因为现代编译器试图用其他独立指令填充加载延迟。但内联PTX和SASS汇编可以给您确定性。

在现代CPU-GPU超级芯片上共享CPU和GPU内存的情况下，如果您正在管理GPU轮询CPU更新的内存位置的工作负载，这种细粒度控制可能很有用。在这里，您可以使用适当的内存栅栏如`membar.sys`或`__threadfence_system()`以及加载和存储上所需的缓存操作符，以确保在预期范围内的一致性。这是高级CUDA可能不直接暴露的内容。

> PTX is generally forward compatible; however, SASS assembly will change per GPU architecture generation.

您还可以使用内联PTX利用特殊寄存器。例如，虽然没有C++内建函数用于SM ID（线程块运行的SM），您可以执行`asm("mov.u32 %0, %smid;" : "=r"(smid))`来获取SM ID。这种灵活性对于调试和工作分区很有用。

一些开发人员在持久内核中使用`%smid`，例如让每个SM只有一个块做某些工作。这有效地执行手动SM分区，这超出了CUDA C++ API提供的功能。

如果您的代码已经在算法级别进行了良好优化，内联PTX/SASS的收益通常只是增量的，在大多数情况下约为几个百分点。例如，在内存受限的内核中，您可以仔细展开和调度指令以减少加载到使用的延迟气泡，并通过使用PTX使用两个独立的加载流看到大约5%-10%的加速。在这种情况下，编译器可能更保守。

在计算受限场景中，您可能使用内联汇编来使用更快的数学指令而不是更精确的指令。CUDA为此提供快速数学内建函数，包括`__sinf()`。

在C++内建函数可用之前，编写矩阵乘法内核的开发人员有时会嵌入PTX指令来使用Tensor Core。今天，我们有更高级的内建函数用于此目的。但简而言之，汇编让您在知道硬件功能后立即利用它们——无需等待CUDA支持它们。

Nsight Compute可以帮助指导汇编调优。通过检查"SASS throughput"指标和"Warp Stall Reasons"，您可能会发现大量由于内存依赖导致的停顿。您可以尝试重新排序前面提到的加载。

更改后，您希望看到更少的"Stall Memory Dependency"——以及可能更高的指令发射率，以实现每周期执行更多指令。请注意，汇编调整是劳动密集型的——并且可能降低代码可移植性。

通常只对非常热门的内循环值得这样做，在这些循环中您已经穷尽了高级优化。此外，GPU世代的任何更改都可能需要重新调优并验证您的假设仍然成立，包括内存延迟、缓存行为等。

为了说明潜在的微优化，考虑一个场景，内核已经相当优化但有一个剩余瓶颈：一个进行整数索引计算和内存加载的紧密循环。

您可以使用内联PTX在一条指令中使用`mad.wide.u32`（base + index * stride）计算字节地址。接下来，作为`ld.global.cg`发出加载以绕过L1进行此流式访问模式。结果是循环使用更少的指令并避免L1驱逐。在这种情况下，我们将该内核从1.07 ms到1.0 ms挤出了约7%的加速。表9-2总结了优化CUDA C++与手工编写PTX（带有手动调度和缓存提示）的假设前后比较。

**表9-2. 优化CUDA C++与带有手动调度和缓存提示的手工调优PTX比较**

| 版本 | 线程束停顿（内存） | 发射IPC | 内核时间 | 加速 |
|------|---------------------|---------|----------|------|
| 优化C++（编译器调度） | 35%的周期 | 1.5 | 1.07 ms | 1.0× |
| 手工调优PTX（手动调度和提示） | 20%的周期 | 1.6 | 1.00 ms | 1.07× |

在这里，我们看到调优后，内核的内存停顿通过重叠加载减少了，缓存提示减少了一些延迟。此外，我们增加了ILP和每周期指令数（IPC）。这使整体内核执行时间净改善7%。

这些数字与在已经优化的内核上手动汇编的预期一致。在某些情况下，如果编译器做出了糟糕的选择，收益可能更大——您可以实现修复。否则，如果编译器已经选择了最优实现，收益基本为零。使用不正确的汇编排序也可能损害性能，因此必须实验并分析每个更改。

内联PTX和SASS汇编可以被视为最后的优化工具。它以复杂性为代价提供最终控制。建议在需要CUDA C++中不可访问的硬件功能或指令时使用此最后资源——或者在您已经精确定位编译器调度可以改进的一小段代码时使用。示例包括自定义内存访问模式（缓存提示、预取）、细粒度同步或栅栏，以及在正式支持之前利用新指令。

应用内联汇编时，通过分析验证影响尤为重要。您希望看到停顿原因或指令计数按预期减少。此外，您应该将此类代码隔离并良好记录。它很可能需要为新GPU架构更新——特别是如果您使用SASS汇编，它并不总是向前兼容的。

> While PTX is more stable than SASS, some hardware changes may still require you to update the inline PTX for performance reasons.

内联PTX/SASS调优是专家级微调，用于削减额外延迟或强制执行特定调度。它可以产生适度的加速并启用某些自定义行为，但应该在穷尽所有其他高级优化之后进行。例如，您可能希望为运行数百万次的关键循环手工制作汇编。至少，这是了解硬件如何执行代码的有效方式。

简而言之，谨慎使用内联PTX/SASS并勤奋分析。收益是真实的但通常是增量的。维护成本要高得多。对于大多数用例，依赖CUDA的内置优化——或高度调优的库如CUB和Thrust——可能就足够了。但知道如果需要，您可以下降到汇编并获得对GPU的完全控制是很好的。

### DeepSeek使用内联PTX进行内存分配优化 (DeepSeek's Use of Inline PTX for Memory Allocation Optimization)

自定义PTX的一个著名例子来自DeepSeek的DeepEP专家并行库。该库使用定制PTX指令`ld.global.nc.l1::no_allocate.l2::256b`来优化全局内存访问，方法是绕过L1缓存分配、保留关键数据并利用256字节L2缓存块。这非常适合将大型数据集直接流式传输到L2缓存，而不干扰L1缓存中的频繁访问内存操作。

此指令不是NVIDIA官方PTX ISA规范的一部分，而是被DeepSeek工程师"文档外"发现的，用于在其受美国出口限制的Hopper GPU H800变体上微调缓存行为。

让我们分解`ld.global.nc.l1::no_allocate.l2::256b` PTX指令。`ld.global.nc`前缀发出非一致性（nc）全局内存加载（`ld.global`），而修饰符`l1::no_allocate`和`l2::256b`指示硬件避免在L1缓存中分配数据（`l1::no_allocate`）。相反，它一次获取256字节到L2（`l2::256b`）。

通过绕过L1，加载可以直接将大数据块流式传输到L2，而不驱逐频繁使用的L1驻留数据。当您有需要在L1中保持低延迟内存访问的热工作集时，这至关重要。

在实践中，专家并行全对全通信内核等流式工作负载从此方法中受益，因为它们通常每次调度准确读取一次大型连续缓冲区。如果这些加载通过L1，它们可能会驱逐SM仍在主动使用的早期缓存行。

通过以256字节对齐块直接获取到L2，该指令减少不必要的L1流量。这有助于为内存受限的通信和计算操作保持高吞吐量。

然而，以这种方式使用PTX带有一些风险，因为`ld.global.nc.l1::no_allocate.l2::256b`不保证在GPU世代间保持稳定。因此，依赖它的代码可能在未来的架构上中断或产生不正确的结果。

DeepSeek的DeepEP设置甚至包括构建时标志`DISABLE_AGGRESSIVE_PTX_INSTRS=1`，以便在出现兼容性问题时禁用这些激进指令。虽然DeepEP的定制PTX技巧可以产生显著的加速，但内联PTX/SASS应谨慎使用，并在更新到新GPU架构时彻底测试。

## 关键要点 (Key Takeaways)

您看到了如何通过将工作从慢速全局内存移动到更快的片上资源和计算单元来揭示和消除GPU内核瓶颈。通过遵循分析、诊断、优化和重新分析的循环，您可以将内核从利用不足或内存受限转变为计算饱和、高吞吐量的例程。这些技术将帮助利用GPU的全部能力：

**通过分块和融合增加算术强度**
要提高每传输字节的FLOPS，使用多级分块将数据暂存到共享内存和寄存器中。例如，将32 × 32子矩阵加载到SMEM，以便每个元素在许多FMA中重用。通过融合逐元素操作组合连续内核，使中间结果保持在片上。通过CUDA Pipeline API的`cuda::memcpy_async`利用软件预取进行异步内存加载，将数据移动与计算重叠。这将隐藏DRAM延迟。

**利用混合精度、Tensor Core和Transformer Engine**
从FP32降到TF32/BF16/FP16/FP8/FP4将权重流量减少2倍到8倍。这相应地提高算术强度。在PyTorch中，您可以使用`torch.set_float32_matmul_precision('high')`和`torch.cuda.amp`启用混合精度。在现代GPU上，TMEM和TMA引擎将块流式传输到Tensor Core。现代GPU还提供Transformer Engine来专门为AI工作负载利用这些低精度。这可以进一步提高吞吐量。

**利用CUTLASS实现高性能GEMM和融合内核**
与其手工编码MMA循环，实例化CUTLASS GEMM模板以自动管理双缓冲、TMEM暂存和Tensor Core流水线。这将产生性能在手工调优内核几个百分点内的内核。像cuBLAS和TorchInductor这样的高级框架已经依赖CUTLASS。只有在需要非标准布局或独特融合模式时才需要自定义。

**PyTorch特定最佳实践**
优先使用内置张量操作，如`torch.matmul`、融合注意力和嵌套张量，因为PyTorch通常随时间从CUDA继承Tensor Core和其他硬件优化。如果您为PyTorch编写自定义内核扩展，沿用相同策略：最小化寄存器使用、对齐数据以进行合并内存加载、并针对Tensor Core。这确保您的内核与原生内核持平。

通过遵循系统化的分析、诊断和应用目标优化的工作流程，从占用率调优和线程束级改进到ILP以及通过分块、融合和Tensor Core实现高算术强度，您可以将内存受限的GPU内核转变为计算受限的内核。这可以提供大幅加速，在强内存受限工作负载中，有时甚至是数量级的改进。

## 结论 (Conclusion)

在本章中，您学习了与高级内存和计算硬件功能相关的优化技术，如TMA、TMEM、Transformer Engine和Tensor Core。相同的原则从单个GPU扩展到多GPU集群。首先，使用系统级分析器（例如NVIDIA Nsight Systems）关联CPU活动、GPU内核和跨多个GPU的NVLink/NVSwitch流量。然后使用Nsight Compute深入了解每个内核的详细信息。

> In this chapter, you learned about optimization techniques related to advanced memory and compute hardware features such as TMA, TMEM, Transformer Engine, and Tensor Cores. The same principles scale from single GPUs to multi-GPU clusters. First, use a system-level profiler (e.g., NVIDIA Nsight Systems) to correlate CPU activity, GPU kernels, and NVLink/NVSwitch traffic across multiple GPUs. Then use Nsight Compute for per-kernel deep dives.

通过系统化分析、消除主导停顿并掌握高级硬件功能，您可以将内存受限的工作负载转变为计算受限的工作负载——通常产生大幅加速，包括在强内存受限路径上的数量级改进。

即使使用超大规模多GPU系统，如NVIDIA的GB200/GB300 NVL72及其高NVLink/NVSwitch带宽，每个GPU的算术强度优化仍然是消除大多数瓶颈的关键。每字节工作太少的内核将需要太多内存移动，使互连饱和，并耗尽内存带宽。

这种互连和内存带宽饱和将在我们的内核能够充分利用许多互连GPU（例如NVL72的情况下为72个）的计算能力之前很久就发生。因此，增加内核算术强度是高效扩展多GPU训练和推理工作负载的关键。

在下一章中，我们将继续深入探讨持久内核、兆内核、线程束专用化、协作组和线程块集群等技术。这些思想是大多数现代LLM运行时优化的基础，因此了解它们的实现——以及如何在自己的低级系统优化工作中应用它们很重要。
