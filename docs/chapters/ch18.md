# 第18章 高级Prefill-Decode和KV缓存调优 (Advanced Prefill-Decode and KV Cache Tuning)

本章建立在第17章的基础上，深入探讨推理prefill和decode阶段的高级优化。我们将在高级扩展策略的基础上，介绍底层技术，包括单decode"超级内核"（mega kernels）、智能KV缓存调优和跨GPU共享、提示状态的快速GPU到GPU传输、自适应资源调度，以及prefill和decode工作节点之间的动态路由。

我们还将重点介绍硬件和软件创新，它们提供了新的性能和效率水平。通过应用这些技术，您可以显著减少decode延迟，提高每GPU吞吐量，并在大规模场景下满足严格的延迟SLO。

## 优化的Decode内核 (Optimized Decode Kernels)

到目前为止，我们一直专注于高级系统和集群优化策略。在增加超大规模推理时需要考虑的另一组技术是底层内核和内存管理调优——特别是对于decode阶段。

decode阶段是分布式的，通常受内存限制。这促使研究人员和从业者尽可能快地优化decode阶段——并针对特定硬件进行调优。该领域的两个显著创新是FlashMLA（DeepSeek）、ThunderMLA（Stanford）和FlexDecoding（PyTorch）。这些技术专门针对LLM工作负载中常见的可变序列场景下transformer的多头注意力效率进行优化。接下来让我们逐一介绍。

> Until now, we have been focused on high-level system and cluster optimization strategies. Another set of techniques to consider when increasing ultrascale inference is low-level kernel and memory management tuning—especially for the decode phase. The decode phase is distributed and often memory bound. This has motivated researchers and practitioners to make the decode phase as fast as possible—and tuned for specific hardware. Two notable innovations in this space are FlashMLA (DeepSeek), ThunderMLA (Stanford), and FlexDecoding (PyTorch). These specifically target the transformer's multihead attention efficiency during decode in variable-sequence scenarios common in LLM workloads. Let's cover each of these next.

### FlashMLA (DeepSeek)

Flash Multi-Latent Attention，即FlashMLA，是DeepSeek推出的优化解码内核。它专门针对单token解码步骤，这本质上是用于生成下一个token的transformer层的前向传递。FlashMLA通过融合操作和更好地利用GPU内存层次结构来加速解码。

FlashMLA（decode）之于推理，就像FlashAttention（prefill）之于训练。它减少了内存访问开销和延迟。使用FlashMLA，与标准内核相比，您可以显著减少decode阶段的延迟。

FlashMLA通过将多个注意力操作融合为一个来提高算术强度。这样，它可以在一次融合内核启动中处理多个头和多个时间步。这通过让数学单元保持忙碌（尽管批次大小较小）来提高decode期间的GPU利用率。

图18-1显示了在Hopper H100 GPU上，MLA与其他注意力实现（如分组查询注意力（GQA）和多查询注意力（MQA））相比在算术强度方面的改进。（注：Blackwell通过更高的TFLOPs和HBM带宽将两条roofline都向上移动。）

![图18-1 MLA接近计算受限区域（在NVIDIA Hopper H100架构上测量）](../assets/images/ch18/fig18_01.png)

> Figure 18-1. MLA approaches the compute-bound regime (measured on the NVIDIA Hopper H100 architecture)

FlashMLA的推出意义重大，因为它表明decode阶段的瓶颈——内存带宽和内核启动开销——可以被减少——即使是在次优的GPU硬件上。它减少了单独的GPU内核启动次数，并优化了内存访问模式——在受限硬件上为解码任务挤出尽可能多的性能。

DeepSeek开源的FlashMLA实现已经可用，并正在被采用。SGLang和vLLM都为DeepSeek模型提供一流支持。因此，您应该评估FlashMLA以在不改变高级架构的情况下增加每token解码吞吐量。

由于DeepSeek开源的FlashMLA已集成到现代推理服务系统中，您应该探索它作为增加每个decode工作节点吞吐量的方式——或减少每token延迟——而无需任何高级架构更改。

> FlashMLA (decode) is to inference what FlashAttention (prefill) is to training. It reduces memory access overhead and latency. With FlashMLA, you can achieve large latency reductions for the decode phase compared to standard kernels. FlashMLA increases arithmetic intensity by fusing multiple attention operations into one. This way, it can process multiple heads and multiple time steps in one fused kernel launch. This increases GPU utilization during the decode by keeping the math units busy despite small batch sizes. Figure 18-1 shows the improvement in arithmetic intensity for MLA compared to other attention implementations like grouped-query attention (GQA) and multiquery attention (MQA) on a Hopper H100 GPU. (Note: Blackwell shifts both rooflines upward with higher TFLOPs and HBM bandwidth.) The introduction of FlashMLA was significant because it showed that the decode phase's bottlenecks, memory bandwidth, and kernel-launch overhead can be reduced—even on suboptimal GPU hardware. It reduced the number of separate GPU kernel launches and optimized memory access patterns—squeezing as much performance out of constrained hardware as possible for decoding tasks. DeepSeek's open sourced FlashMLA implementation is available and seeing adoption. SGLang and vLLM both provide first-class support for DeepSeek models. As such, you should evaluate FlashMLA to increase per-token decode throughput without changing higher-level architecture. Since DeepSeek's open sourced FlashMLA is integrated into modern inference serving systems, you should explore it as a way to increase the throughput of each decode worker—or reduce latency per token—without any higher-level architectural change.

### ThunderMLA (Stanford)

在FlashMLA的基础上，斯坦福大学的研究人员推出了ThunderMLA，这是一个完全融合的注意力解码"超级内核"，专注于解码和调度（而不是融合完整的前馈块）。这个"超级内核"通过将多次内核启动合并为一次，以及合并中间内存写入，减少了启动开销和尾部效应。ThunderMLA报告称，在不同工作负载下，解码吞吐量比FlashMLA快20-35%。

ThunderMLA的关键思想是，当解码不同长度的序列时，使用细粒度调度和融合操作可以避免尾部效应，即一些序列较早完成，而其他序列让GPU部分空闲。ThunderMLA即使某些解码流较早完成，也能让GPU保持忙碌。它通过使用融合方法动态打包和处理剩余流来实现这一点。

> Building on FlashMLA, researchers at Stanford introduced ThunderMLA, a completely fused attention decode "megakernel" that focuses on decoding and scheduling (rather than fusing the full feed-forward block.) This "megakernel" reduces launch overhead and tail effects by combining multiple kernel launches into one—as well as consolidating intermediate memory writes. ThunderMLA reports 20–35% faster decode throughput compared to FlashMLA across different workloads. The key idea of ThunderMLA is that when decoding sequences of different lengths, using fine-grained scheduling and fused operations can avoid the tail effect in which some sequences finish earlier while others leave the GPU partially idle. ThunderMLA keeps the GPU busy even if some decode streams complete earlier. It does this by dynamically packing and processing the remaining streams using its fused approach.

这些优势在具有更大L2缓存和更快注意力原语的现代GPU上更加明显。值得注意的是，现代NVIDIA GPU还为FP8和FP4提供Transformer Engine支持（以及FP6，尽管我们在本文中主要关注FP8/FP4格式，因为FP6在现有AI框架和工具中未被广泛使用）。结合更高的内存带宽，Tensor Core让ThunderMLA等内核能够更接近硬件极限运行。在现代GPU上，由于这些架构进步，ThunderMLA实现了更低的每token延迟。

> These benefits are amplified on modern GPUs with larger L2 caches and faster attention primitives. Notably, modern NVIDIA GPUs also provide Transformer Engine support for FP8 and FP4 (and FP6, although we focus mostly on FP8/FP4 formats in this text since FP6 is not widely used in existing AI frameworks and tools). Combined with higher memory bandwidth, the Tensor Cores let kernels like ThunderMLA operate much closer to hardware limits. On modern GPUs, ThunderMLA achieves even lower latency per token due to these architectural advances.

### FlexDecoding (PyTorch)

在第14章中，我们讨论了PyTorch的FlexAttention，它让您为注意力中的任意稀疏模式JIT编译融合内核，包括局部窗口、块稀疏模式等——所有这些都无需编写自定义CUDA。在底层，TorchInductor + OpenAI的Triton生成一个融合内核，仅计算该模式允许的查询-键对。Triton在给定硬件上有利时自动应用性能优化技术，如warp特化和异步拷贝。但是，您也可以通过配置num_consumer_groups等参数来进一步调整triton.Config。

FlexDecoding是`torch.nn.attention.flex_attention`的解码后端。FlexDecoding还让您就地管理KV，并像FlexAttention一样支持掩码和偏置。具体来说，FlexDecoding为decode阶段（Q_len=1）编译专门的内核，在增长的KV缓存上进行注意力计算。

在运行时，FlexDecoding实现选择专门的解码内核，并在多个解码步骤中重用它。这有助于在形状和数据类型保持兼容时最小化开销——大大加速长序列LLM推理。

> In Chapter 14, we discussed PyTorch's FlexAttention, which lets you JIT-compile fused kernels for arbitrary sparsity patterns in attention, including local windows, block-sparse patterns, etc.—all without writing custom CUDA. Under the hood, TorchInductor + OpenAI's Triton generate a fused kernel that computes only the allowed query-key pairs for that pattern. Triton automatically applies performance optimization techniques like warp specialization and asynchronous copies when beneficial on the given hardware. However, you can also tune triton.Config to further customize by configuring num_consumer_groups for example. FlexDecoding is the decoding backend of torch.nn.attention.flex_attention. FlexDecoding also lets you manage KV in place and supports masks and biases just like FlexAttention. Specifically, FlexDecoding compiles a specialized kernel for the decode phase (Q_len=1) attending over a growing KV cache. At runtime, the FlexDecoding implementation picks the specialized decode kernel and reuses it across multiple decode steps. This helps to minimize overhead when shapes and dtypes remain compatible—greatly speeding up long-sequence LLM inference.

> 对于稳定、延迟关键的解码，一旦重编译得到控制，优先使用`torch.compile(mode="max-autotune")`。保持捕获边界狭窄（每层或注意力块），以减少由不规则批处理引起的图失效。prefill和decode优先使用Transformer Engine FP8（MXFP8）。当精度允许且性能提高时，考虑FP4（NVFP4）。截至本文撰写时，FP4支持仍在成熟中，短期内可能表现不如8位和16位格式。继续设置`torch.set_float32_matmul_precision("high")`以在剩余的FP32操作上启用TF32回退。FlexAttention的解码后端支持常见的性能增强，包括分组查询注意力（GQA）和PagedAttention。

> Prefer torch.compile(mode="max-autotune") for stable, latency-critical decode once recompilations are under control. Keep the capture boundary narrow (per-layer or attention block) to reduce graph invalidations from ragged batching. Prefer Transformer Engine FP8 (MXFP8) for prefill and decode. Consider FP4 (NVFP4) when accuracy permits and performance increases. As of this writing, FP4 support is still maturing and can underperform 8-bit and 16-bit formats in the near-term. Continue to set torch.set_float32_matmul_precision("high") to enable TF32 fallback on remaining FP32 ops. FlexAttention's decode backend supports common performance enhancements including grouped-query attention (GQA) and PagedAttention.

FlexAttention和FlexDecoding的一个关键特性是支持嵌套锯齿布局张量（NJT）。这些允许在解码期间对可变长度序列（LLM工作负载中常见）进行不规则批处理。图18-2显示了各种序列的锯齿张量表示。

![图18-2 不规则批次作为嵌套锯齿张量（偏移量）；三个序列（上图）表示为具有偏移量的单个嵌套锯齿张量表示（下图）；解码时批处理优先使用PyTorch NJT](../assets/images/ch18/fig18_02.png)

> Figure 18-2. Ragged batch as a nested jagged tensor (offsets); three sequences (top) represented as a single nested jagged tensor representation with offsets (bottom); prefer PyTorch NJT for decode-time batching

此外，FlexDecoding支持偏置项，并通过使用块掩码转换接口将逻辑块映射到物理缓存布局来与PagedAttention集成。这将逻辑KV块分散到物理缓存布局中——无需创建额外副本，如图18-3所示。

FlexDecoding利用捕获的张量在每次迭代期间改变某些掩码或偏置值——无需重新编译。它与PagedAttention集成。要使用全局KV缓存（如vLLM LMCache），将缓存的页表映射到FlexAttention的BlockMask。这将动态地将逻辑KV页转换为物理内存地址。

> Additionally, FlexDecoding supports bias terms and integrates with PagedAttention by using a block mask conversion interface that maps logical blocks to the physical cache layout. This scatters logical KV blocks into the physical cache layout—without creating extra copies, as shown in Figure 18-3. FlexDecoding leverages captured tensors to vary certain mask or bias values during each iteration—without requiring a recompile. And it integrates with PagedAttention. To use a global KV cache such as vLLM LMCache, map the cache's page table to FlexAttention's BlockMask. This will translate logical KV pages into physical memory addresses on the fly.

![图18-3 PagedAttention将逻辑KV块分散到物理KV块中，以优化序列间的缓存重用；块大小与LMCache页大小对齐——较大的页（例如64-128个token）减少分离设置中的RDMA开销](../assets/images/ch18/fig18_03.png)

> Figure 18-3. PagedAttention scatters logical KV blocks into physical KV blocks for optimal cache reuse between sequences; align block sizes with LMCache page size—larger pages (e.g., 64–128 tokens) reduce RDMA overhead in disaggregated setups

使用FlexDecoding，开发人员拥有完全的Python级灵活性来定义自定义注意力稀疏模式。这对于MoE模型推理特别有用。FlexDecoding允许您在无需编写任何自定义CUDA内核的情况下实现接近最优的性能。本质上，它允许任意注意力模式像密集注意力模式一样被优化。随着新推理技术的出现，这变得更加有价值。

> With FlexDecoding, developers have full Python-level flexibility for custom attention sparsity patterns. This is particularly useful for MoE model inference. FlexDecoding allows you to achieve near-optimal performance without requiring you to write any custom CUDA kernels. Essentially, it allows arbitrary attention patterns to be optimized similarly to dense attention patterns. This becomes even more valuable as new inference techniques emerge.

许多这些功能，如解码的融合注意力以及对PyTorch嵌套锯齿张量（NJT）批处理的支持，都可在核心PyTorch库中使用。这使得对于典型模式，自定义融合变得不那么必要。

> 对于LLM工作负载中常见的不规则序列批处理，优先使用NJT布局。

> Many of these capabilities, such as fused attention for decoding and support for PyTorch's nested jagged tensors (NJT) batching, are available in the core PyTorch library. This makes custom fusion less necessary for typical patterns. Prefer the NJT layout when batching ragged sequences common in LLM workloads.

这些内核级进步是高度技术性的，充分利用了GPU、网络和内存的全部能力。这些软件优化可以显著提高解码性能——即使在相同的硬件上。

在设计超大规模系统时，如果可能，您应该整合这些优化的内核。务必使用带有基于硬件的CUDA跟踪的Nsight Systems验证重叠和内核效率。此外，使用Nsight Compute获取特定的内存和链路指标。

> 启用某些高级内核可能需要安装自定义库或启用专门的CUDA内核——特别是对于较新的技术。但是，这些技术通常在发布后不久就得到PyTorch和流行推理引擎的支持。即使您确实需要安装自定义资源，这项工作也是值得的，因为它将直接转化为更低的延迟和更少的decode工作节点池中的GPU。

> These kernel-level advancements are highly technical and leverage the full power of the GPU, network, and memory. These software optimizations can significantly improve decode performance—even on the same hardware. When designing an ultrascale system, you should incorporate these optimized kernels, if possible. Be sure to verify overlap and kernel efficiency using Nsight Systems with hardware-based CUDA traces. Additionally, use Nsight Compute for specific memory and link metrics. Enabling some of these advanced kernels might require installing a custom library or enabling a specialized CUDA kernel—especially for newer techniques. However, these techniques are typically supported by PyTorch and the popular inference engines soon after they are released. Even if you do need to install custom resources, the effort is worthwhile since it will directly translate to lower latency and fewer GPUs in the decode worker pool.

## 调优KV缓存利用和管理 (Tuning KV Cache Utilization and Management)

分离要求您将KV缓存视为集群中的一等共享资源。由于KV缓存现在可以存在更长时间并在节点之间移动，高性能推理系统改进了KV缓存的存储和共享方式。

特别是，分布式KV缓存池和跨请求的前缀重用成为强大的技术。此外，关注新GPU和HBM代际带来的内存带宽改进也很重要。让我们在提高KV缓存性能的背景下逐一讨论。

> Disaggregation requires that you treat the KV cache as a first-class shared resource across the cluster. Since KV caches can now live longer and move between nodes, high-performance inference systems have improved how the KV cache is stored and shared. In particular, distributed KV cache pools and prefix reuse across requests become powerful techniques. Additionally, it's important to keep an eye on the memory bandwidth improvements with newer GPU and HBM generations. Let's discuss each of these in the context of improving KV cache performance.

### 分离式KV缓存池 (Disaggregated KV Cache Pool)

与其让每个GPU仅存储其当前正在服务的请求的KV，分离式KV缓存池将KV存储与单个GPU解耦。相反，它将数据分散到整个集群的GPU内存中。

该池还可以卸载到CPU内存，包括Grace Blackwell和Vera Rubin平台的统一CPU和GPU内存。它也可以卸载到持久存储，如NVMe SSD。

使用分离式KV缓存池，当prefill计算提示的KV张量——或decode扩展KV张量时——KV块以分布式方式存储在许多计算节点上。这如图18-4所示，改编自分离式KV池的研究工作。

![图18-4 分离式（分布式）KV缓存池（来源：https://oreil.ly/2xtK-）](../assets/images/ch18/fig18_04.png)

> Figure 18-4. Disaggregated (distributed) KV cache pool (source: https://oreil.ly/2xtK-)

考虑一个非常长的250,000 token上下文（例如，具有多轮对话的聊天会话），使用700亿参数的transformer模型，具有80层和32个头，每个头的维度为128。这会为每个token生成巨大的KV缓存占用空间。

每个token生成一个长度等于模型隐藏维度（num_heads × head_dim）的键和值向量。让我们假设我们的模型为4,096。这导致每层8,192个浮点数。对80层求和，这会为每个token创建655,360个浮点数的KV数据。让我们假设16位精度，或每个浮点数2字节。这大约需要每token 1.31 MB。

将其扩展到250,000个token会产生约328 GB——仅用于KV数据！

> 这些计算基于大型模型的每token KV大小。这解释了为什么FP8 KV缓存在vLLM等引擎中被广泛采用，以减少占用空间并增加批处理机会。

> Consider a very long 250,000-token context (e.g., a chat session with many turns) using a 70 billion-parameter transformer model with 80 layers and 32 heads in which each head is dimension 128. This generates a huge KV cache footprint per token. Each token generates a key and value vector of length equal to the model's hidden dimension (num_heads × head_dim). Let's assume this is 4,096 for our model. This results in 8,192 floats per layer. Summing across 80 layers, this creates 655,360 floats of KV data per token. Let's assume 16-bit precision, or 2 bytes per float. This is roughly 1.31 MB needed per token. Scaling this to 250,000 tokens produces about 328 GB—just for the KV data! These calculations are based on per-token KV sizes for large models. This shows why FP8 KV caches are widely adopted in engines such as vLLM to reduce footprint and increase batching opportunities.

假设我们将KV缓存量化到FP8并使用选择性层缓存等技术，我们可以将这个250,000 token提示的占用空间减少到大约100-150 GB范围。单个GPU可能没有容量容纳所有token的KV以及它需要保存的其他所有内容，例如模型权重等——特别是当多轮对话继续时。因此，系统需要截断上下文——或者导致对早期token进行昂贵的KV重新计算。

然而，使用分离式KV池，上下文的较旧部分的KV可以从GPU中驱逐并推送到分布在集群中的KV缓存池——以及CPU DRAM或NVMe存储中。然后在需要时将数据取回到GPU内存中。

分离式KV缓存池实现了一个多层内存层次结构，其中GPU设备内存保存活动KV缓存，而CPU主机RAM（或NVMe存储）作为溢出后备存储。

> Assuming we quantize the KV cache down to FP8 and use techniques like selective-layer caching, we can reduce this footprint down to maybe the 100–150 GB range for this 250,000-token prompt. A single GPU will likely not have capacity for all of the tokens' KV along with everything else it needs to hold, such as model weights, etc.—especially as the multiturn conversation continues. As such, the system would need to truncate context—or cause expensive KV recomputation on earlier tokens. With a disaggregated KV pool, however, older parts of the context's KV can be evicted from the GPU and pushed out to the KV cache pool spread across the cluster—and in CPU DRAM or NVMe storage. The data is then fetched back into GPU memory when it's needed. A disaggregated KV cache pool implements a multitier memory hierarchy in which GPU device memory holds the active KV cache while the CPU host RAM (or NVMe storage) serves as the overflow backing store.

现代推理引擎可以选择将KV缓存卸载到CPU内存或NVMe。这有效地虚拟化了GPU内存，就像操作系统的虚拟内存子系统一样。

这种设计通过在GPU内存和KV缓存池之间异步分页KV块来实现超长上下文——假设有良好的通信-计算重叠，正如本书所讨论的。

此外，通过将请求状态与单个GPU解耦，系统可以使用全局KV缓存池在集群中的多个节点之间动态分片KV数据，以自适应地平衡负载。这简化了扩展并改善了大型推理集群中的故障隔离。

由于任何decode节点都可以访问全局KV池，因此任何decode节点都可以参与解码任何请求（如果由于故障转移或负载均衡需要）。这为调度器增加了灵活性，因为它可以选择最接近相关KV缓存块位置的decode节点。

如果某个前缀的一些KV块缓存在服务器A的DRAM中，在服务器A上调度解码可能更快，因为它可以快速将它们拉入其GPU。这与服务器B形成对比，后者必须通过网络获取KV块。

> 这描述了分布式系统的经典最佳实践：选择最接近被计算数据的计算节点。这样，系统最小化了昂贵的数据移动。

> Modern inference engines can choose to offload KV cache to CPU memory or NVMe. This effectively virtualizes GPU memory much like an OS's virtual memory subsystem. This design allows for ultralong contexts by asynchronously paging KV blocks between GPU memory and the KV cache pool without stalling the compute pipelines—assuming good communication-computation overlap, as discussed throughout this book. Additionally, by decoupling request state from individual GPUs, the system can use the global KV cache pool to dynamically shard KV data across multiple nodes in the cluster to adaptively balance the load. This simplifies scaling and improves fault isolation in large inference clusters. And since any decode node can access the global KV pool, then any decode node can participate in decoding any request, if needed due to failover or load balancing. This adds flexibility for the scheduler since it can choose a decode node closest to where the relevant KV cache blocks are located. If some KV blocks of a prefix are cached in DRAM on server A, it might be faster to schedule the decode on server A since it can quickly pull them into its GPU. This is in contrast to server B, which would have to fetch the KV blocks over the network. This describes a classic distributed-systems best practice of choosing a compute node that is closest to the data being computed. This way, the system minimizes expensive data movement.

高效的KV缓存调度器可以查看池中KV块的分布——以及网络拓扑——并相应地分配prefill和decode任务。因此，prefill节点可以将KV数据放入实现为集群范围内可访问的分布式内存空间的池中。

当KV缓存位于集群端共享内存空间中时，任何decode节点都可以检索数据。这避免了每次都必须为直接的prefill到decode传输进行调度。

这增加了一些额外的开销，因为需要额外的一跳来从池中检索KV数据，但它提供了更多的灵活性，因为所有decode节点都可以访问所有KV缓存数据。这也意味着没有直接从特定prefill接收数据的decode节点仍然可以在需要时从池中访问KV数据。

如果decode节点崩溃——或者请求由于某种原因需要在中途移动——KV数据不会丢失。数据存在于池中，另一个节点可以使用保存的KV从中断的地方继续。这提高了容错性。

全局KV缓存池还提供跨请求的缓存持久性。这样，如果两个请求共享某个前缀，该前缀的KV可以计算一次并在集群中重用——即使请求最终在不同的decode服务器上。

简而言之，分离式KV缓存池用内存（或较冷的存储）换取计算。通过存储更大的KV缓存，系统可以在许多场景中避免重新计算KV数据。这种方法利用了一个事实：重用数据——即使从DRAM或SSD——通常比重复重新计算具有二次时间复杂度O(N²)的大型注意力矩阵乘法更便宜。

> An efficient KV cache scheduler can look at the distribution of KV blocks in the pool—in addition to the network topology—and assign prefill and decode tasks accordingly. As such, a prefill node can place KV data into a pool implemented as a distributed memory space accessible across the cluster. With the KV cache in a cluster-side shared-memory space, any decode node can retrieve the data. This avoids having to schedule for direct prefill-to-decode transfers every time. This adds a bit of extra overhead due to an extra hop to retrieve KV data from the pool, but it allows more flexibility because all decode nodes have access to all KV-cached data. It also means that a decode node that didn't directly receive data from a particular prefill can still access the KV data from the pool, if needed. If a decode node crashes—or a request needs to move mid-generation for whatever reason—the KV data isn't lost. The data lives in the pool, and another node can pick it up and continue where it left off using the saved KV. This improves fault tolerance. A global KV cache pool also provides cache persistence across requests. This way, if two requests share some prefix, the KV for that prefix can be computed once and reused across the cluster—even if the requests end up on different decode servers. In short, a disaggregated KV cache pool trades memory (or colder storage) for compute. By storing a larger KV cache, the system can avoid recomputing KV data in many scenarios. This approach leverages the fact that reusing data—even from DRAM or SSD—is often cheaper than repeatedly recomputing large attention matrix multiplications with quadratic time complexity, O(N2).

### KV缓存重用和前缀共享 (KV Cache Reuse and Prefix Sharing)

如前所述，对于共享公共前缀的提示，跨请求重用缓存的KV数据是有益的。这种情况在多轮对话、共享系统提示和附加文档等形式中相当常见。

系统可以为该前缀存储KV输出并直接重用它们，而不是为每个请求重新计算该前缀的transformer注意力输出。本质上，这跳过了输入那部分的prefill计算，节省了大量时间和GPU周期。

> As mentioned, it's beneficial to reuse cached KV data across requests for prompts that share a common prefix. This scenario arises fairly often in the form of multiturn conversations, shared system prompts, and attached documents. Instead of recomputing the transformer attention outputs for that prefix for every request, the system can store the KV outputs for the prefix and reuse them directly. Essentially, this skips the prefill computation for that portion of the input, which saves a lot of time and GPU cycles.

以KV缓存为中心的调度器在分配工作时，会通过查看"前缀缓存命中长度"（即此提示有多少token已存在于缓存池中）来考虑前缀缓存命中。在实践中，如果新请求到来，其前N个token与KV池中的某个缓存前缀匹配，系统可以决定重用该KV数据。

vLLM使用其PagedAttention机制通过KV"页"的全局哈希表实现自动前缀缓存。在这里，每个唯一的16 token上下文块都有一个哈希值。如果新请求需要与存储块（通过哈希）匹配的前缀，它可以直接复制这些KV张量而不是重新计算。

如果相同的上下文再次出现，系统从内存中提供它。本质上，它将上下文的KV视为可以通过哈希按内容查找的可重用数据。实现通常维护一个全局"提示树"来管理这些缓存上下文并在必要时驱逐它们。这优化了最频繁重用的前缀。

> A proper KV-cache-centric scheduler takes into account prefix cache hits by looking at the "prefix cache hit length," or how many tokens of this prompt are already present in the cache pool, when assigning work. In practice, if a new request comes and its first N tokens match some cached prefix in the KV pool, the system can decide to reuse that KV data. vLLM implements automatic prefix caching using a global hash table of KV "pages" using its PagedAttention mechanism. Here, each unique 16-token block of context has a hash. If a new request needs a prefix that matches a stored block (by hash), it can directly copy those KV tensors instead of recomputing. If the same context appears again, the system serves it from memory. In essence, it treats the KV of a context as reusable data that can be looked up by content using hashing. Implementations typically maintain a global "prompt tree" to manage these cached contexts and evict them when necessary. This optimizes for the most frequently reused prefixes.

有效KV重用的关键是识别相同或重叠的前缀。通常，系统为简单起见专注于精确匹配，使得如果前N个token完全匹配，它们重用该块。合并部分前缀重叠更复杂，因为您需要以某种方式合并缓存，这并不总是直截了当的。因此，典型的缓存使用精确前缀缓存。

然而，存在一个权衡。无限期存储许多用户的KV缓存会消耗大量内存。系统必须为KV块实现LRU等驱逐策略，以丢弃不太可能被重用的缓存。这为新缓存释放空间。调度器还可以根据重用可能性决定保留哪些缓存。目标是在内存约束内最大化缓存命中。

如果某个prefill节点已经在其本地GPU内存或本地DRAM缓存中持有所需KV的一部分，将请求路由到该节点以最小化数据传输可能是有益的。这是数据感知调度的一个例子，其中它将计算发送到数据所在的位置，而不是总是将数据拉到任何有计算可用的地方。

> 这类似于分布式系统中的位置感知调度。在我们之前的路由讨论中，我们触及了这一点。如果可能，您应该将请求路由到生成其前缀的服务器。这最大化了缓存命中的可能性。

> A key to effective KV reuse is identifying identical or overlapping prefixes. Usually, systems focus on exact matches for simplicity such that if the first N tokens match exactly, they reuse that chunk. Combining partial-prefix overlaps is more complex since you need to somehow merge caches, which isn't always straightforward. So typical caching uses exact prefix caching. There is a trade-off, however. Storing many users' KV caches indefinitely can consume a lot of memory. A system must implement eviction policies like LRU for KV blocks to drop caches that are unlikely to be reused. This frees space for new ones. The scheduler might also decide which caches to keep based on likelihood of reuse. The idea is to maximize cache hits within memory constraints. If a certain prefill node already holds a portion of the KV needed in its local GPU memory or local DRAM cache, it might be beneficial to route the request to that node to minimize data transfer. This is an example of data-aware scheduling in which it sends the compute to where the data is, rather than always pulling data to wherever compute is available. This is analogous to locality-aware scheduling in distributed systems. In our earlier routing discussion, we touched on this. If possible, you should route a request to the server that generated its prefix. This maximizes the likelihood of a cache hit.

在分离的更广泛背景下，前缀缓存通过跨许多请求拥有KV的统一视图来支持，并可能将其存储在可共享的地方，如全局池。这与孤立的每请求或每节点方法形成对比。

这也有助于减少分离可能产生的重新计算开销，如果相同的提示在不同时间到达不同的节点。使用全局KV存储或协调缓存，即使用户的请求到达不同的decode服务器，它们也可以从彼此的缓存工作中受益。

> In the broader context of disaggregation, prefix caching is supported by having a unified view of KV across many requests and possibly storing it in a shareable place like the global pool. This is in contrast to a siloed per-request or per-node approach. This also helps reduce the overhead of recomputation that disaggregation might otherwise incur if the same prompt goes to different nodes at different times. With a global KV store or coordinated caching, even if a user's requests hit different decode servers, they can benefit from each other's cached work.

### 优化的KV缓存内存布局 (Optimized KV Cache Memory Layout)

底层创新的另一个领域是优化KV缓存内存布局。KV缓存存储每个序列中所有过去token的键和值，对于许多并发解码流可能变得巨大，因为每个流使用的内存大致与`num_layers × 2 × sequence_length × d_head`成正比。

分层缓存等技术很有用，因为并非所有KV对都需要始终保持在GPU内存中。KV缓存的较旧部分可以交换到CPU——甚至压缩。

由于我们强调保持解码延迟低，大多数设计将活动KV缓存保持在GPU内存中以快速访问。在这种情况下，您可以调整内存的布局和访问方式。

DeepSeek的FlashMLA对KV缓存进行分页，并以固定大小的块（页）分配缓存，以便活动序列可以进行连续内存访问。这减少了缓存未命中和DRAM流量。

此外，如果提示的前缀将不再被关注，例如因为上下文窗口已移动，某些系统会实现前缀压缩。在这种情况下，KV缓存管理器可能会丢弃或压缩这些KV条目。这在长对话中更相关，因为上下文窗口会滑动。但它可以为极长的序列节省内存和带宽。

> 这种驱逐/压缩技术在模型使用滑动窗口或其他受限注意力模式时是安全的。但是，不应将其应用于在完整内容窗口（或检索钩子）上保留完整注意力的层，除非经过仔细评估。

> Another area of low-level innovation is optimizing the KV cache memory layouts. The KV cache, which stores keys and values for all past tokens in each sequence, can become huge for many concurrent decode streams since each stream uses memory roughly proportional to num_layers × 2 × sequence_length × d_head. Techniques like tiered caching are useful since not all KV pairs need to be kept in GPU memory at all times. Older parts of the KV cache can be swapped to CPU—or even compressed. Since we emphasize keeping decode latency low, most designs keep the active KV cache in GPU memory for quick access. In this case, you can tune how the memory is laid out and accessed. DeepSeek's FlashMLA pages KV cache and allocates the cache in fixed-size blocks (pages) so that contiguous memory accesses can happen for active sequences. This reduces cache misses and DRAM traffic. Additionally, some systems implement prefix compression if a prompt's prefix will no longer be attended to because the context window has moved, for instance. In this case, the KV cache manager might drop or compress these KV entries. This is more relevant in long conversations because the context window slides. But it can save memory and bandwidth for extremely long sequences. This eviction/compression technique is safe when the model uses a sliding-window or other restricted-attention pattern. However, it should not be applied to layers that retain full attention over the full content window (or retrieval hooks) without careful evaluation.

另一种称为POD-Attention的技术类似地重新组织注意力计算以减少HBM流量。具体来说，它使用SM感知线程块（或协作线程阵列[CTA]）调度。这实现了运行时操作绑定，动态将在SM上运行的每个CTA分配为执行prefill或decode任务。这如图18-5所示。

![图18-5 SM感知线程块（CTA）调度，将prefill任务与SM上的decode任务匹配以最小化内存移动](../assets/images/ch18/fig18_05.png)

> Figure 18-5. SM-aware thread-block (CTA) scheduling to match prefill tasks with decode tasks on SMs to minimize memory movement

因此，不是为每个阶段静态启动单独的内核，而是启动一个足够CTA覆盖两个工作负载的内核。在运行时，每个CTA检查它所在的SM，并使用每SM计数器根据该SM上其他正在运行的内容决定应该运行哪个操作（prefill或decode）。

SM感知调度逻辑尝试在运行时将prefill与decode操作匹配。这避免了内存流量的隔离突发并平滑了资源需求。

具体来说，POD-Attention将prefill和decode工作共置在同一个SM上，以便融合内核可以提高局部性并减少冗余的HBM事务。这最小化了内存移动，最大化了带宽利用率，并平衡了每个SM上的计算受限和内存受限工作负载。通过在相同SM上共置prefill和decode工作并进行适当的SM感知CTA调度以解锁完全重叠，POD-Attention可以将注意力性能提高约29%。

> So rather than statically launching separate kernels for each phase, a single kernel launches enough CTAs to cover both workloads. At runtime, each CTA inspects which SM it's on and uses per-SM counters to decide which operation (prefill or decode) should be run based on what else is running on that SM. The SM-aware scheduling logic tries to match prefill with decode operations at runtime. This avoids isolated bursts of memory traffic and smooths out resource demands. Specifically, POD-Attention colocates prefill and decode work on the same SM so the fused kernel can improve locality and reduce redundant HBM transactions. This minimizes memory movement, maximizes bandwidth utilization, and balances compute-bound and memory-bound workloads on each SM. POD-Attention can improve attention performance by up to about 29% by colocating prefill and decode work on the same SMs with proper SM-aware CTA scheduling to unlock full overlap.

POD-Attention的动态绑定将硬件的CTA-SM分配与软件的CTA角色分配（prefill或decode）解耦。这种创新表明越来越关注硬件和软件协同设计，以最小化内存移动并充分利用系统性能。

> POD-Attention's dynamic binding decouples the hardware's CTA-SM assignment from the software's CTA-role assignment—either prefill or decode. This type of innovation shows a growing focus on hardware and software codesign to minimize memory movement and get the most out of your system's performance.

### GPU和CPU-GPU超级芯片改进 (GPU and CPU-GPU Superchip Improvements)

您还应该考虑新硬件中的内存带宽改进。更高的内存带宽和更大的L2缓存直接有利于内存受限的decode阶段的性能。

NVIDIA的Grace Blackwell GB200 NVL72系统是一个机架级平台，具有36个Grace CPU和72个Blackwell GPU，允许单个逻辑解码单元拥有数十TB的KV缓存内存。这种硬件具有约30 TB的统一内存，非常适合将非常大的上下文保持在内存中。这些上下文可以达到数百万token的规模。

在这样的平台上，统一内存占用空间很大。然而，对于延迟关键的解码，您仍然希望活动键和值驻留在GPU HBM中。因此，您应该将Grace CPU内存（LPDDR5X，而非HBM）用作较低层缓存或用于非常旧的token。当上下文超过可用HBM时——即使在NVL72这样的系统上——prefill和键值卸载仍然很重要。

简而言之，宏观层面的分离应该与微观层面的优化相结合，以充分实现最大的推理性能。高级解码内核如FlashMLA/ThunderMLA、高效的内存布局（分页缓存等）以及最新的GPU架构将产生高效且可扩展的解码。

> You should also consider memory bandwidth improvements in new hardware. Higher memory bandwidth and larger L2 caches directly benefit the performance of the memory-bound decode phase. NVIDIA's Grace Blackwell GB200 NVL72 system, a rack-scale platform with 36 Grace CPUs and 72 Blackwell GPUs, allows a single logical decode unit with tens of terabytes of memory for KV cache. This hardware, with its ~30 TB of unified memory, is ideal to keep very large contexts in memory. These contexts can be on the order of millions of tokens. With such a platform, the unified memory footprint is large. However, for latency-critical decode, you still want the active keys and values to live in GPU HBM. As such, you should use Grace CPU memory (LPDDR5X, not HBM) as a lower-tier cache or for very old tokens. Prefill and key-value offloading remain important when contexts exceed available HBM—even on a system like the NVL72. In short, disaggregation at a macro level should be paired with microlevel optimizations to fully achieve maximum inference performance. Advanced decode kernels like FlashMLA/ThunderMLA, efficient memory layouts (paged caches, etc.), and the latest GPU architectures will produce efficient and scalable decode.

