# 第4章 高速、低开销数据传输

现代AI工作负载——从训练到推理——依赖于快速的数据移动。无论是多GPU训练期间的梯度同步，还是推理期间的KV缓存传输，通信延迟都可能成为瓶颈。本章探讨如何在GPU集群中实现高速、低开销的数据传输，包括RDMA、NCCL和NIXL等关键技术。

> Modern AI workloads—from training to inference—rely on fast data movement. Whether it's gradient synchronization during multi-GPU training or KV cache transfers during inference, communication latency can become a bottleneck. This chapter explores how to achieve high-speed, low-overhead data transfers in GPU clusters, including key technologies like RDMA, NCCL, and NIXL.

## 4.1 计算与通信重叠 (Overlapping Compute and Communication)

在分布式训练中，一个关键的性能优化技术是将计算与通信重叠。当GPU在计算梯度时，网络可以在后台传输数据。这种重叠可以显著减少总迭代时间。图4-1展示了在多个CUDA流上重叠主机到设备（H2D）和设备到主机（D2H）通信与计算。

![图4-1 在多个CUDA流0-3上重叠主机到设备（H2D）和设备到主机（D2H）通信与计算](../assets/images/ch04/fig04_165_1.png)

> In distributed training, a key performance optimization technique is to overlap computation with communication. When GPUs are computing gradients, the network can transfer data in the background. This overlap can significantly reduce total iteration time. Figure 4-1 shows overlapping host-to-device (H2D) and device-to-host (D2H) communication with computation on multiple CUDA streams 0–3.

> Figure 4-1. Overlapping host-to-device (H2D) and device-to-host (D2H) communication with computation on multiple CUDA streams 0–3

### 4.1.1 无重叠场景 (No Overlap Scenario)

假设我们不使用DDP的内置重叠功能，而是手动实现分布式训练，使得每个进程在本地计算所有梯度，然后在反向传播结束时对这些梯度执行all-reduce。这将模拟未优化、非重叠的场景，因为通信仅在所有计算完成后发生。

> Suppose we don't use DDP's built-in overlap and instead implement distributed training manually such that each process computes all gradients locally, then performs an all-reduce on those gradients at the end of the backward pass. This will mimic the unoptimized, nonoverlapping scenario since communication happens only after all computation is done.

在以下代码中，我们在单个节点上的gpu/rank 0和gpu/rank 1上启动两个进程，使用一个简单模型。我们运行前向和反向传播，然后手动在两个进程之间平均梯度：

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
import torch.multiprocessing as mp

class MultiLayerNet(nn.Module):
    def __init__(self, size):
        super().__init__()
        self.fc1 = nn.Linear(size, size)
        self.fc2 = nn.Linear(size, size)
        self.fc3 = nn.Linear(size, 1)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

def train_no_overlap(rank, world_size):
    dist.init_process_group("nccl", init_method="env://",
                            world_size=world_size, rank=rank)
    torch.cuda.set_device(rank)

    batch_size = 256
    data = torch.randn(batch_size, 1024, device=rank)
    target = torch.randn(batch_size, 1, device=rank)

    model = MultiLayerNet(data.size(1)).to(rank)
    optimizer = optim.SGD(model.parameters(), lr=0.01)

    output = model(data)
    loss = nn.functional.mse_loss(output, target)
    loss.backward()

    for p in model.parameters():
        dist.all_reduce(p.grad, op=dist.ReduceOp.SUM)
        p.grad /= world_size

    optimizer.step()
    dist.destroy_process_group()

if __name__ == "__main__":
    import torch.multiprocessing as mp
    mp.set_start_method("spawn", force=True)
    world_size = min(2, torch.cuda.device_count() or 1)
    if world_size > 1:
        mp.spawn(train_no_overlap, args=(world_size,), nprocs=world_size, join=True)
    else:
        train_no_overlap(0, 1)
```

> We launch two processes on gpu/rank 0 and gpu/rank 1 on a single node, in this case, with a simple model. We'll run a forward and backward pass, then manually average gradients across the two processes:

在这段代码中，每个进程独立计算MultiLayerNet的梯度。在`loss.backward()`之后，我们显式地对每个参数的梯度执行all-reduce以平均它们。这实际上是DDP内部所做的，但这里我们在整个反向传播完成后才执行——而不是在反向传播期间并发执行all-reduce。

> In this code, each process computes gradients for MultiLayerNet independently. After loss.backward(), we explicitly perform an all-reduce for each parameter's gradient to average them. This is effectively what DDP does internally, but here we wait and do it after the entire backward pass has finished—rather than doing the all-reduce concurrently during the backward pass.

如果我们对这个迭代进行计时，all-reduce操作将直接增加迭代时间，因为它没有与任何其他步骤重叠。例如，假设前向和反向计算总共需要10毫秒，梯度all-reduce需要12毫秒。在这种方法中，总迭代时间大约是22毫秒。相比之下，完全重叠的实现可能实现接近这些值中最大值的总时间，在我们的例子中是12毫秒。这是可能的，因为all-reduce通信几乎可以完全隐藏在计算之下。

> If we were to time this iteration, the all-reduce operations would add directly to the iteration time since it's not overlapping with any other steps. For example, say the forward and backward computations together take 10 ms and the gradient all-reduces take 12 ms. The total iteration time would be roughly 22 ms in this approach. In contrast, a fully overlapped implementation might achieve a total time closer to the max of these values, or 12 ms in our case. This is possible since the all-reduce communication can be almost completely hidden under the computation.

### 4.1.2 使用DDP重叠 (Overlap with DDP)

现在让我们使用PyTorch的DDP来执行相同的操作并实现重叠。DDP将挂钩到反向传播中，并将梯度归约与反向计算重叠。代码类似，但我们只需用DistributedDataParallel包装模型，让它处理同步：

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
import torch.multiprocessing as mp

class MultiLayerNet(nn.Module):
    def __init__(self, size):
        super().__init__()
        self.fc1 = nn.Linear(size, size)
        self.fc2 = nn.Linear(size, size)
        self.fc3 = nn.Linear(size, 1)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

def train_ddp(rank, world_size):
    rank = int(os.environ.get("LOCAL_RANK", rank))
    torch.cuda.set_device(rank)
    dist.init_process_group("nccl", init_method="env://",
                            world_size=world_size, rank=rank)
    torch.cuda.set_device(rank)

    model = MultiLayerNet(1024).to(rank)
    ddp_model = nn.parallel.DistributedDataParallel(model, device_ids=[rank])
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.01)

    batch_size = 256
    data = torch.randn(batch_size, 1024, device=rank)
    target = torch.randn(batch_size, 1, device=rank)

    output = ddp_model(data)
    loss = nn.functional.mse_loss(output, target)
    loss.backward()
    optimizer.step()
    dist.destroy_process_group()

def main():
    world_size = min(2, torch.cuda.device_count() or 1)
    mp.set_start_method("spawn", force=True)
    if world_size > 1:
        mp.spawn(train_ddp, args=(world_size,), nprocs=world_size, join=True)
    else:
        print("Only one GPU present; running DDP demo with world_size=1")
        train_ddp(0, 1)

if __name__ == "__main__":
    main()
```

> Now let's use PyTorch's DDP to perform the same operation with overlap. DDP will hook into the backward pass and overlap gradient reduction with backward computation. The code is similar, but we simply wrap the model with DistributedDataParallel and let it handle synchronization:

使用DDP重叠方法，代码更简单，因为我们依赖DistributedDataParallel来处理梯度同步，而不是自己编写。当调用`loss.backward()`时，DDP的内部reducer将梯度分成桶，并在每个桶准备好时在单独的CUDA流上启动NCCL all-reduce操作。例如，它可能会在fc3.weight和fc3.bias梯度计算完成后立即对它们进行all-reduce，因为这些来自最后一层，在反向传播中最后计算，而来自较早层的fc1和fc2梯度在我们到达反向传播结束时已经完成all-reduce。

> With the DDP overlapping approach, the code is simpler since we rely on DistributedDataParallel to handle gradient synchronization rather than writing it ourselves. When loss.backward() is called, DDP's internal reducer splits the gradients into buckets and launches NCCL all-reduce operations as soon as each bucket is ready on a separate CUDA stream. For instance, it might all-reduce the fc3.weight and fc3.bias gradients immediately after they are computed since those are from the last layer and are computed last in the backward pass, while the fc1 and fc2 gradients from earlier layers will have already been all-reduced by the time we reach the end of the backward pass.

如果模型非常小，所有梯度都适合放入一个桶，DDP可能只在最后执行一次all-reduce，这不会有太多重叠。但对于较大的模型和较大的批量大小，将有多个桶和显著的重叠——展示出这项技术更令人印象深刻的性能提升。

> If the model is very small such that all gradients fit into one bucket, DDP might do only one all-reduce at the end, which wouldn't overlap much. But with larger models and bigger batch sizes, there will be multiple buckets and significant overlap—showing even more impressive performance gains from this technique.

表4-1显示了重叠的好处：

| 指标 | 无重叠（手动同步） | 重叠（DDP） | 说明 |
|------|-------------------|-------------|------|
| 总反向+通信时间 | 100%（基准） | ~70%基准 | 例如，由于重叠每次迭代快30% |
| 通信开始时间 | 反向完成后 | 反向期间 | DDP中，通信在反向中途开始 |
| 通信期间GPU空闲 | 是——反向后，GPU在all-reduce期间等待 | 最小——通信在其他层仍在计算时运行 | DDP隐藏了大部分延迟 |
| SM（GPU）利用率 | 较低（通信期间SM有些周期空闲） | 较高（持续活动） | 重叠使GPU更持续地忙碌 |
| 实现的重叠（计算覆盖的通信百分比） | 0%（串行执行） | ~50%（或更多） | 粗略估计：较大的模型或批量可以重叠更多 |

> Table 4-1 shows the benefit of overlap:

在这个例子中，将通信与计算重叠在我们的示例工作负载中产生了大约30%的迭代时间改进。在较大的训练作业中，收益会更显著，因为较大的模型会产生更多通信瓶颈的可能性。PyTorch DistributedDataParallel默认使用25 MiB桶。这样，它在每个桶准备好时启动all-reduce。调整`bucket_cap_mb`可以帮助增加特定模型拓扑的重叠，但较大的桶会增加最后一个桶的延迟。

> In this example, overlapping communication with computation yields roughly a 30% iteration time improvement in our example workload. In larger training jobs, the gains would be more substantial since larger models create more potential for communication bottlenecks. PyTorch DistributedDataParallel uses 25 MiB buckets by default. This way, it launches an all-reduce for each bucket as soon as it is ready. Tuning bucket_cap_mb can help increase overlap for your specific model topology, but larger buckets increase latency for the last bucket.

## 4.2 NVIDIA Magnum IO优化栈 (NVIDIA Magnum IO Optimization Stack)

Magnum IO是NVIDIA的总体I/O加速平台，汇集了一系列技术来加速GPU、CPU、存储和网络接口之间的数据移动、访问和管理。Magnum IO架构有四个关键组件，涵盖存储、网络、网络内计算和I/O管理，如图4-2所示。

![图4-2 NVIDIA Magnum IO加速平台的四个组件](../assets/images/ch04/fig04_178_1.png)

> Magnum IO, NVIDIA's overarching I/O acceleration platform, brings together a range of technologies to speed up data movement, access, and management across GPUs, CPUs, storage, and network interfaces. There are four key components of the Magnum IO architecture spanning storage, network, in-network computing, and I/O management, as shown in Figure 4-2.

以下是图4-2中四个组件的描述：

**存储I/O**：由NVIDIA GPUDirect Storage（GDS）和BlueField SNAP等技术实现。这些技术让GPU直接访问存储（包括NVMe SSD），而无需通过主机CPU内存进行不必要的复制。我们将在第5章深入探讨GDS。

> **Storage I/O**: This is implemented by technologies such as NVIDIA GPUDirect Storage (GDS) and BlueField SNAP. These let GPUs access storage including NVMe SSDs directly without unnecessary copies through host CPU memory. We'll dive deeper into GDS in Chapter 5.

**网络I/O**：包括GPUDirect RDMA、NCCL、NVSHMEM、UCX和HPC-X（MPI/SHMEM软件包）等技术，用于在节点间的GPU之间实现直接、高速的数据传输，绕过CPU进行节点间通信。

> **Network I/O**: This includes technologies like GPUDirect RDMA, NCCL, NVSHMEM, UCX, and HPC-X (MPI/SHMEM software bundle) to enable direct, high-speed data transfers between GPUs across nodes, bypassing the CPU for internode communication.

**网络内计算**：SHARP在Quantum级InfiniBand交换机内执行网络内归约。归约算术在交换机硅片中发生。BlueField DPU卸载网络，并可以托管控制服务，如子网管理器和SHARP聚合管理器。当启用NCCL RDMA SHARP插件且架构具有SHARP固件和活动的聚合管理器时，符合条件的集合可以卸载到IB交换机，减少主机和GPU开销。

> **In-network compute**: SHARP performs in-network reductions inside Quantum-class InfiniBand switches. The reduction arithmetic happens in the switch silicon. BlueField DPUs offload networking and can host control services such as the Subnet Manager and the SHARP Aggregation Manager. When the NCCL RDMA SHARP plugin is enabled and the fabric has SHARP firmware and an active Aggregation Manager, eligible collectives can be offloaded to the IB switches, reducing host and GPU overhead.

**I/O管理**：NVIDIA NetQ和统一架构管理器（UFM）等工具属于此类，为数据中心的I/O架构提供实时遥测、诊断和生命周期管理。

> **I/O management**: Tools like NVIDIA NetQ and Unified Fabric Manager (UFM) fall in this category, providing real-time telemetry, diagnostics, and lifecycle management for the data center's I/O fabric.

## 4.3 使用RDMA实现高速、低开销数据传输 (High-Speed, Low-Overhead Data Transfers with RDMA)

RDMA是一种针对低延迟、高吞吐量数据传输优化的技术。RDMA通过允许设备之间直接的内存到内存通信来工作，而不会给CPU带来不必要的数据复制操作负担。简而言之，RDMA绕过了传统内核网络栈的大部分内容，允许NIC直接读/写应用程序内存。这避免了CPU参与每个数据包，并减少了上下文切换和缓冲区复制。

> RDMA is a technology optimized for low-latency, high-throughput data transfers. RDMA works by allowing direct memory-to-memory communication between devices without burdening the CPU with unnecessary data-copy operations. In a nutshell, RDMA bypasses much of the traditional kernel network stack and allows a NIC to directly read/write application memory. This avoids CPU involvement in each packet and reduces context switches and buffer copies.

在Docker和Kubernetes等容器环境中，确保容器可以直接访问主机的InfiniBand设备（例如，/dev/infiniband）。否则，NCCL可能会静默回退到TCP套接字而不是GPUDirect RDMA——而且没有任何明显的错误来突出这种降级。这导致吞吐量从数十GB/s下降到仅几Gb/s，没有明显的错误消息。

> In container environments like Docker and Kubernetes, ensure the container has direct access to the host's InfiniBand devices (e.g., /dev/infiniband). Otherwise, NCCL may silently fall back to TCP sockets instead of GPUDirect RDMA—and without any obvious errors to highlight the degradation. This results in throughput dropping from tens of GB/s to only a few Gb/s, with no obvious error messages.

NVIDIA的GPU RDMA实现称为GPUDirect RDMA。GPUDirect RDMA让RDMA capable NIC（如InfiniBand和RDMA over Converged Ethernet（RoCE））在两台服务器之间直接对GPU设备内存执行直接内存访问（DMA）——完全绕过主机CPU和系统RAM。图4-3显示了使用RoCE的数据传输。

![图4-3 使用RoCE实现GPU到GPU直接数据传输](../assets/images/ch04/fig04_182_1.png)

> NVIDIA's RDMA implementation for GPUs is called GPUDirect RDMA. GPUDirect RDMA lets an RDMA-capable NIC such as InfiniBand and RDMA over Converged Ethernet (RoCE) perform direct memory access (DMA) to and from the GPU's device memory across two servers—bypassing host CPU and system RAM entirely. A data transfer with RoCE is shown in Figure 4-3.

通过向NIC注册GPU缓冲区，GPUDirect RDMA在远程GPU之间启用单边RDMA读取和写入。这最小化了多节点训练中的延迟和CPU开销。

> By registering GPU buffers with the NIC, GPUDirect RDMA enables one-sided RDMA reads and writes between remote GPUs. This minimizes both latency and CPU overhead in multinode training.

RDMA本身由InfiniBand支持，也由一些高速以太网网络通过RoCE支持。使用RoCE，假设网络设备支持RDMA并正确配置，你可以在以太网上获得类似RDMA的零拷贝传输。使用RoCE的RDMA通常需要正确配置的系统，包括必要的驱动程序，如用于InfiniBand/RoCE的NVIDIA OFED。

> RDMA is supported inherently by InfiniBand and also by some high-speed Ethernet networks through RoCE. With RoCE, you get RDMA-like zero-copy transfers over Ethernet, assuming the network gear supports RDMA and is properly configured for it. Using RDMA with RoCE usually requires a properly configured system with the necessary drivers, including NVIDIA OFED for InfiniBand/RoCE.

使用RDMA与标准TCP/IP网络之间的性能差异可能很大。例如，现代InfiniBand链路可能为小消息提供几微秒量级的延迟，而标准以太网上的TCP可能产生5-10倍更高的延迟。

> The performance difference between using RDMA versus standard TCP/IP networking can be huge. For example, a modern InfiniBand link might provide latency on the order of a few microseconds for a small message, whereas standard TCP over Ethernet might incur 5–10× higher latency.

## 4.4 调优多节点连接性 (Tuning Multinode Connectivity)

对于使用GPU的分布式、多节点训练，确保网络不是瓶颈至关重要。这涉及使用正确的通信和网络技术，以及正确配置这些技术。以下是一些采用的建议和需要避免的陷阱：

**了解拓扑**：使用`nvidia-smi topo -m`获取基本的GPU互连视图，但对于基于NVSwitch和NVLink的系统，还建议使用`nvidia-smi nvlink`或Nsight Systems来了解多跳交换架构连接性。

> **Understand the topology**: Use nvidia-smi topo -m to get a basic GPU interconnect view, but for NVSwitch- and NVLink-based systems, it's recommended to also use nvidia-smi nvlink or Nsight Systems to understand multihop switch fabric connectivity.

**利用NVLink Switch域（如果可用）**：多节点NVIDIA的GB200和GB300 NVL72机柜解决方案使用NVLink Switch在单个NVLink域中连接多达72个GPU，提供极低的每跳延迟——几百纳秒量级。GB200 NVL72架构在机柜内所有GPU之间提供高达约130 TB/s的全对全带宽和亚微秒延迟。如果你的集群包括此类基础设施，请确保你的作业放置在同一NVLink域内，以充分利用这种超快互连。这可以显著减少对节点间较慢的InfiniBand和以太网通信的需求。

> **Leverage NVLink Switch domains if available**: The multinode NVIDIA's GB200 and GB300 NVL72 rack solutions connect up to 72 GPUs in a single NVLink domain using NVLink Switch, which provides extremely low per-hop latency—on the order of a few hundred nanoseconds. The GB200 NVL72 architecture provides up to ~130 TB/s of all-to-all bandwidth with submicrosecond latencies across all GPUs in the rack. If your cluster includes such infrastructure, make sure your jobs are placed within the same NVLink domain to fully utilize this ultrafast interconnect. This can significantly reduce the need for slower InfiniBand and Ethernet communication between nodes.

**尽可能使用RDMA**：如果在InfiniBand或支持RoCE的硬件上运行，请确保你的通信库（如NCCL）实际使用RDMA。如果可用，NCCL将自动使用GPUDirect RDMA。但如果RDMA配置错误或不支持，NCCL可能会静默回退到TCP。一个警示信号是，如果你注意到在all-reduce操作期间，GPU利用率下降而CPU利用率飙升。这表明CPU正在为通信复制数据。

> **Use RDMA whenever possible**: If running on InfiniBand or RoCE-capable hardware, make sure your communication library, such as NCCL, is actually using RDMA. NCCL will automatically use GPUDirect RDMA if available. But if RDMA is misconfigured or unsupported, NCCL may silently fall back to TCP. One red flag for this is if you notice that during all-reduce operations, GPU utilization drops and CPU utilization spikes. This indicates that the CPU is copying data for communications.

**如果可用，聚合多个NIC的带宽**：某些服务器有多个网络接口（NIC）。NCCL可以在多个NIC之间条带化流量（称为多轨）以增加带宽。但你可能需要设置一些环境变量，如`NCCL_NSOCKS_PERTHREAD`和`NCCL_SOCKET_NTHREADS`来优化这一点。只需确保每个NIC在不同的子网上，并且NCCL可以发现两者。通过正确的设置，例如并行使用两个800 Gbps NIC，可以获得1.6 Tbps的NCCL流量聚合。四个这样的NIC链路（例如，两个双端口NIC）可以实现约3.2 Tbps。

> **Aggregate bandwidth with multiple NICs if available**: Some servers have multiple network interfaces (NICs). NCCL can stripe traffic across multiple NICs (called multirail) to increase bandwidth. But you may need to set some environment variables like NCCL_NSOCKS_PERTHREAD and NCCL_SOCKET_NTHREADS to optimize this. Just make sure that each NIC is on a different subnet and that NCCL can discover both. With proper setup, using two 800 Gbps NICs in parallel, for instance, gives an aggregate of 1.6 Tbps for NCCL traffic. And four such NIC links (e.g., two dual-port NICs) can achieve ~3.2 Tbps.

现代GPU系统支持GPU发起的网络，使用InfiniBand GPUDirect Async（IBGDA）和直接NIC路径，如图4-4所示。这让GPU可以在没有CPU干预的情况下驱动全带宽RDMA。

![图4-4 通过GPU和NIC之间的直接连接绕过CPU瓶颈](../assets/images/ch04/fig04_187_1.png)

> Modern GPU systems support GPU-initiated networking with InfiniBand GPUDirect Async (IBGDA) and the direct NIC path, as shown in Figure 4-4. This lets the GPU drive full-bandwidth RDMA without CPU intervention.

> Figure 4-4. Bypassing CPU bottlenecks with direct connectivity between GPUs and NICs

**检查配置错误**：一个常见的陷阱是网络配置不匹配导致回退到较慢的路径。例如，如果RDMA由于配置错误而不工作，NCCL可能正在100 Gbps以太网上使用TCP，但由于内核开销只能获得其中的一小部分。更糟糕的是，如果集群的高速网络被错误识别，流量可能通过仅运行10 Gbps以太网的较慢管理网络，而用户没有意识到。NCCL的调试输出和网络接口计数器（ibstat、ifstat）等工具可以帮助验证哪个接口使用更频繁。

> **Check for misconfigurations**: A common pitfall is a mismatch in network configuration that causes a fallback to a slower path. If RDMA is not working due to a misconfiguration, for instance, NCCL might be using TCP on a 100 Gbps Ethernet network but getting only a fraction of that due to kernel overhead. Even worse, if the cluster's high-speed network is misidentified, traffic might go over a slower management network running only 10 Gbps Ethernet without the user realizing. Tools like NCCL's debugging output and network interface counters (ibstat, ifstat) can help verify which interface is being used more heavily.

## 4.5 多节点通信陷阱 (Multinode Communication Pitfalls)

跨集群中的多个节点扩展训练引入了一类新的陷阱。这里我们强调一些常见问题，并演示如何用具体示例修复它们。

### 4.5.1 陷阱#1：使用CPU绑定的Gloo后端而不是NCCL (Pitfall #1: Using a CPU-bound Gloo Backend Instead of NCCL)

PyTorch的分布式框架支持多种通信后端。对于多GPU训练，NCCL是NVIDIA GPU的首选后端，但也有一个名为Gloo的回退后端，它使用CPU和TCP套接字。如果错误地使用Gloo初始化GPU训练的ProcessGroup——或者如果NCCL初始化失败并回退到Gloo，训练仍然可以正常工作，但所有跨GPU通信都将通过CPU和以太网栈。这导致极慢的性能。

> PyTorch's distributed framework supports multiple backends for communication. For multi-GPU training, NCCL is the preferred backend for NVIDIA GPUs, but there is also a fallback backend called Gloo, which uses CPUs and TCP sockets. If one mistakenly initializes ProcessGroup with Gloo for GPU training—or if NCCL fails to initialize and it falls back to Gloo, the training will still function correctly but all cross-GPU communication will go through the CPU and Ethernet stack. This results in extremely slow performance.

不幸的是，意外使用这种错误配置相当常见，因为代码看起来正常工作且不会崩溃。它只是运行慢一个数量级，需要分析器或仔细的日志分析才能检测到。总之，始终为多GPU训练指定NCCL以利用RDMA通信。幸运的是，这是PyTorch的默认设置。否则，回退到Gloo将静默限制你的性能（甚至完全出错）。

> Unfortunately, it's fairly common to accidentally use this misconfiguration since the code appears to work normally and not crash. It just runs an order of magnitude slower and requires either a profiler or careful log analysis to detect. In summary, always specify NCCL for multi-GPU training to utilize RDMA communication. Fortunately, this is PyTorch's default. Otherwise, falling back to Gloo will silently limit your performance (or even error out completely.)

### 4.5.2 陷阱#2：NCCL版本不匹配 (Pitfall #2: Mismatched NCCL Versions)

如果你运行PyTorch捆绑的NCCL（例如，`torch.cuda.nccl.version() == ()`）与系统安装的不同版本的libnccl，系统将挂起。或者更糟，你将静默回退到较慢的实现。这可能很难检测。确保通过对齐nvidia-nccl-cu*包或针对系统NCCL重新构建PyTorch来避免这些兼容性陷阱。

> If you run PyTorch's bundled NCCL (e.g., torch.cuda.nccl.version() == ()) against a different version of the system-installed libnccl, you will hang the system. Or worse, you will silently fall back to a slower implementation. This can be difficult to detect. Make sure you have alignment by matching nvidia-nccl-cu* packages or rebuilding PyTorch against the system NCCL and avoid these compatibility pitfalls.

### 4.5.3 陷阱#3：NCCL引导期间TCP端口耗尽 (Pitfall #3: TCP Port Exhaustion During NCCL Bootstrap)

NCCL使用临时TCP端口进行其带外设置，如果你的OS的`net.ipv4.ip_local_port_range`太窄，你可能会耗尽可用端口，导致握手失败或停滞。建议在`/proc/sys/net/ipv4/ip_local_port_range`中扩大端口范围（例如，50000 51000）以避免隐藏的引导失败。请注意，现代NCCL版本已改进引导处理，但在大型集群上主动设置宽端口范围仍然是最佳做法。

> NCCL uses ephemeral TCP ports for its out-of-band setup, and if your OS's net.ipv4.ip_local_port_range is too narrow, you can exhaust available ports, causing failed or stalled handshakes. It's recommended that you widen your port range in /proc/sys/net/ipv4/ip_local_port_range (e.g., 50000 51000) to avoid hidden bootstrap failures. Note that modern NCCL versions have improved bootstrap handling, but it's still best to proactively set a broad port range on large clusters.

### 4.5.4 陷阱#4：网络带宽不足或NIC配置错误 (Pitfall #4: Insufficient Network Bandwidth or Misconfigured NICs)

另一个多节点陷阱是根本没有足够的网络带宽用于同步的数据量——或者没有使用所有可用接口。随着GPU集群扩展，这个陷阱变得更加常见。例如，使用Blackwell GPU很容易使每个节点单个400 Gbps链路饱和。

> Another multinode pitfall is simply not having enough network bandwidth for the amount of data being synced—or not using all of the available interfaces. This pitfall becomes more common as GPU clusters scale. For example, saturating a single 400 Gbps link per node is easy with Blackwell GPUs.

在这些不幸条件下分析工作负载时，你会观察到扩展到多个节点显著减慢训练。换句话说，"每GPU吞吐量"将下降。在这种情况下，检查网络链路。

> When profiling your workload under these unfortunate conditions, you will observe that scaling to multiple nodes significantly slows down training. In other words, the "per-GPU throughput" will drop. In this case, check the network links.

### 4.5.5 陷阱#5：落后节点或进程 (Pitfall #5: Straggler Nodes or Processes)

在多节点训练中，最慢的节点或GPU将决定整体速度，因为同步需要等待每个节点和GPU响应。如果一台机器的网络链路较慢，或过载其他任务，它将拖慢整个作业。

> In multinode training, the slowest node, or GPU, will determine the overall pace because synchronization needs to wait for every node and GPU to respond. If one machine has a slower network link, or is overloaded with other tasks, it will slow down the entire job.

为避免落后者，重要的是尽可能使用同构硬件并为每个训练作业使用专用集群资源。这样，你的环境是可预测的。例如，如果你在云环境中运行，混合不同实例类型或使用不同的交换架构可能会引入变异性。

> To avoid stragglers, it's important to use homogeneous hardware and dedicated cluster resources for each training job, if possible. This way, your environment is predictable. If you're running in a cloud environment, for instance, mixing different instance types or using different switch fabrics can introduce variability.

### 4.5.6 陷阱#6：UCX/RDMA下的GPU内存碎片化 (Pitfall #6: GPU Memory Fragmentation Under UCX/RDMA)

PyTorch的缓存分配器在迭代之间保留GPU内存。在使用UCX/RDMA的分布式设置中，这些长期存在的分配可能会耗尽注册池或使内存碎片化，导致零星的分配失败或性能悬崖。监控`torch.cuda.memory_reserved()`与`memory_allocated()`有助于发现这些边缘情况：

```python
import torch
import torch.distributed as dist
import os
import time

def log_mem(iteration):
    reserved = torch.cuda.memory_reserved()
    allocated = torch.cuda.memory_allocated()
    print(f"[Iter {iteration:02d}] Reserved: {reserved/1e9:.3f} GB, "
          f"Allocated: {allocated/1e9:.3f} GB")

def run(rank, world_size):
    dist.init_process_group(backend="nccl", init_method="env://")
    rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(rank)

    big_buffer = torch.empty(int(2e8), device=rank)  # ~0.8 GB
    log_mem(0)

    for i in range(1, 11):
        small = torch.randn(int(1e7), device=rank)  # ~40 MB
        medium = torch.randn(int(5e7), device=rank) # ~200 MB

        del small, medium
        torch.cuda.synchronize()

        log_mem(i)

        dist.barrier()
        time.sleep(0.1)

    dist.destroy_process_group()

if __name__ == "__main__":
    run(0, 1)
```

> PyTorch's caching allocator holds onto GPU memory across iterations. In distributed settings using UCX/RDMA, these long-lived allocations can exhaust registration pools or fragment memory, causing sporadic allocation failures or performance cliffs. Monitoring torch.cuda.memory_reserved() versus memory_allocated() helps surface these edge cases:

## 4.6 用于分布式多GPU通信的NCCL (NCCL for Distributed Multi-GPU Communication)

NVIDIA NCCL是一个多对多通信库，用于称为集合的操作，由GPU组用于共享数据。NCCL支撑着NVIDIA生态系统中大多数多GPU训练工作负载。

> NVIDIA NCCL is a many-to-many communication library for operations, called collectives, used by groups of GPUs to share data. NCCL underpins most multi-GPU training workloads in NVIDIA's ecosystem.

NCCL提供集合通信操作的优化实现，如all-reduce、all-gather、broadcast和reduce-scatter，可以从几个GPU扩展到数千个，甚至有一天数百万个。当跨多个GPU执行模型训练和推理时，必须快速交换模型权重、梯度和激活等数据，以保持GPU忙碌。NCCL是高效协调这些交换的库。

> NCCL provides optimized implementations of collective communication operations like all-reduce, all-gather, broadcast, and reduce-scatter that scale from a few GPUs to many thousands and, someday, millions. When performing model training and inference across multiple GPUs, data such as model weights, gradients, and activations must be exchanged quickly to keep the GPUs busy. NCCL is the library that orchestrates these exchanges efficiently.

### 4.6.1 NCCL中的拓扑感知 (Topology Awareness in NCCL)

拓扑感知在NCCL性能中起着重要作用。NCCL检测GPU如何物理连接，并相应地优化其通信模式。例如，在完全连接的NVLink和NVSwitch系统中，每个GPU将使用这些高速互连与每个其他GPU通信。

> Topology awareness plays a major role in NCCL's performance. NCCL detects how GPUs are physically connected and optimizes its communication pattern accordingly. For example, in a system of fully connected NVLink and NVSwitches, every GPU will communicate with every other GPU using these high-speed interconnects.

虽然NCCL可以使用简单的模式通信（如环all-reduce）与每个链路平等通信，但它会自动使用拓扑感知的层次通信模式来最大化通信性能。例如，对于具有多个NUMA节点域的系统，NCCL可能首先执行节点内归约，然后执行跨节点归约，然后执行节点内广播，这实际上是层次all-reduce。目标是在最快的互连上最大化流量。

> While NCCL can use a simple pattern communication like ring all-reduce to communicate with each link equally, it will automatically use a topology-aware hierarchical communication pattern to maximize communication performance. For systems with multiple NUMA node domains, for instance, NCCL might first do an intranode reduce, then a cross-node reduce, then an intranode broadcast, which is effectively a hierarchical all-reduce. The goal is to maximize traffic over the fastest interconnects.

### 4.6.2 NCCL通信算法 (NCCL Communication Algorithms)

在内部，NCCL可以根据数据大小、GPU数量和拓扑采用不同的通信算法。NCCL用于集合的主要算法包括Ring、Tree、CollTree、CollNet和并行聚合树（PAT）等：

**Ring（环）**：在环all-reduce中，GPU在逻辑上排列成环。每个GPU以流水线方式向其邻居发送数据并从其另一个邻居接收数据。对于all-reduce，数据的每个块将在环中循环，累积部分和。环算法具有良好的属性，即完美平衡网络负载，使每个GPU发送和接收完全相同的数据量。它是带宽最优的，因为每个链路在all-reduce集合中传输2 × (data_size ÷ num_gpus)字节。缺点是延迟。总时间随GPU数量扩展，因为数据必须遍历所有跳。环通常对大消息很有利，因为当你发送非常大的消息时，实际移动字节所花费的时间远远超过实际开始传输的成本。这被称为带宽主导的工作负载。

> **Ring**: In the ring all-reduce, GPUs are logically arranged in a ring. Each GPU sends data to its neighbor and receives data from its other neighbor in a pipelined fashion. For an all-reduce, each chunk of the data will circulate around the ring, accumulating partial sums. The ring algorithm has the nice property that it perfectly balances network load such that each GPU sends and receives exactly the same amount of data. It is bandwidth-optimal, as each link transmits 2 × (data_size ÷ num_gpus) bytes in an all-reduce collective. The downside is latency. The total time scales with the number of GPUs since the data has to traverse all hops. Ring is often great for large messages because, when you send very large messages, the time spent actually moving bytes far outweighs the cost of actually starting the transfer. This is known as a bandwidth-dominated workload.

**Tree和NVLSTree（树）**：在树算法中，归约和广播使用生成树算法在树结构中完成。all-reduce实际上是reduce-scatter后跟广播。树可以在N个GPU的O(log N)步中完成all-reduce——而不是环的O(N)。因此，树算法为较小的消息提供更低的延迟。但是，它可能无法为大数据充分利用所有链路，因为并非所有GPU都一直在传输。一些GPU是树的叶子，只向树发送一次。NCCL的树算法经过优化，通常用于总时间由传输启动延迟主导的较小消息大小。这被称为延迟主导的工作负载——与环算法的大消息带宽主导用例形成对比。使用NVLSTree将启用NVLink SHARP卸载。

> **Tree and NVLSTree**: In the tree algorithm, reductions and broadcasts are done in a tree structure using the spanning tree algorithm. An all-reduce is actually a reduce-scatter followed by a broadcast. A tree can complete an all-reduce in O(log N) steps for N GPUs—as opposed to O(N) for the ring. As such, a tree algorithm provides lower latency for smaller messages. However, it may not fully utilize all links for large messages because not all GPUs transmit all the time. Some GPUs are leaves of the tree and send only one time up the tree, for instance. NCCL's tree algorithm is optimized and often used for smaller message sizes in which the total time is dominated by the transfer-startup latency. This is known as a latency-dominated workload—in contrast to the ring algorithm's bandwidth-dominated use case for large messages. Using NVLSTree will enable NVLink SHARP offload.

**CollTree（层次树集合）**：CollTree构建两级树以减少延迟同时保持高本地带宽。在每个快速本地域内（例如，节点上的所有GPU或一个NVSwitch岛内），它形成本地树并执行reduce-scatter后跟广播。然后每个本地组的单个领导者通过RDMA参与跨组的二级树。两级是流水线的，以便完成本地阶段的张量块可以立即进入跨组阶段。与平面环相比，这将跨节点步骤数减少到O(log N)并缩短了中小消息的延迟。同时，它仍在节点内使用全带宽。在NVLink域上，本地树通过NVSwitch路由，并可以在可用时使用NVLink SHARP进行交换机内聚合。在InfiniBand架构上，当启用NCCL SHARP插件时，跨组树可以卸载到SHARP。对于非常大的消息，Ring或并行聚合树（PAT，稍后讨论）可能实现更高的峰值吞吐量，而当跨节点延迟占主导时，CollTree更受青睐。

> **CollTree (hierarchical tree collectives)**: CollTree builds a two level tree to reduce latency while preserving high local bandwidth. Within each fast local domain (e.g., all GPUs on a node or within one NVSwitch island), it forms a local tree and performs a reduce scatter followed by a broadcast. A single leader from each local group then participates in a second-level tree across groups over RDMA. The two levels are pipelined so that tensor chunks that finish the local phase can immediately enter the cross group phase. Compared with a flat ring, this reduces the number of cross node steps to O(log N) and shortens latency for small and medium messages. At the same time, it still uses full bandwidth inside the node. On NVLink domains, the local tree is routed over NVSwitch and can use NVLink SHARP for in-switch aggregation when available. On InfiniBand fabrics, the cross group tree can be offloaded to SHARP when the NCCL SHARP plugin is enabled. For very large messages, Ring or parallel aggregated tree (or PAT, discussed later in this list) may achieve higher peak throughput while CollTree is preferred when cross node latency dominates.

**CollNet（跨节点层次集合）**：CollNet，也称为树并行性，结合两种集合策略以在不同规模优化通信。首先，它将共享快速本地互连的GPU分组，例如单个节点中的所有GPU或NVSwitch岛内。然后CollNet应用高吞吐量算法（如环或本地树）来聚合每组GPU内的数据。每组的一个指定领导者GPU参与跨组的二级树归约。这最小化了跨组通信轮数。通过在全局树交换之上分层本地归约，CollNet既提供跨节点传输的低延迟，又提供节点内流量的高带宽。这使其在减少非常大、多节点GPU集群中的网络负载方面特别有效。

> **CollNet (hierarchical collectives across nodes)**: CollNet, also known as tree parallelism, combines two collective strategies to optimize communication at different scales. First, it groups GPUs that share a fast local interconnect, such as all GPUs in a single node or within an NVSwitch island. CollNet then applies a high-throughput algorithm such as a ring or local tree to aggregate the data within each group of GPUs. One designated leader GPU from each group participates in the second-level tree reduction across groups. This minimizes the number of cross-group communication rounds. By layering a local reduction on top of a global tree exchange, CollNet delivers both low latency for internode transfers and high bandwidth for intranode traffic. This makes it especially effective at reducing network load in very large, multinode GPU clusters.

**并行聚合树（PAT）**：PAT是NCCL的环和树算法的流水线混合。一旦张量的一个段在其GPU树上完成归约，下一个段就以交错、轮询方式同时开始自己的树归约。这种连续归约阶段的重叠让PAT保持链路饱和，并实现接近纯环all-reduce的带宽。同时，它将传输启动延迟限制为每段O(log N)，类似于树算法。在实践中，PAT将大消息分成多个块，在块1上启动基于树的reduce-scatter，然后立即在块2上发出相同操作，依此类推。这交错进行，以便始终有工作进行中。结果是大数据传输的接近环级吞吐量加上较小段的树级延迟优势。这是两全其美的方法。

> **Parallel aggregated tree (PAT)**: PAT is NCCL's pipelined hybrid of ring and tree algorithms. As soon as one segment of the tensor has been reduced across its tree of GPUs, the next segment simultaneously begins its own tree reduction in a staggered, round-robin manner. This overlap of successive reduction phases lets PAT keep links saturated and achieve bandwidth close to a pure ring all-reduce. At the same time, it bounds transfer-startup latency to O(log N) per segment, similar to the tree algorithm. In practice, PAT splits a large message into multiple chunks, launches a tree-based reduce-scatter on chunk 1, then immediately issues the same on chunk 2, and so on. This interleaves so that there is always work in flight. The result is near–ring-level throughput for large data transfers plus tree-level latency advantages for smaller segments. It's a best-of-both-worlds approach.

## 4.7 分布式数据并行策略 (Distributed Data Parallel Strategies)

在实践中，大规模训练和推理数十亿和数万亿参数模型需要结合并行策略，包括数据并行、张量模型并行、流水线并行、专家并行、上下文并行等。这些是线性扩展训练集群所必需的，不会因过多开销而浪费GPU资源。

> In practice, large-scale training and inferencing of multi-billion- and multi-trillion-parameter models requires a combination of parallelism strategies, including data parallel, tensor model parallel, pipeline parallel, expert parallel, context parallel, etc. These are required to scale your training clusters linearly and not waste GPU resources with excessive overhead.

关键是在每个层次上将通信与计算重叠。这包括使用NCCL进行all-reduce和使用NIXL进行一对一传输。使用这些机制，你可以高效地扩展到数千甚至数百万个GPU。

> The key is to overlap communication with computation at every level. This includes using NCCL for all-reduce and NIXL for one-to-one transfers. Using these mechanisms, you can scale to thousands and millions of GPUs with high efficiency.

### 4.7.1 数据并行（DP）与分布式数据并行（DDP）

当在单个节点上扩展到多个GPU时，PyTorch在框架级别提供数据并行（拆分数据）和模型并行（拆分模型）方法。我们将在后面的章节中更详细地介绍这些，但现在，让我们从系统性能角度比较两种最基本的数据并行策略：`nn.DataParallel`（DP）和`torch.distributed.DistributedDataParallel`（DDP）。了解它们的差异很重要，因为选择错误的策略会严重影响性能：

**数据并行（DP）**：DP是一个易于使用的API，涉及单个进程或单个Python线程控制多个GPU。该模块自动将每个输入批次拆分到可用GPU上。然后它在每个拆分上执行前向传播，将输出收集回主GPU，并计算聚合损失。最后，在反向传播期间，它将梯度收集回主GPU，平均它们，并广播回其他GPU。

在DataParallel中，整个训练循环是单进程的。这使得集成更简单，因为不需要启动多个进程。尽管DP使用多线程，但它受Python的GIL（全局解释器锁）限制，用于在不同设备上启动操作。因此，DP不能很好地扩展超过2-4个GPU，因为单个Python线程成为瓶颈，GPU利用率受到影响。此外，DP中的梯度收集步骤是同步的，不与计算重叠。这意味着它的行为类似于我们之前描述的"无重叠"场景。

> **Data parallelism (DP)**: DP is an easy-to-use API that involves a single process, or single Python thread, controlling multiple GPUs. The module automatically splits each input batch across the available GPUs. It then performs forward passes on each split, gathers the outputs back to the main GPU, and computes the aggregated loss. Finally, during the backward pass, it gathers gradients back to the main GPU, averages them, and broadcasts back to the others. In DataParallel, the entire training loop is single-process. This makes it simpler to integrate since there is no need to launch multiple processes. Even though DP uses multithreading, it is limited by Python's GIL (Global Interpreter Lock) for launching operations on different devices. As such, DP does not scale well beyond 2–4 GPUs because the single Python thread becomes a bottleneck and the GPU utilization suffers. Additionally, the gradient gathering step in DP is synchronous and does not overlap with computation. This means it behaves similarly to our "no overlap" scenario described earlier.

**分布式数据并行（DDP）**：DDP每个GPU设备使用一个进程，并依赖NCCL来通信梯度。像大多数简单的数据并行策略（FSDP是例外）一样，每个进程都有自己的模型副本。

在反向传播期间，梯度在GPU之间直接交换或all-reduce。这种all-reduce通信通常与反向计算重叠，这是理想的，正如我们之前讨论的。

DDP通过使用单独的进程完全避免了GIL问题。NCCL的高效C++内核处理通信。结果是DDP几乎总是比DP在多GPU训练中表现更好。事实上，PyTorch开发者建议在任何严肃的多GPU工作中使用DistributedDataParallel而不是DataParallel，因为DP的Python线程受可怕的GIL限制，经常成为瓶颈。

> **Distributed Data Parallel (DDP)**: DDP uses one process per GPU device and relies on NCCL to communicate gradients. Like most simple data parallel strategies (FSDP being the exception), each process has its own copy of the model. During the backward pass, gradients are exchanged, or all-reduced, directly among GPUs. This all-reduce communication is typically overlapped with the backward computation, which is ideal, as we discussed earlier. DDP avoids the GIL issue entirely by using separate processes. And NCCL's efficient C++ kernels handle communication. The result is that DDP nearly always outperforms DP for multi-GPU training. In fact, PyTorch developers recommend using DistributedDataParallel over DataParallel for any serious multi-GPU work because DP's Python threading, limited by the dreaded GIL, often becomes a bottleneck.

## 4.8 NCCL通信器生命周期和环境陷阱 (NCCL Communicator Lifecycle and Environment Gotchas)

虽然NCCL抽象了大多数低级细节，但我们在代码中使用NCCL的方式仍然会影响性能。此外，NCCL有许多环境变量来控制其行为。错误配置这些变量可能会降低性能甚至导致挂起。

### 4.8.1 陷阱#1：过于频繁创建NCCL通信器 (Pitfall #1: Creating NCCL Communicators Too Often)

NCCL通信器代表一组可以集体通信的GPU或rank。使用C++的`ncclCommInitRank`或PyTorch的`torch.distributed.init_process_group`创建通信器是一个昂贵的操作。初始化器要求所有rank相互交换信息，包括唯一ID、网络地址等。它们还设置环/树并分配缓冲区。

如果你的代码重复初始化NCCL通信器，你每次都要付出沉重的代价。考虑一个有32个GPU的系统。如果你创建32个单独的NCCL通信器，每个rank一个，这可能需要2-3分钟，而不是2-3秒（或更快）。通信器初始化可能具有比rank数量更差的线性扩展性，因为它通常需要所有GPU之间的全对全握手和协调。

> An NCCL communicator represents a group of GPUs, or ranks, that can communicate collectively. Creating a communicator with either C++'s ncclCommInitRank in C++ or PyTorch's torch.distributed.init_process_group is an expensive operation. Initializers require that all ranks exchange information with one another, including unique IDs, network addresses, etc. They also set up rings/trees and allocate buffers. If your code repeatedly initializes NCCL communicators, you'll pay a heavy cost each time. Consider a system with 32 GPUs. If you create 32 separate NCCL communicators, one per rank, this could require 2–3 minutes as opposed to 2–3 seconds (or quicker). Communicator initialization can have worse-than-linear scaling with number of ranks because it often requires all-to-all handshakes and coordination among the many GPUs.

### 4.8.2 陷阱#2：不要在每次迭代中创建和销毁NCCL通信器 (Pitfall #2: Do Not Create and Destroy NCCL Communicators on Every Iteration)

因为NCCL通信器初始化如此昂贵，所以在为模型并行和流水线并行定义进程子组时要小心不要意外创建新的NCCL通信器。相反，在开始时使用PyTorch的`torch.distributed.new_group()`创建子通信器并重用这些通信器。永远不要在每次迭代中创建和销毁通信器。

> Because NCCL communicator initialization is so expensive, be careful not to accidentally create new NCCL communicators when defining subgroups of processes for model parallelism and pipeline parallelism. Instead, create the subcommunicators once at the beginning using PyTorch's torch.distributed.new_group() and reuse these communicators. Never create and destroy communicators on every iteration.

### 4.8.3 陷阱#3：避免使用环境变量过度调优或禁用NCCL功能 (Pitfall #3: Avoid Overtuning or Disabling NCCL Features with Environment Variables)

NCCL有许多环境变量，如`NCCL_BUFFSIZE`、`NCCL_NSOCKS_PERTHREAD`、`NCCL_P2P_LEVEL`、`NCCL_SHM_DISABLE`等。通常最好将它们保留为默认值，除非你有特定原因。更好的是，将它们的值设置为当前默认值以明确并不依赖默认值！务必查看发行说明并相应调整值。

> NCCL has many environment variables such as NCCL_BUFFSIZE, NCCL_NSOCKS_PERTHREAD, NCCL_P2P_LEVEL, NCCL_SHM_DISABLE, etc. It's usually best to leave them at their defaults unless you have a specific reason. Better yet, set their values to their current defaults to be explicit and not rely on defaults! Be sure to review the release notes and adjust the values accordingly.

### 4.8.4 陷阱#4：验证NCCL线程的CPU-GPU NUMA节点亲和性 (Pitfall #4: Verify CPU-GPU NUMA-Node Affinity for NCCL Threads)

NCCL为网络轮询和内核调度启动后台CPU线程。当你使用`torch.multiprocessing`或消息传递接口（MPI）启动多个进程时，每个进程继承一个CPU亲和性掩码，该掩码可能针对所有核心或仅针对使用taskset或numactl等工具绑定的子集。

NCCL通常将其线程分配给离它们服务的GPU最近的核心，但如果进程被绑定到一组狭窄的核心，它可能会将所有NCCL线程折叠到单个核心上，并遭受调度不良和吞吐量低的问题。为防止这种情况，设置环境变量`NCCL_IGNORE_CPU_AFFINITY=1`，以便NCCL忽略继承的CPU亲和性掩码，并自由地将其工作线程分布在本地NUMA域中的核心上。推荐的方法是将每个GPU进程绑定到其NUMA域的CPU核心，然后设置`NCCL_IGNORE_CPU_AFFINITY=1`，以便NCCL可以在这些核心内微调线程放置。

> NCCL launches background CPU threads for network polling and kernel dispatch. When you start multiple processes with torch.multiprocessing or Message Passing Interface (MPI), each process inherits a CPU affinity mask that may target all cores or only a subset if bound with tools such as taskset or numactl. NCCL normally assigns its threads to the cores nearest the GPUs they serve, but if the process is pinned to a narrow set of cores, it may collapse all NCCL threads onto a single core and suffer from poor scheduling and low throughput. To prevent this, set the environment variable NCCL_IGNORE_CPU_AFFINITY=1 so that NCCL ignores the inherited CPU affinity mask and freely spreads its worker threads across the cores in the local NUMA domain. The recommended approach is to bind each GPU process to the CPU cores for its NUMA domain and then set NCCL_IGNORE_CPU_AFFINITY=1 so that NCCL can fine-tune thread placement within those cores.

### 4.8.5 陷阱#5：抵制忽略NCCL警告和错误的诱惑 (Pitfall #5: Resist the Temptation to Ignore NCCL Warnings and Errors)

如果谨慎启用日志记录，NCCL会打印许多日志。在这些日志中，可能会有关于回退到较慢PCIe带宽等警告。这些是需要解决的重要警告。

如果你看到类似"unable to enable P2P, falling back to copy"的日志，不要忽略这些！它们通常表示次优条件。如果你看到此警告，这意味着NCCL无法在两个GPU之间建立直接GPU P2P。可能是因为它们在不同的PCIe根复合体上且不支持。这意味着数据传输将慢得多，因为它们必须通过主机CPU内存缓冲区传输。

> NCCL prints many logs if logging is judiciously enabled. In those logs, there could be warnings about falling back to slower PCIe bandwidth, for instance. These are important warnings to address. If you see logs like "unable to enable P2P, falling back to copy," don't ignore these! They often indicate suboptimal conditions. If you see this warning, it means that NCCL was unable to establish direct GPU P2P between two GPUs. Perhaps because they're on different PCIe root complexes with no support. This means that data transfers will be much slower, as they must travel through host CPU memory buffers.

### 4.8.6 陷阱#6：NCCL通信器挂起、错误或完全关闭 (Pitfall #6: NCCL Communicator Hangs, Errors, or Shuts Down Completely)

偶尔，如果一个进程崩溃或一个GPU rank遇到错误，NCCL通信器可能会挂起其他rank，因为集合无法完成。不幸的是，这在大型集群中相当常见，因为如Meta在表4-4中描述的那样，规模上GPU故障的频率相对较高。

表4-4显示了Llama 3 405B预训练54天期间意外中断的根本原因分类：

| 组件 | 类别 | 中断次数 | 中断百分比 |
|------|------|----------|------------|
| GPU故障 | GPU | 148 | 30.1% |
| GPU HBM3内存 | GPU | 72 | 17.2% |
| 软件bug | 依赖 | 54 | 12.9% |
| 网络交换机/电缆 | 网络 | 35 | 8.4% |
| 主机维护 | 非计划维护 | 32 | 7.6% |
| GPU SRAM内存 | GPU | 19 | 4.5% |
| GPU系统处理器 | GPU | 17 | 4.1% |
| NIC | 主机 | 7 | 1.7% |
| NCCL看门狗超时 | 未知 | 7 | 1.7% |
| 静默数据损坏 | GPU | 6 | 1.4% |
| GPU热接口和传感器 | GPU | 6 | 1.4% |

> Occasionally, if a process crashes or one GPU rank hits an error, NCCL communicators might hang the other ranks since the collectives won't complete. Unfortunately this is quite common in large-scale clusters given the relatively high frequency of GPU failures at scale, as described by Meta in Table 4-4.

启用`NCCL_ASYNC_ERROR_HANDLING=1`可以通过允许NCCL异步中止错误来提高弹性，但这可能会产生轻微的开销。PyTorch在最近版本中当你使用`init_process_group`时默认设置此值。但是，为了清晰和可重复性，最好显式设置此值。

> Enabling NCCL_ASYNC_ERROR_HANDLING=1 can improve resiliency by allowing NCCL to abort on errors asynchronously, but this may incur a slight overhead. PyTorch sets this by default in recent versions when you use init_process_group. However, it's a good idea to keep explicitly setting this value for clarity and reproducibility.

## 4.9 网络内SHARP聚合 (In-Network SHARP Aggregation)

当使用支持网络内计算的先进网络硬件（如NVIDIA的可扩展层次聚合和归约协议（SHARP））时，可以通过将集合操作的部分卸载到网络本身来实现额外的性能提升。

SHARP是一种InfiniBand网络内归约技术，与使用NCCL-SHARP插件的Quantum级InfiniBand交换机一起使用。在NVLink域中，类似的功能是NVLink SHARP（NVLS），它在NVSwitch架构内卸载集合。在现代NVLink Switch域（例如，NVL72）中，NVLS加速集合，并在域内（例如，72-GPU NVL72域）实现高效的全对全和广播。

> When using advanced network hardware that supports in-network computing, such as NVIDIA's Scalable Hierarchical Aggregation and Reduction Protocol (SHARP), additional performance gains can be realized by offloading parts of collective operations to the network itself. SHARP is an InfiniBand in-network reduction technology used with Quantum-class InfiniBand switches using the NCCL-SHARP plugin. In NVLink domains, the analogous capability is NVLink SHARP (NVLS), which offloads collectives within the NVSwitch fabric. In modern NVLink Switch domains (e.g., NVL72), NVLS accelerates collectives and enables efficient all-to-all and broadcast across the domain (e.g., 72-GPU NVL72 domain).

实际上，SHARP使all-reduce等集合可以由网络架构部分计算。当来自多个GPU的数据流入交换机时，它将归约/聚合（例如，求和）数据并共享部分归约的结果。这节省了每个GPU必须冗余传输许多中间结果到其他GPU。这减少了每个GPU必须处理的总体数据量，从而减少了大型MPI和NCCL集合的延迟。

> In practical terms, SHARP enables collectives such as all-reduce to be partially computed by the network fabric. As data from multiple GPUs flows into the switch, it will reduce/aggregate (e.g., sum) the data and share the partially reduced result. This saves each GPU from having to redundantly transfer many intermediate results between other GPUs. This reduces the overall amount of data that each GPU must handle, which reduces latency for large MPI and NCCL collectives.

SHARP的性能影响可能是巨大的。在某些情况下，NVIDIA报告使用SHARP的大型AI系统上all-reduce有2倍到5倍的加速。

> SHARP's performance impact can be substantial. In some cases, NVIDIA reports 2× to 5× speedups for all-reduce on large-scale AI systems using SHARP.

## 4.10 NVIDIA的NIXL和分解推理 (NVIDIA's NIXL and Disaggregated Inference)

虽然NCCL擅长训练期间常用的多对多组通信模式，但现代大规模AI推理引入了新的通信需求。NVIDIA的NIXL是一个开源、高吞吐量、低延迟的点对点通信库，于2025年初发布。

NIXL专门设计用于加速大规模LLM分布式和分解推理。我们将介绍分解推理如何将推理的不同阶段分离到单独的工作器中。分解推理使用NIXL在这些阶段之间跨GPU进行快速数据交换，延迟和开销最小。

> While NCCL excels at many-to-many group communication patterns often used during model training, modern AI inference at scale has introduced new communication needs. NVIDIA's NIXL is an open source, high-throughput, low-latency, point-to-point communication library released in early 2025. NIXL was designed specifically to accelerate large-scale LLM distributed and disaggregated inference. We'll cover how disaggregated inference separates different stages of inference into separate workers. Disaggregating inference uses NIXL for fast data exchange between these stages across GPUs with minimal latency and overhead.

NIXL是NVIDIA开源Dynamo推理引擎的核心组件。NIXL简化了一对一和一对少数据传输，例如以最小延迟和开销移动键值（KV）缓存（由分解阶段共享）。它补充了主要用于多对多集合的NCCL。

> NIXL is a core component of NVIDIA's open source Dynamo inference engine. NIXL streamlines one-to-one and one-to-few data transfers such as moving a key-value (KV) cache (shared by the disaggregated stages) with minimal latency and overhead. It complements NCCL, which is mainly used for many-to-many collectives.

NIXL具有一致的异步API，用于在GPU、CPU、SSD和共享网络存储之间移动数据。它始终为每个缓存数据块的放置选择最快的路径。这个层次结构如图4-5所示，在NVIDIA Dynamo的KV缓存管理器上下文中，该管理器使用NIXL为每个KV缓存传输选择可用的最快路径。

![图4-5 NVIDIA Dynamo分布式KV缓存管理器将访问频率较低的KV缓存卸载到更经济的内存层次结构](../assets/images/ch04/fig04_230_1.png)

> NIXL has a consistent asynchronous API for moving data across GPUs, CPUs, SSDs, and shared network storage. It always picks the fastest path for the placement of each cache chunk of data being moved. This hierarchy is shown in Figure 4-5 in the context of NVIDIA Dynamo's KV Cache Manager, which uses NIXL to choose the fastest path available for each KV cache transfer.

> Figure 4-5. NVIDIA Dynamo Distributed KV Cache Manager offloads less frequently accessed KV cache to more economical memory hierarchies (source: https://oreil.ly/nsxdl)

### 4.10.1 分离的预填充和解码推理阶段 (Separate Prefill and Decode Inference Stages)

我们将在后面的章节中更深入地探讨高度调优的推理系统的性能细节，但在继续讨论NIXL之前了解一些背景很重要。基于transformer的模型的推理路径实际上分为两个不同的阶段：预填充和解码。

第一阶段，预填充，通常是计算绑定的，因为它使用许多矩阵乘法从传入请求数据（即提示）构建KV缓存。第二阶段，解码，通常是内存吞吐量绑定的，因为它需要从GPU HBM内存收集模型权重来计算下一组token（即完成或响应）。

> We will dive deeper into the performance details of a highly tuned inference system in a later chapter, but it's important to understand a bit of context before going further with NIXL. The inference path of a transformer-based model is actually split into two different stages: prefill and decode. The first stage, prefill, is often compute bound as it uses many matrix multiplications to build the KV cache from the incoming request data (aka prompt). The second stage, decode, is often memory-throughput bound, as it needs to gather the model weights from GPU HBM memory to calculate the next set of tokens (aka completion or response).

这种预填充/解码拆分在常见推理引擎vLLM、SGLang和NVIDIA的Dynamo和TensorRT-LLM中实现。预填充（提示摄入）创建KV缓存，解码（生成）使用此缓存。NIXL专门加速此工作流中节点之间KV缓存的传输。图4-6比较了传统的"单体"服务模型与"分解"服务模型，其中两个阶段在不同的基于GPU的计算节点上运行以增加规模并最大化吞吐量。

![图4-6 分解服务将预填充和解码阶段分离到不同的GPU集群](../assets/images/ch04/fig04_233_1.png)

> This prefill/decode split is implemented in common inference engines vLLM, SGLang, and NVIDIA's Dynamo and TensorRT-LLM. The prefill (prompt ingestion) creates the KV cache, and the decode (generation) uses this cache. NIXL specifically accelerates the transfer of the KV cache between nodes in this workflow. Figure 4-6 compares the traditional "monolithic" serving model to the "disaggregated" serving model in which two stages run on different GPU-based compute nodes to increase scale and maximize throughput.

### 4.10.2 KV缓存传输的智能互连路由 (Intelligent Interconnect Routing for KV Cache Transfers)

传统上，人们可能通过CPU将此数据从GPU传输到GPU，但这会太慢，正如我们已经讨论的那样。另一个提高性能的选择是要求源和目标GPU在同一计算节点上，但这限制了我们的扩展灵活性。

NIXL是为解决此问题而创建的。它专为跨GPU、计算节点和机架（如有必要）的大型有效载荷（如KV缓存）的直接GPU到GPU传输而设计。

NIXL以高带宽运行，并尽可能将通信与计算重叠。这允许目标GPU在从源GPU接收KV缓存时开始生成下一组token。

此外，NIXL是互连无关的。如果GPU在同一计算节点中，它将使用NVLink；如果在同一计算节点中，使用NVSwitch；跨节点使用带RDMA的InfiniBand或以太网，甚至需要时使用PCIe或NVMe。与NCCL类似，NIXL将始终选择最快的互连来路由数据传输。它还支持以统一方式在不同内存层之间传输，跨GPU HBM、CPU DRAM，甚至NVMe SSD！

> Traditionally, one might transfer this data from GPU to GPU through a CPU, but this would be too slow, as we've already discussed. Another option to improve performance is to require that the source and destination GPUs are on the same compute node, but this limits our scaling flexibility. NIXL was created to address this issue. It was designed for direct GPU-to-GPU transfers of large payloads like the KV cache across GPUs, compute nodes, and racks, if necessary. NIXL operates at high bandwidth and overlaps communication with computation as much as possible. This allows the destination GPUs to start generating the next set of tokens while they are receiving the KV cache from the source GPU. Additionally, NIXL is interconnect-agnostic. It will use NVLink if GPUs are in the same compute node, NVSwitch if in the same compute node, InfiniBand or Ethernet with RDMA across nodes, or even PCIe or NVMe if needed. Similar to NCCL, NIXL will always select the fastest interconnects to route the data transfer. It also supports transferring to and from different memory tiers in a unified way across GPU HBM, CPU DRAM, and even NVMe SSD!

### 4.10.3 带回调的NIXL异步API (NIXL Asynchronous API with Callbacks)

从开发者的角度来看，NIXL提供了一个简单的API。你使用指向数据的指针和目标（GPU、CPU或Amazon S3等存储目标）发布传输请求。NIXL将尽可能快地传输该数据。

例如，NIXL传输请求可以将KV缓存段发送到另一个GPU、CPU主机内存缓冲区，甚至对象存储服务。它可以在同一API中完成所有这些，如图4-7所示。

![图4-7 NIXL架构](../assets/images/ch04/fig04_237_1.png)

> From a developer's perspective, NIXL offers a straightforward API. You post a transfer request with a pointer to the data and a destination—either GPUs, CPUs, or storage targets like Amazon S3. NIXL will transfer that data as fast as possible. For instance, a NIXL transfer request could send a KV cache segment to another GPU, a CPU host memory buffer, or even an object storage service. And it can do this all within the same API, as shown in Figure 4-7.

### 4.10.4 使用NIXL进行KV缓存卸载 (KV Cache Offloading with NIXL)

NIXL的设计动机与LLM推理处理大内存的最佳实践密切相关。如果GPU没有足够的内存来容纳长序列或多轮对话的整个KV缓存，NIXL允许推理服务器（例如，NVIDIA Dynamo）将KV缓存卸载到CPU内存——甚至NVMe SSD——并根据需要带回。

NIXL与Dynamo中的KV缓存管理器一起，可以高效管理此传输层次结构。考虑NVIDIA的Grace Hopper和Grace Blackwell Superchips，它们具有大量统一的CPU和GPU内存，通过快速NVLink互连共享（见图4-8）。

![图4-8 基于ARM的Grace Hopper Superchip架构利用900 GB/s NVLink-C2C并克服传统PCIe瓶颈](../assets/images/ch04/fig04_243_1.png)

> The design motivation for NIXL is closely related to best practices for handling large memory for LLM inference. If GPUs don't have enough memory to hold the entire KV cache for a long sequence or multiturn conversation, NIXL allows the inference server (e.g., NVIDIA Dynamo) to offload the KV cache to CPU memory—or even NVMe SSD—and bring it back as needed. NIXL, in conjunction with the KV Cache Manager in Dynamo, for instance, can manage this transfer hierarchy efficiently. Consider NVIDIA's Grace Hopper and Grace Blackwell Superchips with a huge amount of unified CPU and GPU memory shared over the fast NVLink interconnect (see Figure 4-8).

推理服务器可以快速将大型KV缓存卸载到大CPU内存以释放有限的GPU HBM。这产生了推理性能的巨大提升。具体来说，与重新计算缓存相比，基于PCIe的x86 + H100系统可以将长输入序列的首token时间（TTFT）延迟提高多达14倍。图4-9显示了这种加速。

![图4-9 与从头重新计算相比，基于x86的NVIDIA H100 GPU系统在大输入序列长度上通过KV缓存卸载测量到14倍TTFT加速](../assets/images/ch04/fig04_244_1.png)

> The inference server can quickly offload a large KV cache to the large CPU memory to free up the limited GPU HBM. This yields a huge boost in inference performance. Specifically, a PCIe-based x86 + H100 system can improve time-to-first-token (TTFT) latency by as much as 14× for long input sequences compared to recomputing the cache. This speedup is shown in Figure 4-9.

此外，凭借其900 GB/s NVLink-C2C互连，基于ARM的Grace Hopper Superchip比前面描述的基于非superchip、x86的H100版本提供2倍更快的TTFT延迟。图4-10显示了这种加速。

![图4-10 由于900 GB/s NVLink-C2C互连的KV缓存卸载，基于ARM的Grace Hopper Superchip与基于x86的H100 GPU系统相比，TTFT加速2倍](../assets/images/ch04/fig04_246_1.png)

> Furthermore, with its 900 GB/s NVLink-C2C interconnect, the ARM-based Grace Hopper Superchip delivers 2× faster TTFT latency compared to the non-superchip, x86-based H100 version described previously. This speedup is shown in Figure 4-10.

## 4.11 NCCL与NIXL对比 (NCCL Versus NIXL)

重要的是要注意，NIXL不是NCCL的替代品，而是补充。NCCL仍然处理并行处理单个任务/阶段的GPU的同步集合，例如跨多个GPU拆分的all-reduce。另一方面，NIXL在分布式系统中的任务/阶段之间——或不同组件（例如，GPU、CPU、存储）之间执行异步数据传输。表4-5显示了NCCL与NIXL的比较。

| 方面 | NCCL（集合通信） | NIXL（点对点通信） |
|------|------------------|-------------------|
| 主要用例 | 多对多集合（例如，all-reduce、all-gather），用于训练中紧密耦合的GPU组。 | 一对一或一对少传输（例如，发送大型张量或缓存），用于分布式推理或流水线。 |
| 通信模式 | 同步集合操作——所有参与者必须到达调用（屏障语义）。 | 异步发送/接收——一个发起者，一个或多个目标（支持单向数据移动）。 |
| 与计算重叠 | 可以在某种程度上重叠（例如，在DDP中使用单独的CUDA流将反向计算与all-reduce重叠）。 | 设计用于最大重叠——传输与计算完全并行运行，通过轮询通知检测完成。 |
| 拓扑感知 | 是——自动检测拓扑，为集合最优使用环/树和NVLink/NVSwitch。 | 是——互连无关；根据源-目标位置自动使用NVLink、NVSwitch、PCIe、InfiniBand/RDMA或GDS。 |
| 数据范围 | 通常是需要跨所有GPU聚合的小到中等张量（例如，梯度）。 | 为需要快速点对点传输的大型数据块（例如，数百MB或更多，如LLM KV缓存或模型分片）优化。 |
| 集成 | 集成在训练框架中（PyTorch DDP、Horovod等在底层调用NCCL）。 | 作为NVIDIA Dynamo使用的开源库提供。它在Dynamo项目中开发。开发者调用NIXL API在推理服务器或自定义代码中根据需要发送/接收。 |
| 示例 | 跨8个GPU并行进行100 MB梯度的all-reduce。 | 在推理流水线期间将1 GB KV缓存从GPU 0发送到GPU 1（或发送到CPU内存或NVMe SSD）。 |

> It's important to note that NIXL is not a replacement for NCCL but a complement. NCCL still handles synchronized collectives for GPUs working on a single task/stage in parallel, such as an all-reduce split across multiple GPUs. NIXL, on the other hand, performs asynchronous data transfers between tasks/stages—or between distinct components (e.g., GPUs, CPUs, storage) in a distributed system. Table 4-5 shows a comparison of NCCL versus NIXL.

## 4.12 关键要点 (Key Takeaways)

通过精心的工程——以及使用本章描述的技术——你通常可以达到或接近物理"光速"硬件限制的性能。以下是调优网络层时需要记住的一些关键教训：

**拓扑很重要**：节点间（internode，InfiniBand）和节点内（intranode，NVLink/NVSwitch）的互连影响最佳通信策略。考虑多节点和多GPU配置的层次方法。始终确保你使用最快的可用互连——而不是由于配置错误或意外默认值而意外通过慢速路径发送数据！在实践中，检查NCCL的行为，并在可用时利用SHARP等功能进行大规模网络内聚合/归约。

> **Topology matters**: The interconnects between nodes (internode, InfiniBand) and within nodes (intranode, NVLink/NVSwitch) influence the optimal communication strategy. Consider hierarchical approaches for multinode and multi-GPU configurations. Always ensure that you're using the fastest interconnect available—and not accidentally sending data over slow paths due to a misconfiguration or unexpected default value! In practice, check NCCL's behavior and utilize features like SHARP for large-scale, in-network aggregations/reductions if available.

**调优环境和系统**：有时单个环境变量或OS设置可以提高吞吐量。例如，可以增加NIC缓冲区、启用/禁用NCCL功能和日志记录，以及正确绑定CPU。在OS和驱动程序级别执行系统优化（例如，IRQ亲和性）可以帮助消除瓶颈，如第3章所述。

> **Tune the environment and system**: Sometimes a single environment variable or OS setting can boost throughput. For instance, one can increase NIC buffers, enable/disable NCCL features and logging, and pin CPUs correctly. Performing system optimizations at the OS and driver levels (e.g., IRQ affinity) can help remove bottlenecks, as described in Chapter 3.

**利用最新的硬件创新**：NVIDIA Grace Hopper和Grace Blackwell Superchips等新硬件提供大量CPU内存和快速CPU-GPU互连。将它们用于托管大型数据集、拆分数据、分区模型以及将大型KV缓存卸载到CPU等事情。SHARP等网络内计算可以将集合操作加速2倍-5倍——尤其是在规模上。随时了解这些新的计算和网络硬件创新，因为它们将在每一代改变最佳配置。

> **Utilize the latest hardware innovations**: New hardware like NVIDIA Grace Hopper and Grace Blackwell Superchips provide massive CPU memory and fast CPU-GPU interconnects. Use them for things like hosting large datasets, splitting your data, partitioning your model, and offloading large KV caches to the CPU. In-network computing like SHARP can accelerate collective operations by 2×–5×—especially at scale. Stay informed on these new compute and networking hardware innovations because they will change the optimal configuration with each new generation.

你希望将GPU饱和到它们100%的时间都在计算，同时后台在通信的程度。你还希望用有用的数据饱和你的网络链路。你希望让你的磁盘全速流式传输数据。所有这些应该完美和谐地一起发生。

> You want to saturate your GPUs to the point where they're computing 100% of the time while simultaneously communicating in the background. You also want to saturate your network links with useful data. And you want to keep your disks streaming data at full throttle. All of this should happen together in perfect harmony.

实现所有这些需要迭代调优和验证，以及一些权衡，如更多内存使用和更多代码复杂性。但这种调优以更快的模型训练和推理以及昂贵基础设施的更好整体利用率为回报。

> Achieving all of this requires iterative tuning and validation, as well as some trade-offs like more memory usage and more code complexity. But this kind of tuning pays off with faster model training and inference—as well as better overall utilization of expensive infrastructure.

## 4.13 结论 (Conclusion)

高性能、分布式和多GPU通信及存储系统的演进代表了调优大型复杂AI系统的基础。通过使用专门的库，如用于集合操作的NCCL、用于高效推理数据传输的NIXL以及用于超低延迟通信的RDMA，AI系统可以显著减少瓶颈并提高性能。

> The evolution of high-performance, distributed, and multi-GPU communication and storage systems represents the foundation for tuning large, complex AI systems. By utilizing specialized libraries such as NCCL for collective operations, NIXL for efficient inference data transfers, and RDMA for ultra-low-latency communication, AI systems can significantly reduce bottlenecks and increase performance.

智能网络硬件（如NVSwitch和启用SHARP的InfiniBand交换机）的集成直接转化为更高的训练和推理性能。同样，保持软件最新是关键，因为较新版本的CUDA和PyTorch为最新的GPU和网络技术（例如，SHARP）内置了这些优化。使用NVIDIA Dynamo、vLLM和类似服务框架可以帮助轻松为推理工作负载部署这些改进。

> The integration of smart networking hardware like NVSwitch and SHARP-enabled InfiniBand switches translates directly into higher training and inference performance. Likewise, keeping software up-to-date is key, as newer versions of CUDA and PyTorch come with these optimizations built in for the latest GPUs and networking technology (e.g., SHARP). Utilizing NVIDIA Dynamo, vLLM, and similar serving frameworks can help deploy these improvements easily for inference workloads.

最终，本章强调没有单个组件可以单独提供峰值性能。正是高速通信、高效数据处理和系统级调优的仔细协调和协同设计，才能带来可扩展和健壮的AI系统。

> Ultimately, this chapter highlights that no single component can provide peak performance alone. It is the careful coordination and codesign of high-speed communication, efficient data handling, and system-wide tuning that leads to scalable and robust AI systems.

对于性能工程师来说，教训是快速数据移动与原始计算能力同样关键。世界上最快的GPU如果不断等待来自CPU或另一个GPU的数据，几乎没有什么好处。

> For performance engineers, the lesson is that fast data movement is as critical as raw compute power. The fastest GPU in the world provides little benefit if it's constantly waiting for data from a CPU or another GPU.

在下一章中，我们将探讨基于GPU的存储策略和优化。补充RDMA、NCCL和NIXL等网络协议和库，GDS和高效输入管道是让GPU持续有工作可做的整体方法的一部分。

> In the next chapter, we will explore GPU-based storage strategies and optimizations. Complementing networking protocols and libraries like RDMA, NCCL, and NIXL, GDS, and highly efficient input pipelines are part of the holistic approach to keeping the GPUs fed with continuous work.
